From ed73deedc8e9a4faaf9cd7ce50442b1541983b0b Mon Sep 17 00:00:00 2001
From: Shreepadma Venugopalan <shreepadma@cloudera.com>
Date: Wed, 1 May 2013 13:08:20 -0700
Subject: [PATCH 107/121] CDH-11803. Restrict getTables(), getColumns() and getSchemas() JDBC APIs when authorization is enabled

---
 ql/src/java/org/apache/hadoop/hive/ql/Driver.java  |   12 ++-
 .../hive/ql/HiveDriverFilterHookContext.java       |    1 +
 .../hive/ql/HiveDriverFilterHookContextImpl.java   |   14 +++-
 .../service/cli/operation/GetColumnsOperation.java |   13 +++-
 .../service/cli/operation/GetSchemasOperation.java |   12 +++-
 .../service/cli/operation/GetTablesOperation.java  |   14 +++-
 .../service/cli/operation/MetadataOperation.java   |   74 ++++++++++++++++++++
 7 files changed, 127 insertions(+), 13 deletions(-)

diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/Driver.java b/src/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
index 929061f..2bcce7f 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
@@ -1435,18 +1435,22 @@ public class Driver implements CommandProcessor {
       if (res != null && !res.isEmpty() &&
            filterHooks != null && !filterHooks.isEmpty() &&
              isExecMetadataLookup(hiveOperation)) {
+        String currentDbName = Hive.get().getCurrentDatabase();
         HiveDriverFilterHookContext hookCtx = new HiveDriverFilterHookContextImpl(conf,
-                                                  hiveOperation, userName, res);
+                                                  hiveOperation, userName, res, currentDbName);
         HiveDriverFilterHookResult hookResult;
+        List<String> filteredValues = null;
         for (HiveDriverFilterHook hook : filterHooks) {
           // result set 'res' is passed to the filter hooks. The filter hooks shouldn't mutate res
           // directly. They should return a filtered result set instead.
           hookResult = hook.postDriverFetch(hookCtx);
           // pass the filtered result set back to the client
-          res.clear();
-          List<String> filteredValues = hookResult.getResult();
-          res.addAll(filteredValues);
+          filteredValues = hookResult.getResult();
+          ((HiveDriverFilterHookContextImpl)hookCtx).setResult(filteredValues);
+
         }
+        res.clear();
+        res.addAll(filteredValues);
       }
     } catch (Exception e) {
        throw new CommandNeedRetryException(e);
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/HiveDriverFilterHookContext.java b/src/ql/src/java/org/apache/hadoop/hive/ql/HiveDriverFilterHookContext.java
index 80c086f..85ad24d 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/HiveDriverFilterHookContext.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/HiveDriverFilterHookContext.java
@@ -31,4 +31,5 @@ public interface HiveDriverFilterHookContext extends Configurable{
   public HiveOperation getHiveOperation ();
   public String getUserName();
   public List<String> getResult();
+  public String getDbName();
 }
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/HiveDriverFilterHookContextImpl.java b/src/ql/src/java/org/apache/hadoop/hive/ql/HiveDriverFilterHookContextImpl.java
index feefaf5..7800836 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/HiveDriverFilterHookContextImpl.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/HiveDriverFilterHookContextImpl.java
@@ -28,14 +28,16 @@ public class HiveDriverFilterHookContextImpl implements HiveDriverFilterHookCont
   private Configuration conf;
   private final HiveOperation hiveOperation;
   private final String userName;
-  private final List<String> result;
+  private List<String> result;
+  private final String dbName;
 
   public HiveDriverFilterHookContextImpl(Configuration conf, HiveOperation hiveOperation,
-    String userName, List<String> result) {
+    String userName, List<String> result, String dbName) {
     this.conf = conf;
     this.hiveOperation = hiveOperation;
     this.userName = userName;
     this.result = result;
+    this.dbName = dbName;
   }
 
   @Override
@@ -59,4 +61,12 @@ public class HiveDriverFilterHookContextImpl implements HiveDriverFilterHookCont
     this.conf = conf;
   }
 
+  public String getDbName() {
+    return dbName;
+  }
+
+  public void setResult(List<String> result) {
+    this.result = result;
+  }
+
 }
diff --git a/src/service/src/java/org/apache/hive/service/cli/operation/GetColumnsOperation.java b/src/service/src/java/org/apache/hive/service/cli/operation/GetColumnsOperation.java
index 3424890..afc2b9b 100644
--- a/src/service/src/java/org/apache/hive/service/cli/operation/GetColumnsOperation.java
+++ b/src/service/src/java/org/apache/hive/service/cli/operation/GetColumnsOperation.java
@@ -26,6 +26,8 @@ import java.util.regex.Pattern;
 
 import org.apache.hadoop.hive.metastore.IMetaStoreClient;
 import org.apache.hadoop.hive.metastore.api.Table;
+import org.apache.hadoop.hive.ql.metadata.Hive;
+import org.apache.hadoop.hive.ql.plan.HiveOperation;
 import org.apache.hive.service.cli.ColumnDescriptor;
 import org.apache.hive.service.cli.FetchOrientation;
 import org.apache.hive.service.cli.HiveSQLException;
@@ -130,11 +132,14 @@ public class GetColumnsOperation extends MetadataOperation {
       }
 
       List<String> dbNames = metastoreClient.getDatabases(schemaPattern);
-      Collections.sort(dbNames);
-      for (String dbName : dbNames) {
+      String currentDbName = Hive.get().getCurrentDatabase();
+      List<String> filteredDbNames = filterResultSet(dbNames, HiveOperation.SHOWDATABASES, currentDbName);
+      Collections.sort(filteredDbNames);
+      for (String dbName : filteredDbNames) {
         List<String> tableNames = metastoreClient.getTables(dbName, tablePattern);
-        Collections.sort(tableNames);
-        for (Table table : metastoreClient.getTableObjectsByName(dbName, tableNames)) {
+        List<String> filteredTableNames = filterResultSet(tableNames, HiveOperation.SHOWTABLES, dbName);
+        Collections.sort(filteredTableNames);
+        for (Table table : metastoreClient.getTableObjectsByName(dbName, filteredTableNames)) {
           TableSchema schema = new TableSchema(metastoreClient.getSchema(dbName, table.getTableName()));
           for (ColumnDescriptor column : schema.getColumnDescriptors()) {
             if (columnPattern != null && !columnPattern.matcher(column.getName()).matches()) {
diff --git a/src/service/src/java/org/apache/hive/service/cli/operation/GetSchemasOperation.java b/src/service/src/java/org/apache/hive/service/cli/operation/GetSchemasOperation.java
index ae67cd3..e26bdb4 100644
--- a/src/service/src/java/org/apache/hive/service/cli/operation/GetSchemasOperation.java
+++ b/src/service/src/java/org/apache/hive/service/cli/operation/GetSchemasOperation.java
@@ -19,8 +19,11 @@
 package org.apache.hive.service.cli.operation;
 
 import java.util.EnumSet;
+import java.util.List;
 
 import org.apache.hadoop.hive.metastore.IMetaStoreClient;
+import org.apache.hadoop.hive.ql.metadata.Hive;
+import org.apache.hadoop.hive.ql.plan.HiveOperation;
 import org.apache.hive.service.cli.FetchOrientation;
 import org.apache.hive.service.cli.HiveSQLException;
 import org.apache.hive.service.cli.OperationState;
@@ -60,7 +63,14 @@ public class GetSchemasOperation extends MetadataOperation {
     try {
       IMetaStoreClient metastoreClient = getParentSession().getMetaStoreClient();
       String schemaPattern = convertSchemaPattern(schemaName);
-      for (String dbName : metastoreClient.getDatabases(schemaPattern)) {
+      List<String > dbNames = metastoreClient.getDatabases(schemaPattern);
+
+      // filter the list of dbnames
+      HiveOperation hiveOperation = HiveOperation.SHOWDATABASES;
+      String currentDbName = Hive.get().getCurrentDatabase();
+      List<String> filteredDbNames = filterResultSet(dbNames, hiveOperation, currentDbName);
+
+      for (String dbName : filteredDbNames) {
         rowSet.addRow(RESULT_SET_SCHEMA, new Object[] {dbName, DEFAULT_HIVE_CATALOG});
       }
       setState(OperationState.FINISHED);
diff --git a/src/service/src/java/org/apache/hive/service/cli/operation/GetTablesOperation.java b/src/service/src/java/org/apache/hive/service/cli/operation/GetTablesOperation.java
index 3cfef3a..5335234 100644
--- a/src/service/src/java/org/apache/hive/service/cli/operation/GetTablesOperation.java
+++ b/src/service/src/java/org/apache/hive/service/cli/operation/GetTablesOperation.java
@@ -24,6 +24,8 @@ import java.util.List;
 
 import org.apache.hadoop.hive.metastore.IMetaStoreClient;
 import org.apache.hadoop.hive.metastore.api.Table;
+import org.apache.hadoop.hive.ql.metadata.Hive;
+import org.apache.hadoop.hive.ql.plan.HiveOperation;
 import org.apache.hive.service.cli.FetchOrientation;
 import org.apache.hive.service.cli.HiveSQLException;
 import org.apache.hive.service.cli.OperationState;
@@ -74,9 +76,17 @@ public class GetTablesOperation extends MetadataOperation {
       IMetaStoreClient metastoreClient = getParentSession().getMetaStoreClient();
       String schemaPattern = convertSchemaPattern(schemaName);
       String tablePattern = convertIdentifierPattern(tableName, true);
-      for (String dbName : metastoreClient.getDatabases(schemaPattern)) {
+
+      HiveOperation hiveOperation = HiveOperation.SHOWDATABASES;
+      String currentDbName = Hive.get().getCurrentDatabase();
+      List<String> dbNames = metastoreClient.getDatabases(schemaPattern);
+      List<String> filteredDbNames = filterResultSet(dbNames, hiveOperation, currentDbName);
+
+      for (String dbName : filteredDbNames) {
         List<String> tableNames = metastoreClient.getTables(dbName, tablePattern);
-        for (Table table : metastoreClient.getTableObjectsByName(dbName, tableNames)) {
+        hiveOperation = HiveOperation.SHOWTABLES;
+        List <String> filteredTableNames = filterResultSet(tableNames, hiveOperation, dbName);
+        for (Table table : metastoreClient.getTableObjectsByName(dbName, filteredTableNames)) {
           Object[] rowData = new Object[] {
               DEFAULT_HIVE_CATALOG,
               table.getDbName(),
diff --git a/src/service/src/java/org/apache/hive/service/cli/operation/MetadataOperation.java b/src/service/src/java/org/apache/hive/service/cli/operation/MetadataOperation.java
index 8dc82ab..227ba31 100644
--- a/src/service/src/java/org/apache/hive/service/cli/operation/MetadataOperation.java
+++ b/src/service/src/java/org/apache/hive/service/cli/operation/MetadataOperation.java
@@ -18,6 +18,18 @@
 
 package org.apache.hive.service.cli.operation;
 
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.hadoop.hive.common.JavaUtils;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.HiveDriverFilterHook;
+import org.apache.hadoop.hive.ql.HiveDriverFilterHookContext;
+import org.apache.hadoop.hive.ql.HiveDriverFilterHookContextImpl;
+import org.apache.hadoop.hive.ql.HiveDriverFilterHookResult;
+import org.apache.hadoop.hive.ql.hooks.Hook;
+import org.apache.hadoop.hive.ql.plan.HiveOperation;
+import org.apache.hadoop.util.StringUtils;
 import org.apache.hive.service.cli.HiveSQLException;
 import org.apache.hive.service.cli.OperationState;
 import org.apache.hive.service.cli.OperationType;
@@ -96,4 +108,66 @@ public abstract class MetadataOperation extends Operation {
           .replaceAll("([^\\\\])_", "$1.").replaceAll("\\\\_", "_").replaceAll("^_", ".");
   }
 
+  private <T extends Hook> List<T> getHooks(HiveConf.ConfVars hookConfVar, Class<T> clazz)
+      throws Exception {
+    HiveConf conf = getParentSession().getHiveConf();
+    List<T> hooks = new ArrayList<T>();
+    String csHooks = conf.getVar(hookConfVar);
+    if (csHooks == null) {
+      return hooks;
+    }
+
+    csHooks = csHooks.trim();
+    if (csHooks.equals("")) {
+      return hooks;
+    }
+
+    String[] hookClasses = csHooks.split(",");
+
+    for (String hookClass : hookClasses) {
+      try {
+        T hook =
+            (T) Class.forName(hookClass.trim(), true, JavaUtils.getClassLoader()).newInstance();
+        hooks.add(hook);
+      } catch (ClassNotFoundException e) {
+        LOG.error(hookConfVar.varname + " Class not found:" + e.getMessage());
+        throw e;
+      }
+    }
+    return hooks;
+  }
+
+  protected List<String> filterResultSet(List<String> inputResultSet, HiveOperation hiveOperation, String dbName)
+    throws Exception {
+    List<String> filteredResultSet = new ArrayList<String>();
+    HiveConf conf = getParentSession().getHiveConf();
+    String userName = getParentSession().getUserName();
+    List<HiveDriverFilterHook> filterHooks = null;
+
+    try {
+      filterHooks = getHooks(HiveConf.ConfVars.HIVE_EXEC_FILTER_HOOK,
+          HiveDriverFilterHook.class);
+    } catch (Exception e) {
+      LOG.error("Failed to obtain filter hooks");
+      LOG.error(StringUtils.stringifyException(e));
+    }
+
+    // if the result set is non null, non empty and exec filter hooks are present
+    // invoke the hooks to filter the result set
+    if (inputResultSet != null && !inputResultSet.isEmpty() && filterHooks != null && !filterHooks.isEmpty() )  {
+      HiveDriverFilterHookContext hookCtx = new HiveDriverFilterHookContextImpl(conf,
+                                                           hiveOperation, userName, inputResultSet, dbName);
+      HiveDriverFilterHookResult hookResult = null;
+      for (HiveDriverFilterHook hook : filterHooks) {
+        // result set 'inputResultSet' is passed to the filter hooks. The filter hooks shouldn't
+        // mutate inputResultSet directly. They should return a filtered result set instead.
+        hookResult = hook.postDriverFetch(hookCtx);
+        ((HiveDriverFilterHookContextImpl)hookCtx).setResult(hookResult.getResult());
+      }
+      filteredResultSet.addAll(hookResult.getResult());
+      return filteredResultSet;
+    } else {
+      return inputResultSet;
+    }
+  }
 }
-- 
1.7.0.4

