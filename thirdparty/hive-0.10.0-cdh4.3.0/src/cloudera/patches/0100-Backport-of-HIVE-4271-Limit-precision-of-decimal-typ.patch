From 8cbd0ac084e8633f492d322c7bb61d55d3a01fc7 Mon Sep 17 00:00:00 2001
From: Ashutosh Chauhan <hashutosh@apache.org>
Date: Thu, 24 Jan 2013 18:08:52 +0000
Subject: [PATCH 100/121] Backport of HIVE-4271 (Limit precision of decimal type)

---
 .../org/apache/hadoop/hive/common/ObjectPair.java  |   47 ++
 .../hadoop/hive/metastore/MetaStoreUtils.java      |   29 +-
 .../apache/hadoop/hive/metastore/ObjectStore.java  |    2 +-
 .../org/apache/hadoop/hive/ql/exec/ExecMapper.java |    2 -
 .../apache/hadoop/hive/ql/exec/FetchOperator.java  |   90 +++-
 .../apache/hadoop/hive/ql/exec/MapOperator.java    |  222 ++++++--
 .../hadoop/hive/ql/exec/SMBMapJoinOperator.java    |    2 +-
 .../org/apache/hadoop/hive/ql/exec/Utilities.java  |   30 +-
 .../apache/hadoop/hive/ql/metadata/Partition.java  |    6 +-
 .../org/apache/hadoop/hive/ql/metadata/Table.java  |    4 +-
 .../hadoop/hive/ql/optimizer/GenMapRedUtils.java   |    7 +-
 .../hadoop/hive/ql/parse/SemanticAnalyzer.java     |    2 +-
 .../apache/hadoop/hive/ql/plan/PartitionDesc.java  |    2 +-
 .../org/apache/hadoop/hive/ql/plan/TableDesc.java  |   42 ++-
 .../org/apache/hadoop/hive/ql/util/ObjectPair.java |   47 --
 .../hadoop/hive/ql/metadata/TestPartition.java     |   68 ---
 .../clientpositive/partition_wise_fileformat10.q   |   13 +
 .../clientpositive/partition_wise_fileformat11.q   |   19 +
 .../clientpositive/partition_wise_fileformat12.q   |   26 +
 .../clientpositive/partition_wise_fileformat13.q   |   17 +
 .../clientpositive/partition_wise_fileformat14.q   |   57 ++
 .../clientpositive/partition_wise_fileformat8.q    |   13 +
 .../clientpositive/partition_wise_fileformat9.q    |   12 +
 .../results/clientpositive/bucketcontext_1.q.out   |    8 -
 .../results/clientpositive/bucketcontext_2.q.out   |    8 -
 .../results/clientpositive/bucketcontext_3.q.out   |    4 -
 .../results/clientpositive/bucketcontext_4.q.out   |    4 -
 .../results/clientpositive/bucketcontext_6.q.out   |    8 -
 .../results/clientpositive/bucketcontext_7.q.out   |    8 -
 .../results/clientpositive/bucketcontext_8.q.out   |    8 -
 .../results/clientpositive/bucketmapjoin1.q.out    |    2 -
 .../results/clientpositive/bucketmapjoin10.q.out   |    2 -
 .../results/clientpositive/bucketmapjoin11.q.out   |    4 -
 .../results/clientpositive/bucketmapjoin12.q.out   |    2 -
 .../results/clientpositive/bucketmapjoin13.q.out   |    5 -
 .../results/clientpositive/bucketmapjoin2.q.out    |    3 -
 .../results/clientpositive/bucketmapjoin3.q.out    |    2 -
 .../results/clientpositive/bucketmapjoin5.q.out    |    4 -
 .../results/clientpositive/bucketmapjoin7.q.out    |    1 -
 .../results/clientpositive/bucketmapjoin8.q.out    |    2 -
 .../results/clientpositive/bucketmapjoin9.q.out    |    2 -
 .../clientpositive/columnstats_partlvl.q.out       |    2 -
 .../results/clientpositive/combine2_hadoop20.q.out |    8 -
 .../clientpositive/filter_join_breaktask.q.out     |    2 -
 .../results/clientpositive/groupby_map_ppr.q.out   |    2 -
 .../groupby_map_ppr_multi_distinct.q.out           |    2 -
 .../test/results/clientpositive/groupby_ppr.q.out  |    2 -
 .../groupby_ppr_multi_distinct.q.out               |    2 -
 .../results/clientpositive/groupby_sort_6.q.out    |  579 ++++++++++++++++++++
 ql/src/test/results/clientpositive/input23.q.out   |    1 -
 ql/src/test/results/clientpositive/input42.q.out   |    6 -
 .../test/results/clientpositive/input_part1.q.out  |    1 -
 .../test/results/clientpositive/input_part2.q.out  |    2 -
 .../test/results/clientpositive/input_part7.q.out  |    2 -
 .../test/results/clientpositive/input_part9.q.out  |    2 -
 ql/src/test/results/clientpositive/join26.q.out    |    1 -
 ql/src/test/results/clientpositive/join33.q.out    |    1 -
 ql/src/test/results/clientpositive/join9.q.out     |    1 -
 .../test/results/clientpositive/join_map_ppr.q.out |    2 -
 .../results/clientpositive/load_dyn_part8.q.out    |    4 -
 .../results/clientpositive/louter_join_ppr.q.out   |   12 -
 ql/src/test/results/clientpositive/merge3.q.out    |    4 -
 .../results/clientpositive/outer_join_ppr.q.out    |    8 -
 .../partition_wise_fileformat10.q.out              |   79 +++
 .../partition_wise_fileformat11.q.out              |  123 +++++
 .../partition_wise_fileformat12.q.out              |  216 ++++++++
 .../partition_wise_fileformat13.q.out              |  128 +++++
 .../partition_wise_fileformat14.q.out              |  234 ++++++++
 .../partition_wise_fileformat8.q.out               |  147 +++++
 .../partition_wise_fileformat9.q.out               |  113 ++++
 ql/src/test/results/clientpositive/pcr.q.out       |   41 --
 .../results/clientpositive/ppd_union_view.q.out    |    3 -
 .../clientpositive/ppr_allchildsarenull.q.out      |    6 -
 .../clientpositive/rand_partitionpruner2.q.out     |    2 -
 .../clientpositive/rand_partitionpruner3.q.out     |    2 -
 .../results/clientpositive/router_join_ppr.q.out   |   12 -
 ql/src/test/results/clientpositive/sample1.q.out   |    1 -
 ql/src/test/results/clientpositive/sample10.q.out  |    4 -
 ql/src/test/results/clientpositive/sample8.q.out   |    4 -
 .../results/clientpositive/smb_mapjoin_11.q.out    |    2 -
 .../results/clientpositive/smb_mapjoin_12.q.out    |    4 -
 .../clientpositive/sort_merge_join_desc_5.q.out    |    2 -
 .../clientpositive/sort_merge_join_desc_6.q.out    |    2 -
 .../clientpositive/sort_merge_join_desc_7.q.out    |    2 -
 ql/src/test/results/clientpositive/stats11.q.out   |    2 -
 .../results/clientpositive/transform_ppr1.q.out    |    4 -
 .../results/clientpositive/transform_ppr2.q.out    |    2 -
 ql/src/test/results/clientpositive/union22.q.out   |    2 -
 ql/src/test/results/clientpositive/union_ppr.q.out |    2 -
 .../results/compiler/plan/case_sensitivity.q.xml   |   24 -
 ql/src/test/results/compiler/plan/cast1.q.xml      |   24 -
 ql/src/test/results/compiler/plan/groupby1.q.xml   |   24 -
 ql/src/test/results/compiler/plan/groupby2.q.xml   |   24 -
 ql/src/test/results/compiler/plan/groupby3.q.xml   |   24 -
 ql/src/test/results/compiler/plan/groupby4.q.xml   |   24 -
 ql/src/test/results/compiler/plan/groupby5.q.xml   |   24 -
 ql/src/test/results/compiler/plan/groupby6.q.xml   |   24 -
 ql/src/test/results/compiler/plan/input1.q.xml     |   24 -
 ql/src/test/results/compiler/plan/input2.q.xml     |   24 -
 ql/src/test/results/compiler/plan/input20.q.xml    |   24 -
 ql/src/test/results/compiler/plan/input3.q.xml     |   24 -
 ql/src/test/results/compiler/plan/input4.q.xml     |   24 -
 ql/src/test/results/compiler/plan/input5.q.xml     |   24 -
 ql/src/test/results/compiler/plan/input6.q.xml     |   24 -
 ql/src/test/results/compiler/plan/input7.q.xml     |   24 -
 ql/src/test/results/compiler/plan/input8.q.xml     |   24 -
 ql/src/test/results/compiler/plan/input9.q.xml     |   24 -
 .../test/results/compiler/plan/input_part1.q.xml   |   12 +-
 .../compiler/plan/input_testsequencefile.q.xml     |   24 -
 .../results/compiler/plan/input_testxpath.q.xml    |   24 -
 .../results/compiler/plan/input_testxpath2.q.xml   |   24 -
 ql/src/test/results/compiler/plan/join1.q.xml      |   48 --
 ql/src/test/results/compiler/plan/join2.q.xml      |   78 +---
 ql/src/test/results/compiler/plan/join3.q.xml      |   72 ---
 ql/src/test/results/compiler/plan/join4.q.xml      |   48 --
 ql/src/test/results/compiler/plan/join5.q.xml      |   48 --
 ql/src/test/results/compiler/plan/join6.q.xml      |   48 --
 ql/src/test/results/compiler/plan/join7.q.xml      |   72 ---
 ql/src/test/results/compiler/plan/join8.q.xml      |   48 --
 ql/src/test/results/compiler/plan/sample1.q.xml    |   12 +-
 ql/src/test/results/compiler/plan/sample2.q.xml    |   24 -
 ql/src/test/results/compiler/plan/sample3.q.xml    |   24 -
 ql/src/test/results/compiler/plan/sample4.q.xml    |   24 -
 ql/src/test/results/compiler/plan/sample5.q.xml    |   24 -
 ql/src/test/results/compiler/plan/sample6.q.xml    |   24 -
 ql/src/test/results/compiler/plan/sample7.q.xml    |   24 -
 ql/src/test/results/compiler/plan/subq.q.xml       |   24 -
 ql/src/test/results/compiler/plan/udf1.q.xml       |   24 -
 ql/src/test/results/compiler/plan/udf4.q.xml       |    4 -
 ql/src/test/results/compiler/plan/udf6.q.xml       |   24 -
 ql/src/test/results/compiler/plan/udf_case.q.xml   |   24 -
 ql/src/test/results/compiler/plan/udf_when.q.xml   |   24 -
 ql/src/test/results/compiler/plan/union.q.xml      |   48 --
 .../apache/hadoop/hive/serde2/NullStructSerDe.java |   61 ++-
 .../objectinspector/ObjectInspectorConverters.java |  212 +++++---
 .../SettableStructObjectInspector.java             |    5 +
 .../objectinspector/StructObjectInspector.java     |    4 +
 137 files changed, 2329 insertions(+), 1880 deletions(-)
 create mode 100644 common/src/java/org/apache/hadoop/hive/common/ObjectPair.java
 delete mode 100644 ql/src/java/org/apache/hadoop/hive/ql/util/ObjectPair.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/metadata/TestPartition.java
 create mode 100644 ql/src/test/queries/clientpositive/partition_wise_fileformat10.q
 create mode 100644 ql/src/test/queries/clientpositive/partition_wise_fileformat11.q
 create mode 100644 ql/src/test/queries/clientpositive/partition_wise_fileformat12.q
 create mode 100644 ql/src/test/queries/clientpositive/partition_wise_fileformat13.q
 create mode 100644 ql/src/test/queries/clientpositive/partition_wise_fileformat14.q
 create mode 100644 ql/src/test/queries/clientpositive/partition_wise_fileformat8.q
 create mode 100644 ql/src/test/queries/clientpositive/partition_wise_fileformat9.q
 create mode 100644 ql/src/test/results/clientpositive/groupby_sort_6.q.out
 create mode 100644 ql/src/test/results/clientpositive/partition_wise_fileformat10.q.out
 create mode 100644 ql/src/test/results/clientpositive/partition_wise_fileformat11.q.out
 create mode 100644 ql/src/test/results/clientpositive/partition_wise_fileformat12.q.out
 create mode 100644 ql/src/test/results/clientpositive/partition_wise_fileformat13.q.out
 create mode 100644 ql/src/test/results/clientpositive/partition_wise_fileformat14.q.out
 create mode 100644 ql/src/test/results/clientpositive/partition_wise_fileformat8.q.out
 create mode 100644 ql/src/test/results/clientpositive/partition_wise_fileformat9.q.out

diff --git a/src/common/src/java/org/apache/hadoop/hive/common/ObjectPair.java b/src/common/src/java/org/apache/hadoop/hive/common/ObjectPair.java
new file mode 100644
index 0000000..a88d0fb
--- /dev/null
+++ b/src/common/src/java/org/apache/hadoop/hive/common/ObjectPair.java
@@ -0,0 +1,47 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.common;
+
+public class ObjectPair<F, S> {
+  private F first;
+  private S second;
+
+  public ObjectPair() {}
+
+  public ObjectPair(F first, S second) {
+    this.first = first;
+    this.second = second;
+  }
+
+  public F getFirst() {
+    return first;
+  }
+
+  public void setFirst(F first) {
+    this.first = first;
+  }
+
+  public S getSecond() {
+    return second;
+  }
+
+  public void setSecond(S second) {
+    this.second = second;
+  }
+}
diff --git a/src/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java b/src/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java
index 3ddd345..a652320 100644
--- a/src/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java
+++ b/src/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java
@@ -158,7 +158,9 @@ public class MetaStoreUtils {
    *          hadoop config
    * @param schema
    *          the properties to use to instantiate the deserializer
-   * @return the Deserializer
+   * @return
+   *   Returns instantiated deserializer by looking up class name of deserializer stored in passed
+   *   in properties. Also, initializes the deserializer with schema stored in passed in properties.
    * @exception MetaException
    *              if any problems instantiating the Deserializer
    *
@@ -190,7 +192,10 @@ public class MetaStoreUtils {
    *          - hadoop config
    * @param table
    *          the table
-   * @return the Deserializer
+   * @return
+   *   Returns instantiated deserializer by looking up class name of deserializer stored in
+   *   storage descriptor of passed in table. Also, initializes the deserializer with schema
+   *   of table.
    * @exception MetaException
    *              if any problems instantiating the Deserializer
    *
@@ -205,7 +210,7 @@ public class MetaStoreUtils {
     }
     try {
       Deserializer deserializer = SerDeUtils.lookupDeserializer(lib);
-      deserializer.initialize(conf, MetaStoreUtils.getSchema(table));
+      deserializer.initialize(conf, MetaStoreUtils.getTableMetadata(table));
       return deserializer;
     } catch (RuntimeException e) {
       throw e;
@@ -227,7 +232,10 @@ public class MetaStoreUtils {
    * @param part
    *          the partition
    * @param table the table
-   * @return the Deserializer
+   * @return
+   *   Returns instantiated deserializer by looking up class name of deserializer stored in
+   *   storage descriptor of passed in partition. Also, initializes the deserializer with
+   *   schema of partition.
    * @exception MetaException
    *              if any problems instantiating the Deserializer
    *
@@ -238,7 +246,7 @@ public class MetaStoreUtils {
     String lib = part.getSd().getSerdeInfo().getSerializationLib();
     try {
       Deserializer deserializer = SerDeUtils.lookupDeserializer(lib);
-      deserializer.initialize(conf, MetaStoreUtils.getSchema(part, table));
+      deserializer.initialize(conf, MetaStoreUtils.getPartitionMetadata(part, table));
       return deserializer;
     } catch (RuntimeException e) {
       throw e;
@@ -494,12 +502,21 @@ public class MetaStoreUtils {
     return ddl.toString();
   }
 
-  public static Properties getSchema(
+  public static Properties getTableMetadata(
       org.apache.hadoop.hive.metastore.api.Table table) {
     return MetaStoreUtils.getSchema(table.getSd(), table.getSd(), table
         .getParameters(), table.getDbName(), table.getTableName(), table.getPartitionKeys());
   }
 
+  public static Properties getPartitionMetadata(
+      org.apache.hadoop.hive.metastore.api.Partition partition,
+      org.apache.hadoop.hive.metastore.api.Table table) {
+    return MetaStoreUtils
+        .getSchema(partition.getSd(), partition.getSd(), partition
+            .getParameters(), table.getDbName(), table.getTableName(),
+            table.getPartitionKeys());
+  }
+
   public static Properties getSchema(
       org.apache.hadoop.hive.metastore.api.Partition part,
       org.apache.hadoop.hive.metastore.api.Table table) {
diff --git a/src/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java b/src/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
index 9a2836e..d44f5ab 100644
--- a/src/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
+++ b/src/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
@@ -1282,7 +1282,7 @@ public class ObjectStore implements RawStore, Configurable {
       return null;
     }
     return new Partition(mpart.getValues(), dbName, tblName, mpart.getCreateTime(),
-        mpart.getLastAccessTime(), convertToStorageDescriptor(mpart.getSd(), true),
+        mpart.getLastAccessTime(), convertToStorageDescriptor(mpart.getSd(), false),
         mpart.getParameters());
   }
 
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecMapper.java b/src/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecMapper.java
index 9a8c237..90893ca 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecMapper.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecMapper.java
@@ -109,8 +109,6 @@ public class ExecMapper extends MapReduceBase implements Mapper {
         dummyOp.setExecContext(execContext);
         dummyOp.initialize(jc,null);
       }
-
-
     } catch (Throwable e) {
       abort = true;
       if (e instanceof OutOfMemoryError) {
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java b/src/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java
index fc2713b..a84a546 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java
@@ -50,6 +50,8 @@ import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.objectinspector.DelegatedObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.InspectableObject;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.Converter;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
@@ -92,6 +94,9 @@ public class FetchOperator implements Serializable {
   private transient Writable value;
   private transient Writable[] vcValues;
   private transient Deserializer serde;
+  private transient Deserializer tblSerde;
+  private transient Converter partTblObjectInspectorConverter;
+
   private transient Iterator<Path> iterPath;
   private transient Iterator<PartitionDesc> iterPartDesc;
   private transient Path currPath;
@@ -220,34 +225,35 @@ public class FetchOperator implements Serializable {
     return inputFormats.get(inputFormatClass);
   }
 
-  private StructObjectInspector setTableDesc(TableDesc table) throws Exception {
+  private StructObjectInspector getRowInspectorFromTable(TableDesc table) throws Exception {
     Deserializer serde = table.getDeserializerClass().newInstance();
     serde.initialize(job, table.getProperties());
-    return createRowInspector(getCurrent(serde));
+    return createRowInspector(getStructOIFrom(serde.getObjectInspector()));
   }
 
-  private StructObjectInspector setPrtnDesc(PartitionDesc partition) throws Exception {
-    Deserializer serde = partition.getDeserializerClass().newInstance();
-    serde.initialize(job, partition.getProperties());
+  private StructObjectInspector getRowInspectorFromPartition(PartitionDesc partition,
+      ObjectInspector partitionOI) throws Exception {
+
     String pcols = partition.getTableDesc().getProperties().getProperty(
         org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_PARTITION_COLUMNS);
     String[] partKeys = pcols.trim().split("/");
     row[1] = createPartValue(partKeys, partition.getPartSpec());
-    return createRowInspector(getCurrent(serde), partKeys);
+
+    return createRowInspector(getStructOIFrom(partitionOI), partKeys);
   }
 
-  private StructObjectInspector setPrtnDesc(TableDesc table) throws Exception {
+  private StructObjectInspector getRowInspectorFromPartitionedTable(TableDesc table)
+      throws Exception {
     Deserializer serde = table.getDeserializerClass().newInstance();
     serde.initialize(job, table.getProperties());
     String pcols = table.getProperties().getProperty(
         org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_PARTITION_COLUMNS);
     String[] partKeys = pcols.trim().split("/");
     row[1] = null;
-    return createRowInspector(getCurrent(serde), partKeys);
+    return createRowInspector(getStructOIFrom(serde.getObjectInspector()), partKeys);
   }
 
-  private StructObjectInspector getCurrent(Deserializer serde) throws SerDeException {
-    ObjectInspector current = serde.getObjectInspector();
+  private StructObjectInspector getStructOIFrom(ObjectInspector current) throws SerDeException {
     if (objectInspector != null) {
       current = DelegatedObjectInspectorFactory.reset(objectInspector, current);
     } else {
@@ -360,16 +366,16 @@ public class FetchOperator implements Serializable {
       job.set("mapred.input.dir", org.apache.hadoop.util.StringUtils.escapeString(currPath
           .toString()));
 
-      PartitionDesc tmp;
+      PartitionDesc partDesc;
       if (currTbl == null) {
-        tmp = currPart;
+        partDesc = currPart;
       } else {
-        tmp = new PartitionDesc(currTbl, null);
+        partDesc = new PartitionDesc(currTbl, null);
       }
 
-      Class<? extends InputFormat> formatter = tmp.getInputFileFormatClass();
+      Class<? extends InputFormat> formatter = partDesc.getInputFileFormatClass();
       inputFormat = getInputFormatFromCache(formatter, job);
-      Utilities.copyTableJobPropertiesToConf(tmp.getTableDesc(), job);
+      Utilities.copyTableJobPropertiesToConf(partDesc.getTableDesc(), job);
       InputSplit[] splits = inputFormat.getSplits(job, 1);
       FetchInputFormatSplit[] inputSplits = new FetchInputFormatSplit[splits.length];
       for (int i = 0; i < splits.length; i++) {
@@ -381,17 +387,32 @@ public class FetchOperator implements Serializable {
       this.inputSplits = inputSplits;
 
       splitNum = 0;
-      serde = tmp.getDeserializerClass().newInstance();
-      serde.initialize(job, tmp.getProperties());
+      serde = partDesc.getDeserializerClass().newInstance();
+      serde.initialize(job, partDesc.getProperties());
+
+      if (currTbl != null) {
+        tblSerde = serde;
+      }
+      else {
+        tblSerde = currPart.getTableDesc().getDeserializerClass().newInstance();
+        tblSerde.initialize(job, currPart.getTableDesc().getProperties());
+      }
+
+      ObjectInspector outputOI = ObjectInspectorConverters.getConvertedOI(
+          serde.getObjectInspector(),
+          tblSerde.getObjectInspector());
+
+      partTblObjectInspectorConverter = ObjectInspectorConverters.getConverter(
+          serde.getObjectInspector(), outputOI);
 
       if (LOG.isDebugEnabled()) {
         LOG.debug("Creating fetchTask with deserializer typeinfo: "
             + serde.getObjectInspector().getTypeName());
-        LOG.debug("deserializer properties: " + tmp.getProperties());
+        LOG.debug("deserializer properties: " + partDesc.getProperties());
       }
 
       if (currPart != null) {
-        setPrtnDesc(currPart);
+        getRowInspectorFromPartition(currPart, outputOI);
       }
     }
 
@@ -503,14 +524,15 @@ public class FetchOperator implements Serializable {
             vcValues = MapOperator.populateVirtualColumnValues(context, vcCols, vcValues, serde);
             row[isPartitioned ? 2 : 1] = vcValues;
           }
-          row[0] = serde.deserialize(value);
+          row[0] = partTblObjectInspectorConverter.convert(serde.deserialize(value));
+
           if (hasVC || isPartitioned) {
             inspectable.o = row;
             inspectable.oi = rowObjectInspector;
             return inspectable;
           }
           inspectable.o = row[0];
-          inspectable.oi = serde.getObjectInspector();
+          inspectable.oi = tblSerde.getObjectInspector();
           return inspectable;
         } else {
           currRecReader.close();
@@ -569,13 +591,33 @@ public class FetchOperator implements Serializable {
   public ObjectInspector getOutputObjectInspector() throws HiveException {
     try {
       if (work.isNotPartitioned()) {
-        return setTableDesc(work.getTblDesc());
+        return getRowInspectorFromTable(work.getTblDesc());
       }
       List<PartitionDesc> listParts = work.getPartDesc();
+      // Chose the table descriptor if none of the partitions is present.
+      // For eg: consider the query:
+      // select /*+mapjoin(T1)*/ count(*) from T1 join T2 on T1.key=T2.key
+      // Both T1 and T2 and partitioned tables, but T1 does not have any partitions
+      // FetchOperator is invoked for T1, and listParts is empty. In that case,
+      // use T1's schema to get the ObjectInspector.
       if (listParts == null || listParts.isEmpty()) {
-        return setPrtnDesc(work.getTblDesc());
+        return getRowInspectorFromPartitionedTable(work.getTblDesc());
       }
-      return setPrtnDesc(listParts.get(0));
+
+      // Choose any partition. It's OI needs to be converted to the table OI
+      // Whenever a new partition is being read, a new converter is being created
+      PartitionDesc partition = listParts.get(0);
+      Deserializer tblSerde = partition.getTableDesc().getDeserializerClass().newInstance();
+      tblSerde.initialize(job, partition.getTableDesc().getProperties());
+
+      Deserializer partSerde = partition.getDeserializerClass().newInstance();
+      partSerde.initialize(job, partition.getProperties());
+
+      ObjectInspector partitionOI = ObjectInspectorConverters.getConvertedOI(
+          partSerde.getObjectInspector(),
+          tblSerde.getObjectInspector());
+
+      return getRowInspectorFromPartition(partition, partitionOI);
     } catch (Exception e) {
       throw new HiveException("Failed with exception " + e.getMessage()
           + org.apache.hadoop.util.StringUtils.stringifyException(e));
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java b/src/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java
index c2a72b3..3083129 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java
@@ -38,6 +38,7 @@ import org.apache.hadoop.hive.ql.metadata.VirtualColumn;
 import org.apache.hadoop.hive.ql.plan.MapredWork;
 import org.apache.hadoop.hive.ql.plan.OperatorDesc;
 import org.apache.hadoop.hive.ql.plan.PartitionDesc;
+import org.apache.hadoop.hive.ql.plan.TableDesc;
 import org.apache.hadoop.hive.ql.plan.TableScanDesc;
 import org.apache.hadoop.hive.ql.plan.api.OperatorType;
 import org.apache.hadoop.hive.serde2.Deserializer;
@@ -45,6 +46,8 @@ import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeStats;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.Converter;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
@@ -78,7 +81,9 @@ public class MapOperator extends Operator<MapredWork> implements Serializable, C
   private transient Writable[] vcValues;
   private transient List<VirtualColumn> vcs;
   private transient Object[] rowWithPartAndVC;
-  private transient StructObjectInspector rowObjectInspector;
+  private transient StructObjectInspector tblRowObjectInspector;
+  // convert from partition to table schema
+  private transient Converter partTblObjectInspectorConverter;
   private transient boolean isPartitioned;
   private transient boolean hasVC;
   private Map<MapInputPath, MapOpCtx> opCtxMap;
@@ -112,9 +117,6 @@ public class MapOperator extends Operator<MapredWork> implements Serializable, C
     public boolean equals(Object o) {
       if (o instanceof MapInputPath) {
         MapInputPath mObj = (MapInputPath) o;
-        if (mObj == null) {
-          return false;
-        }
         return path.equals(mObj.path) && alias.equals(mObj.alias)
             && op.equals(mObj.op);
       }
@@ -141,15 +143,16 @@ public class MapOperator extends Operator<MapredWork> implements Serializable, C
   }
 
   private static class MapOpCtx {
-    boolean isPartitioned;
-    StructObjectInspector rawRowObjectInspector; // without partition
-    StructObjectInspector partObjectInspector; // partition
-    StructObjectInspector rowObjectInspector;
-    Object[] rowWithPart;
-    Object[] rowWithPartAndVC;
-    Deserializer deserializer;
-    public String tableName;
-    public String partName;
+    private final boolean isPartitioned;
+    private final StructObjectInspector tblRawRowObjectInspector; // without partition
+    private final StructObjectInspector partObjectInspector; // partition
+    private StructObjectInspector rowObjectInspector;
+    private final Converter partTblObjectInspectorConverter;
+    private final Object[] rowWithPart;
+    private Object[] rowWithPartAndVC;
+    private final Deserializer deserializer;
+    private String tableName;
+    private String partName;
 
     /**
      * @param isPartitioned
@@ -158,18 +161,20 @@ public class MapOperator extends Operator<MapredWork> implements Serializable, C
      */
     public MapOpCtx(boolean isPartitioned,
         StructObjectInspector rowObjectInspector,
-        StructObjectInspector rawRowObjectInspector,
+        StructObjectInspector tblRawRowObjectInspector,
         StructObjectInspector partObjectInspector,
         Object[] rowWithPart,
         Object[] rowWithPartAndVC,
-        Deserializer deserializer) {
+        Deserializer deserializer,
+        Converter partTblObjectInspectorConverter) {
       this.isPartitioned = isPartitioned;
       this.rowObjectInspector = rowObjectInspector;
-      this.rawRowObjectInspector = rawRowObjectInspector;
+      this.tblRawRowObjectInspector = tblRawRowObjectInspector;
       this.partObjectInspector = partObjectInspector;
       this.rowWithPart = rowWithPart;
       this.rowWithPartAndVC = rowWithPartAndVC;
       this.deserializer = deserializer;
+      this.partTblObjectInspectorConverter = partTblObjectInspectorConverter;
     }
 
     /**
@@ -186,6 +191,10 @@ public class MapOperator extends Operator<MapredWork> implements Serializable, C
       return rowObjectInspector;
     }
 
+    public StructObjectInspector getTblRawRowObjectInspector() {
+      return tblRawRowObjectInspector;
+    }
+
     /**
      * @return the rowWithPart
      */
@@ -206,6 +215,10 @@ public class MapOperator extends Operator<MapredWork> implements Serializable, C
     public Deserializer getDeserializer() {
       return deserializer;
     }
+
+    public Converter getPartTblObjectInspectorConverter() {
+      return partTblObjectInspectorConverter;
+    }
   }
 
   /**
@@ -225,38 +238,46 @@ public class MapOperator extends Operator<MapredWork> implements Serializable, C
   }
 
   private MapOpCtx initObjectInspector(MapredWork conf,
-      Configuration hconf, String onefile) throws HiveException,
+      Configuration hconf, String onefile, Map<TableDesc, StructObjectInspector> convertedOI)
+          throws HiveException,
       ClassNotFoundException, InstantiationException, IllegalAccessException,
       SerDeException {
-    PartitionDesc td = conf.getPathToPartitionInfo().get(onefile);
-    LinkedHashMap<String, String> partSpec = td.getPartSpec();
-    Properties tblProps = td.getProperties();
-
-    Class sdclass = td.getDeserializerClass();
-    if (sdclass == null) {
-      String className = td.getSerdeClassName();
-      if ((className == "") || (className == null)) {
+    PartitionDesc pd = conf.getPathToPartitionInfo().get(onefile);
+    LinkedHashMap<String, String> partSpec = pd.getPartSpec();
+    // Use tblProps in case of unpartitioned tables
+    Properties partProps =
+        (pd.getPartSpec() == null || pd.getPartSpec().isEmpty()) ?
+            pd.getTableDesc().getProperties() : pd.getProperties();
+
+    Class serdeclass = pd.getDeserializerClass();
+    if (serdeclass == null) {
+      String className = pd.getSerdeClassName();
+      if ((className == null) || (className.isEmpty())) {
         throw new HiveException(
             "SerDe class or the SerDe class name is not set for table: "
-                + td.getProperties().getProperty("name"));
+                + pd.getProperties().getProperty("name"));
       }
-      sdclass = hconf.getClassByName(className);
+      serdeclass = hconf.getClassByName(className);
     }
 
-    String tableName = String.valueOf(tblProps.getProperty("name"));
+    String tableName = String.valueOf(partProps.getProperty("name"));
     String partName = String.valueOf(partSpec);
-    // HiveConf.setVar(hconf, HiveConf.ConfVars.HIVETABLENAME, tableName);
-    // HiveConf.setVar(hconf, HiveConf.ConfVars.HIVEPARTITIONNAME, partName);
-    Deserializer deserializer = (Deserializer) sdclass.newInstance();
-    deserializer.initialize(hconf, tblProps);
-    StructObjectInspector rawRowObjectInspector = (StructObjectInspector) deserializer
+    Deserializer partDeserializer = (Deserializer) serdeclass.newInstance();
+    partDeserializer.initialize(hconf, partProps);
+    StructObjectInspector partRawRowObjectInspector = (StructObjectInspector) partDeserializer
         .getObjectInspector();
 
+    StructObjectInspector tblRawRowObjectInspector = convertedOI.get(pd.getTableDesc());
+
+    partTblObjectInspectorConverter =
+    ObjectInspectorConverters.getConverter(partRawRowObjectInspector,
+        tblRawRowObjectInspector);
+
     MapOpCtx opCtx = null;
     // Next check if this table has partitions and if so
     // get the list of partition names as well as allocate
     // the serdes for the partition columns
-    String pcols = tblProps
+    String pcols = partProps
         .getProperty(org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_PARTITION_COLUMNS);
     // Log LOG = LogFactory.getLog(MapOperator.class.getName());
     if (pcols != null && pcols.length() > 0) {
@@ -284,16 +305,16 @@ public class MapOperator extends Operator<MapredWork> implements Serializable, C
       rowWithPart[1] = partValues;
       StructObjectInspector rowObjectInspector = ObjectInspectorFactory
           .getUnionStructObjectInspector(Arrays
-              .asList(new StructObjectInspector[] {rawRowObjectInspector, partObjectInspector}));
+              .asList(new StructObjectInspector[] {tblRawRowObjectInspector, partObjectInspector}));
       // LOG.info("dump " + tableName + " " + partName + " " +
       // rowObjectInspector.getTypeName());
-      opCtx = new MapOpCtx(true, rowObjectInspector, rawRowObjectInspector, partObjectInspector,
-                           rowWithPart, null, deserializer);
+      opCtx = new MapOpCtx(true, rowObjectInspector, tblRawRowObjectInspector, partObjectInspector,
+                           rowWithPart, null, partDeserializer, partTblObjectInspectorConverter);
     } else {
       // LOG.info("dump2 " + tableName + " " + partName + " " +
       // rowObjectInspector.getTypeName());
-      opCtx = new MapOpCtx(false, rawRowObjectInspector, rawRowObjectInspector, null, null,
-                           null, deserializer);
+      opCtx = new MapOpCtx(false, tblRawRowObjectInspector, tblRawRowObjectInspector, null, null,
+                           null, partDeserializer, partTblObjectInspectorConverter);
     }
     opCtx.tableName = tableName;
     opCtx.partName = partName;
@@ -311,15 +332,20 @@ public class MapOperator extends Operator<MapredWork> implements Serializable, C
     isPartitioned = opCtxMap.get(inp).isPartitioned();
     rowWithPart = opCtxMap.get(inp).getRowWithPart();
     rowWithPartAndVC = opCtxMap.get(inp).getRowWithPartAndVC();
-    rowObjectInspector = opCtxMap.get(inp).getRowObjectInspector();
+    tblRowObjectInspector = opCtxMap.get(inp).getRowObjectInspector();
+    partTblObjectInspectorConverter = opCtxMap.get(inp).getPartTblObjectInspectorConverter();
     if (listInputPaths.contains(inp)) {
       return;
     }
 
     listInputPaths.add(inp);
 
+    // The op may not be a TableScan for mapjoins
+    // Consider the query: select /*+MAPJOIN(a)*/ count(*) FROM T1 a JOIN T2 b ON a.key = b.key;
+    // In that case, it will be a Select, but the rowOI need not be ammended
     if (op instanceof TableScanOperator) {
-      StructObjectInspector rawRowObjectInspector = opCtxMap.get(inp).rawRowObjectInspector;
+      StructObjectInspector tblRawRowObjectInspector =
+          opCtxMap.get(inp).getTblRawRowObjectInspector();
       StructObjectInspector partObjectInspector = opCtxMap.get(inp).partObjectInspector;
       TableScanOperator tsOp = (TableScanOperator) op;
       TableScanDesc tsDesc = tsOp.getConf();
@@ -347,22 +373,100 @@ public class MapOperator extends Operator<MapredWork> implements Serializable, C
             this.rowWithPartAndVC = new Object[2];
           }
           if (partObjectInspector == null) {
-            this.rowObjectInspector = ObjectInspectorFactory.getUnionStructObjectInspector(Arrays
+            this.tblRowObjectInspector = ObjectInspectorFactory.getUnionStructObjectInspector(Arrays
                                         .asList(new StructObjectInspector[] {
-                                            rowObjectInspector, vcStructObjectInspector}));
+                                            tblRowObjectInspector, vcStructObjectInspector}));
           } else {
-            this.rowObjectInspector = ObjectInspectorFactory.getUnionStructObjectInspector(Arrays
+            this.tblRowObjectInspector = ObjectInspectorFactory.getUnionStructObjectInspector(Arrays
                                         .asList(new StructObjectInspector[] {
-                                            rawRowObjectInspector, partObjectInspector,
+                                            tblRawRowObjectInspector, partObjectInspector,
                                             vcStructObjectInspector}));
           }
-          opCtxMap.get(inp).rowObjectInspector = this.rowObjectInspector;
+          opCtxMap.get(inp).rowObjectInspector = this.tblRowObjectInspector;
           opCtxMap.get(inp).rowWithPartAndVC = this.rowWithPartAndVC;
         }
       }
     }
   }
 
+  // Return the mapping for table descriptor to the expected table OI
+  /**
+   * Traverse all the partitions for a table, and get the OI for the table.
+   * Note that a conversion is required if any of the partition OI is different
+   * from the table OI. For eg. if the query references table T (partitions P1, P2),
+   * and P1's schema is same as T, whereas P2's scheme is different from T, conversion
+   * might be needed for both P1 and P2, since SettableOI might be needed for T
+   */
+  private Map<TableDesc, StructObjectInspector> getConvertedOI(Configuration hconf)
+      throws HiveException {
+    Map<TableDesc, StructObjectInspector> tableDescOI =
+        new HashMap<TableDesc, StructObjectInspector>();
+    Set<TableDesc> identityConverterTableDesc = new HashSet<TableDesc>();
+    try
+    {
+      for (String onefile : conf.getPathToAliases().keySet()) {
+        PartitionDesc pd = conf.getPathToPartitionInfo().get(onefile);
+        TableDesc tableDesc = pd.getTableDesc();
+        Properties tblProps = tableDesc.getProperties();
+        // If the partition does not exist, use table properties
+        Properties partProps =
+            (pd.getPartSpec() == null || pd.getPartSpec().isEmpty()) ?
+                tblProps : pd.getProperties();
+
+        Class sdclass = pd.getDeserializerClass();
+        if (sdclass == null) {
+          String className = pd.getSerdeClassName();
+          if ((className == null) || (className.isEmpty())) {
+            throw new HiveException(
+                "SerDe class or the SerDe class name is not set for table: "
+                    + pd.getProperties().getProperty("name"));
+          }
+          sdclass = hconf.getClassByName(className);
+        }
+
+        Deserializer partDeserializer = (Deserializer) sdclass.newInstance();
+        partDeserializer.initialize(hconf, partProps);
+        StructObjectInspector partRawRowObjectInspector = (StructObjectInspector) partDeserializer
+            .getObjectInspector();
+
+        StructObjectInspector tblRawRowObjectInspector = tableDescOI.get(tableDesc);
+        if ((tblRawRowObjectInspector == null) ||
+            (identityConverterTableDesc.contains(tableDesc))) {
+          sdclass = tableDesc.getDeserializerClass();
+          if (sdclass == null) {
+            String className = tableDesc.getSerdeClassName();
+            if ((className == null) || (className.isEmpty())) {
+              throw new HiveException(
+                  "SerDe class or the SerDe class name is not set for table: "
+                      + tableDesc.getProperties().getProperty("name"));
+            }
+            sdclass = hconf.getClassByName(className);
+          }
+          Deserializer tblDeserializer = (Deserializer) sdclass.newInstance();
+          tblDeserializer.initialize(hconf, tblProps);
+          tblRawRowObjectInspector =
+              (StructObjectInspector) ObjectInspectorConverters.getConvertedOI(
+                  partRawRowObjectInspector,
+                  (StructObjectInspector) tblDeserializer.getObjectInspector());
+
+          if (identityConverterTableDesc.contains(tableDesc)) {
+            if (!partRawRowObjectInspector.equals(tblRawRowObjectInspector)) {
+              identityConverterTableDesc.remove(tableDesc);
+            }
+          }
+          else if (partRawRowObjectInspector.equals(tblRawRowObjectInspector)) {
+            identityConverterTableDesc.add(tableDesc);
+          }
+
+          tableDescOI.put(tableDesc, tblRawRowObjectInspector);
+        }
+      }
+    } catch (Exception e) {
+      throw new HiveException(e);
+    }
+    return tableDescOI;
+  }
+
   public void setChildren(Configuration hconf) throws HiveException {
 
     Path fpath = new Path((new Path(HiveConf.getVar(hconf,
@@ -374,10 +478,10 @@ public class MapOperator extends Operator<MapredWork> implements Serializable, C
     operatorToPaths = new HashMap<Operator<? extends OperatorDesc>, ArrayList<String>>();
 
     statsMap.put(Counter.DESERIALIZE_ERRORS, deserialize_error_count);
-
+    Map<TableDesc, StructObjectInspector> convertedOI = getConvertedOI(hconf);
     try {
       for (String onefile : conf.getPathToAliases().keySet()) {
-        MapOpCtx opCtx = initObjectInspector(conf, hconf, onefile);
+        MapOpCtx opCtx = initObjectInspector(conf, hconf, onefile, convertedOI);
         Path onepath = new Path(new Path(onefile).toUri().getPath());
         List<String> aliases = conf.getPathToAliases().get(onefile);
 
@@ -513,16 +617,18 @@ public class MapOperator extends Operator<MapredWork> implements Serializable, C
     Object row = null;
     try {
       if (this.hasVC) {
-        this.rowWithPartAndVC[0] = deserializer.deserialize(value);
+        this.rowWithPartAndVC[0] =
+            partTblObjectInspectorConverter.convert(deserializer.deserialize(value));
         int vcPos = isPartitioned ? 2 : 1;
         if (context != null) {
           populateVirtualColumnValues(context, vcs, vcValues, deserializer);
         }
         this.rowWithPartAndVC[vcPos] = this.vcValues;
       } else if (!isPartitioned) {
-        row = deserializer.deserialize((Writable) value);
+        row = partTblObjectInspectorConverter.convert(deserializer.deserialize((Writable) value));
       } else {
-        rowWithPart[0] = deserializer.deserialize((Writable) value);
+        rowWithPart[0] =
+            partTblObjectInspectorConverter.convert(deserializer.deserialize((Writable) value));
       }
     } catch (Exception e) {
       // Serialize the row and output.
@@ -539,24 +645,26 @@ public class MapOperator extends Operator<MapredWork> implements Serializable, C
       throw new HiveException("Hive Runtime Error while processing writable " + rawRowString, e);
     }
 
+    // The row has been converted to comply with table schema, irrespective of partition schema.
+    // So, use tblOI (and not partOI) for forwarding
     try {
       if (this.hasVC) {
-        forward(this.rowWithPartAndVC, this.rowObjectInspector);
+        forward(this.rowWithPartAndVC, this.tblRowObjectInspector);
       } else if (!isPartitioned) {
-        forward(row, rowObjectInspector);
+        forward(row, tblRowObjectInspector);
       } else {
-        forward(rowWithPart, rowObjectInspector);
+        forward(rowWithPart, tblRowObjectInspector);
       }
     } catch (Exception e) {
       // Serialize the row and output the error message.
       String rowString;
       try {
         if (this.hasVC) {
-          rowString = SerDeUtils.getJSONString(rowWithPartAndVC, rowObjectInspector);
+          rowString = SerDeUtils.getJSONString(rowWithPartAndVC, tblRowObjectInspector);
         } else if (!isPartitioned) {
-          rowString = SerDeUtils.getJSONString(row, rowObjectInspector);
+          rowString = SerDeUtils.getJSONString(row, tblRowObjectInspector);
         } else {
-          rowString = SerDeUtils.getJSONString(rowWithPart, rowObjectInspector);
+          rowString = SerDeUtils.getJSONString(rowWithPart, tblRowObjectInspector);
         }
       } catch (Exception e2) {
         rowString = "[Error getting row data with exception " +
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java b/src/ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java
index 97d6d18..6e9b0ae 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java
@@ -30,6 +30,7 @@ import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.common.ObjectPair;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.exec.persistence.RowContainer;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
@@ -40,7 +41,6 @@ import org.apache.hadoop.hive.ql.plan.MapredLocalWork;
 import org.apache.hadoop.hive.ql.plan.OperatorDesc;
 import org.apache.hadoop.hive.ql.plan.SMBJoinDesc;
 import org.apache.hadoop.hive.ql.plan.api.OperatorType;
-import org.apache.hadoop.hive.ql.util.ObjectPair;
 import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.InspectableObject;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java b/src/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
index 47fe32f..8619877 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
@@ -118,8 +118,8 @@ import org.apache.hadoop.hive.ql.plan.MapredLocalWork;
 import org.apache.hadoop.hive.ql.plan.MapredWork;
 import org.apache.hadoop.hive.ql.plan.PartitionDesc;
 import org.apache.hadoop.hive.ql.plan.PlanUtils;
-import org.apache.hadoop.hive.ql.plan.TableDesc;
 import org.apache.hadoop.hive.ql.plan.PlanUtils.ExpressionTypes;
+import org.apache.hadoop.hive.ql.plan.TableDesc;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.ql.stats.StatsFactory;
 import org.apache.hadoop.hive.ql.stats.StatsPublisher;
@@ -135,8 +135,8 @@ import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
 import org.apache.hadoop.hive.shims.ShimLoader;
 import org.apache.hadoop.io.IOUtils;
 import org.apache.hadoop.io.SequenceFile;
-import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.SequenceFile.CompressionType;
+import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.compress.CompressionCodec;
 import org.apache.hadoop.io.compress.DefaultCodec;
 import org.apache.hadoop.mapred.FileOutputFormat;
@@ -557,30 +557,6 @@ public final class Utilities {
     }
   }
 
-  /**
-   * Tuple.
-   *
-   * @param <T>
-   * @param <V>
-   */
-  public static class Tuple<T, V> {
-    private final T one;
-    private final V two;
-
-    public Tuple(T one, V two) {
-      this.one = one;
-      this.two = two;
-    }
-
-    public T getOne() {
-      return this.one;
-    }
-
-    public V getTwo() {
-      return this.two;
-    }
-  }
-
   public static TableDesc defaultTd;
   static {
     // by default we expect ^A separated strings
@@ -691,7 +667,7 @@ public final class Utilities {
 
   public static TableDesc getTableDesc(Table tbl) {
     return (new TableDesc(tbl.getDeserializer().getClass(), tbl.getInputFormatClass(), tbl
-        .getOutputFormatClass(), tbl.getSchema()));
+        .getOutputFormatClass(), tbl.getMetadata()));
   }
 
   // column names and column types are all delimited by comma
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java b/src/ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java
index 4a7db53..407bc27 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java
@@ -214,7 +214,7 @@ public class Partition implements Serializable {
     getInputFormatClass();
     // This will set up field: outputFormatClass
     getOutputFormatClass();
-
+    getDeserializer();
   }
 
   public String getName() {
@@ -276,6 +276,10 @@ public class Partition implements Serializable {
     return MetaStoreUtils.getSchema(tPartition, table.getTTable());
   }
 
+  public Properties getMetadataFromPartitionSchema() {
+    return MetaStoreUtils.getPartitionMetadata(tPartition, table.getTTable());
+  }
+
   public Properties getSchemaFromTableSchema(Properties tblSchema) {
     return MetaStoreUtils.getPartSchemaFromTableSchema(tPartition.getSd(), table.getTTable().getSd(),
         tPartition.getParameters(), table.getDbName(), table.getTableName(), table.getPartitionKeys(),
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java b/src/ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java
index bd67e36..36239ec 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java
@@ -228,8 +228,8 @@ public class Table implements Serializable {
     tTable.getSd().setOutputFormat(outputFormatClass.getName());
   }
 
-  final public Properties getSchema() {
-    return MetaStoreUtils.getSchema(tTable);
+  final public Properties getMetadata() {
+    return MetaStoreUtils.getTableMetadata(tTable);
   }
 
   final public Path getPath() {
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java b/src/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
index 4cd161e..e023419 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
@@ -822,7 +822,12 @@ public final class GenMapRedUtils {
 
         partDir.add(p);
         try {
-          partDesc.add(Utilities.getPartitionDescFromTableDesc(tblDesc, part));
+          if (part.getTable().isPartitioned()) {
+            partDesc.add(Utilities.getPartitionDesc(part));
+          }
+          else {
+            partDesc.add(Utilities.getPartitionDescFromTableDesc(tblDesc, part));
+          }
         } catch (HiveException e) {
           LOG.error(org.apache.hadoop.util.StringUtils.stringifyException(e));
           throw new SemanticException(e.getMessage(), e);
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
index 5fec96e..4d0bed3 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
@@ -40,6 +40,7 @@ import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.PathFilter;
 import org.apache.hadoop.hive.common.FileUtils;
 import org.apache.hadoop.hive.common.JavaUtils;
+import org.apache.hadoop.hive.common.ObjectPair;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hadoop.hive.metastore.TableType;
@@ -162,7 +163,6 @@ import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.Mode;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFHash;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPOr;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;
-import org.apache.hadoop.hive.ql.util.ObjectPair;
 import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.hive.serde2.Deserializer;
 import org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe;
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/plan/PartitionDesc.java b/src/ql/src/java/org/apache/hadoop/hive/ql/plan/PartitionDesc.java
index 9e470a8..f0b16e4 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/plan/PartitionDesc.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/plan/PartitionDesc.java
@@ -87,7 +87,7 @@ public class PartitionDesc implements Serializable, Cloneable {
   public PartitionDesc(final org.apache.hadoop.hive.ql.metadata.Partition part)
       throws HiveException {
     tableDesc = Utilities.getTableDesc(part.getTable());
-    properties = part.getSchema();
+    properties = part.getMetadataFromPartitionSchema();
     partSpec = part.getSpec();
     deserializerClass = part.getDeserializer(properties).getClass();
     inputFileFormatClass = part.getInputFormatClass();
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/plan/TableDesc.java b/src/ql/src/java/org/apache/hadoop/hive/ql/plan/TableDesc.java
index 5f3ab00..a34e89e 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/plan/TableDesc.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/plan/TableDesc.java
@@ -20,9 +20,9 @@ package org.apache.hadoop.hive.ql.plan;
 
 import java.io.Serializable;
 import java.util.Enumeration;
-import java.util.Properties;
 import java.util.LinkedHashMap;
 import java.util.Map;
+import java.util.Properties;
 
 import org.apache.hadoop.hive.ql.io.HiveFileFormatUtils;
 import org.apache.hadoop.hive.ql.io.HiveOutputFormat;
@@ -149,7 +149,7 @@ public class TableDesc implements Serializable, Cloneable {
         org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_STORAGE)
       != null);
   }
-  
+
   @Override
   public Object clone() {
     TableDesc ret = new TableDesc();
@@ -170,4 +170,42 @@ public class TableDesc implements Serializable, Cloneable {
     }
     return ret;
   }
+
+  @Override
+  public int hashCode() {
+    final int prime = 31;
+    int result = 1;
+    result = prime * result + ((deserializerClass == null) ? 0 : deserializerClass.hashCode());
+    result = prime * result +
+        ((inputFileFormatClass == null) ? 0 : inputFileFormatClass.hashCode());
+    result = prime * result +
+        ((outputFileFormatClass == null) ? 0 : outputFileFormatClass.hashCode());
+    result = prime * result + ((properties == null) ? 0 : properties.hashCode());
+    result = prime * result + ((serdeClassName == null) ? 0 : serdeClassName.hashCode());
+    result = prime * result + ((jobProperties == null) ? 0 : jobProperties.hashCode());
+    return result;
+  }
+
+  @Override
+  public boolean equals(Object o) {
+    if (!(o instanceof TableDesc)) {
+      return false;
+    }
+
+    TableDesc target = (TableDesc) o;
+    boolean ret = true;
+    ret = ret && (deserializerClass == null ? target.deserializerClass == null :
+      deserializerClass.equals(target.deserializerClass));
+    ret = ret && (inputFileFormatClass == null ? target.inputFileFormatClass == null :
+      inputFileFormatClass.equals(target.inputFileFormatClass));
+    ret = ret && (outputFileFormatClass == null ? target.outputFileFormatClass == null :
+      outputFileFormatClass.equals(target.outputFileFormatClass));
+    ret = ret && (properties == null ? target.properties == null :
+      properties.equals(target.properties));
+    ret = ret && (serdeClassName == null ? target.serdeClassName == null :
+      serdeClassName.equals(target.serdeClassName));
+    ret = ret && (jobProperties == null ? target.jobProperties == null :
+      jobProperties.equals(target.jobProperties));
+    return ret;
+  }
 }
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/util/ObjectPair.java b/src/ql/src/java/org/apache/hadoop/hive/ql/util/ObjectPair.java
deleted file mode 100644
index 185bcb8..0000000
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/util/ObjectPair.java
+++ /dev/null
@@ -1,47 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.util;
-
-public class ObjectPair<F, S> {
-  private F first;
-  private S second;
-
-  public ObjectPair() {}
-
-  public ObjectPair(F first, S second) {
-    this.first = first;
-    this.second = second;
-  }
-
-  public F getFirst() {
-    return first;
-  }
-
-  public void setFirst(F first) {
-    this.first = first;
-  }
-
-  public S getSecond() {
-    return second;
-  }
-
-  public void setSecond(S second) {
-    this.second = second;
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/metadata/TestPartition.java b/src/ql/src/test/org/apache/hadoop/hive/ql/metadata/TestPartition.java
deleted file mode 100644
index 5133972..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/metadata/TestPartition.java
+++ /dev/null
@@ -1,68 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.ql.metadata;
-
-import java.net.URI;
-import java.net.URISyntaxException;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Map;
-
-import junit.framework.TestCase;
-
-import org.apache.hadoop.hive.metastore.api.FieldSchema;
-import org.apache.hadoop.hive.metastore.api.Partition;
-import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
-
-/**
- * Test the partition class.
- */
-public class TestPartition extends TestCase {
-
-  private static final String PARTITION_COL = "partcol";
-  private static final String PARTITION_VALUE = "value";
-  private static final String TABLENAME = "tablename";
-
-  /**
-   * Test that the Partition spec is created properly.
-   */
-  public void testPartition() throws HiveException, URISyntaxException {
-    StorageDescriptor sd = new StorageDescriptor();
-    sd.setLocation("partlocation");
-
-    Partition tp = new Partition();
-    tp.setTableName(TABLENAME);
-    tp.setSd(sd);
-
-    List<String> values = new ArrayList<String>();
-    values.add(PARTITION_VALUE);
-    tp.setValues(values);
-
-    List<FieldSchema> partCols = new ArrayList<FieldSchema>();
-    partCols.add(new FieldSchema(PARTITION_COL, "string", ""));
-
-    Table tbl = new Table("default", TABLENAME);
-    tbl.setDataLocation(new URI("tmplocation"));
-    tbl.setPartCols(partCols);
-
-    Map<String, String> spec = new org.apache.hadoop.hive.ql.metadata.Partition(tbl, tp).getSpec();
-    assertFalse(spec.isEmpty());
-    assertEquals(spec.get(PARTITION_COL), PARTITION_VALUE);
-  }
-
-}
diff --git a/src/ql/src/test/queries/clientpositive/partition_wise_fileformat10.q b/src/ql/src/test/queries/clientpositive/partition_wise_fileformat10.q
new file mode 100644
index 0000000..f15f72c
--- /dev/null
+++ b/src/ql/src/test/queries/clientpositive/partition_wise_fileformat10.q
@@ -0,0 +1,13 @@
+set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
+
+-- This tests that the schema can be changed for binary serde data
+create table prt(key string, value string) partitioned by (dt string);
+insert overwrite table prt partition(dt='1') select * from src where key = 238;
+
+select * from prt where dt is not null;
+select key+key, value from prt where dt is not null;
+
+alter table prt add columns (value2 string);
+
+select key+key, value from prt where dt is not null;
+select * from prt where dt is not null;
diff --git a/src/ql/src/test/queries/clientpositive/partition_wise_fileformat11.q b/src/ql/src/test/queries/clientpositive/partition_wise_fileformat11.q
new file mode 100644
index 0000000..1a4291f
--- /dev/null
+++ b/src/ql/src/test/queries/clientpositive/partition_wise_fileformat11.q
@@ -0,0 +1,19 @@
+set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
+
+-- This tests that the schema can be changed for binary serde data
+create table partition_test_partitioned(key string, value string) partitioned by (dt string) stored as rcfile;
+alter table partition_test_partitioned set serde 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe';
+insert overwrite table partition_test_partitioned partition(dt='1') select * from src where key = 238;
+
+select * from partition_test_partitioned where dt is not null;
+select key+key, value from partition_test_partitioned where dt is not null;
+
+alter table partition_test_partitioned change key key int;
+
+select key+key, value from partition_test_partitioned where dt is not null;
+select * from partition_test_partitioned where dt is not null;
+
+alter table partition_test_partitioned add columns (value2 string);
+
+select key+key, value from partition_test_partitioned where dt is not null;
+select * from partition_test_partitioned where dt is not null;
diff --git a/src/ql/src/test/queries/clientpositive/partition_wise_fileformat12.q b/src/ql/src/test/queries/clientpositive/partition_wise_fileformat12.q
new file mode 100644
index 0000000..bc51cb5
--- /dev/null
+++ b/src/ql/src/test/queries/clientpositive/partition_wise_fileformat12.q
@@ -0,0 +1,26 @@
+set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
+
+-- This tests that the schema can be changed for binary serde data
+create table partition_test_partitioned(key string, value string) partitioned by (dt string) stored as rcfile;
+alter table partition_test_partitioned set serde 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe';
+insert overwrite table partition_test_partitioned partition(dt='1') select * from src where key = 238;
+
+select * from partition_test_partitioned where dt is not null;
+select key+key, value from partition_test_partitioned where dt is not null;
+
+alter table partition_test_partitioned change key key int;
+
+select key+key, value from partition_test_partitioned where dt is not null;
+select * from partition_test_partitioned where dt is not null;
+
+insert overwrite table partition_test_partitioned partition(dt='2') select * from src where key = 97;
+
+alter table partition_test_partitioned add columns (value2 string);
+
+select key+key, value from partition_test_partitioned where dt is not null;
+select * from partition_test_partitioned where dt is not null;
+
+insert overwrite table partition_test_partitioned partition(dt='3') select key, value, value from src where key = 200;
+
+select key+key, value, value2 from partition_test_partitioned where dt is not null;
+select * from partition_test_partitioned where dt is not null;
diff --git a/src/ql/src/test/queries/clientpositive/partition_wise_fileformat13.q b/src/ql/src/test/queries/clientpositive/partition_wise_fileformat13.q
new file mode 100644
index 0000000..2e4ae69
--- /dev/null
+++ b/src/ql/src/test/queries/clientpositive/partition_wise_fileformat13.q
@@ -0,0 +1,17 @@
+set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
+
+-- This tests that the schema can be changed for partitioned tables for binary serde data for joins
+create table T1(key string, value string) partitioned by (dt string) stored as rcfile;
+alter table T1 set serde 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe';
+insert overwrite table T1 partition (dt='1') select * from src where key = 238 or key = 97;
+
+alter table T1 change key key int;
+insert overwrite table T1 partition (dt='2') select * from src where key = 238 or key = 97;
+
+alter table T1 change key key string;
+
+create table T2(key string, value string) partitioned by (dt string) stored as rcfile;
+insert overwrite table T2 partition (dt='1') select * from src where key = 238 or key = 97;
+
+select /* + MAPJOIN(a) */ count(*) FROM T1 a JOIN T2 b ON a.key = b.key;
+select count(*) FROM T1 a JOIN T2 b ON a.key = b.key;
\ No newline at end of file
diff --git a/src/ql/src/test/queries/clientpositive/partition_wise_fileformat14.q b/src/ql/src/test/queries/clientpositive/partition_wise_fileformat14.q
new file mode 100644
index 0000000..f4d4d73
--- /dev/null
+++ b/src/ql/src/test/queries/clientpositive/partition_wise_fileformat14.q
@@ -0,0 +1,57 @@
+set hive.enforce.bucketing = true;
+set hive.enforce.sorting = true;
+set hive.exec.reducers.max = 1;
+
+CREATE TABLE tbl1(key int, value string) PARTITIONED by (ds string)
+CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS rcfile;
+CREATE TABLE tbl2(key int, value string) PARTITIONED by (ds string)
+CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS rcfile;
+
+alter table tbl1 set serde 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe';
+alter table tbl2 set serde 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe';
+
+insert overwrite table tbl1 partition (ds='1') select * from src where key < 10;
+insert overwrite table tbl2 partition (ds='1') select * from src where key < 10;
+
+alter table tbl1 change key key int;
+insert overwrite table tbl1 partition (ds='2') select * from src where key < 10;
+
+alter table tbl1 change key key string;
+
+-- The subquery itself is being map-joined. Multiple partitions of tbl1 with different schemas are being read for tbl2
+select /*+mapjoin(subq1)*/ count(*) from 
+  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1 
+    join
+  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
+  on subq1.key = subq2.key;
+
+set hive.optimize.bucketmapjoin = true;
+set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
+
+-- The subquery itself is being map-joined. Since the sub-query only contains selects and filters, it should 
+-- be converted to a bucketized mapside join. Multiple partitions of tbl1 with different schemas are being read for each
+-- bucket of tbl2
+select /*+mapjoin(subq1)*/ count(*) from 
+  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1 
+    join
+  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
+  on subq1.key = subq2.key;
+
+set hive.optimize.bucketmapjoin.sortedmerge = true;
+
+-- The subquery itself is being map-joined. Since the sub-query only contains selects and filters, it should 
+-- be converted to a sort-merge join. Multiple partitions of tbl1 with different schemas are being read for a
+-- given file of tbl2
+select /*+mapjoin(subq1)*/ count(*) from 
+  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1 
+    join
+  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
+  on subq1.key = subq2.key;
+
+-- Since the join key is modified by the sub-query, neither sort-merge join not bucketized map-side
+-- join should be performed.  Multiple partitions of tbl1 with different schemas are being read for tbl2
+select /*+mapjoin(subq1)*/ count(*) from 
+  (select a.key+1 as key, concat(a.value, a.value) as value from tbl1 a) subq1 
+    join
+  (select a.key+1 as key, concat(a.value, a.value) as value from tbl2 a) subq2
+  on subq1.key = subq2.key;
diff --git a/src/ql/src/test/queries/clientpositive/partition_wise_fileformat8.q b/src/ql/src/test/queries/clientpositive/partition_wise_fileformat8.q
new file mode 100644
index 0000000..46ea10f
--- /dev/null
+++ b/src/ql/src/test/queries/clientpositive/partition_wise_fileformat8.q
@@ -0,0 +1,13 @@
+set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
+
+-- This tests that a query can span multiple partitions which can not only have different file formats, but
+-- also different serdes
+create table partition_test_partitioned(key string, value string) partitioned by (dt string) stored as rcfile;
+insert overwrite table partition_test_partitioned partition(dt='1') select * from src;
+alter table partition_test_partitioned set fileformat sequencefile;
+insert overwrite table partition_test_partitioned partition(dt='2') select * from src;
+alter table partition_test_partitioned set serde 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe';
+insert overwrite table partition_test_partitioned partition(dt='3') select * from src;
+
+select * from partition_test_partitioned where dt is not null order by key, value, dt limit 20;
+select key+key as key, value, dt from partition_test_partitioned where dt is not null order by key, value, dt limit 20;
diff --git a/src/ql/src/test/queries/clientpositive/partition_wise_fileformat9.q b/src/ql/src/test/queries/clientpositive/partition_wise_fileformat9.q
new file mode 100644
index 0000000..5205585
--- /dev/null
+++ b/src/ql/src/test/queries/clientpositive/partition_wise_fileformat9.q
@@ -0,0 +1,12 @@
+set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
+
+-- This tests that a query can span multiple partitions which can not only have different file formats, but
+-- also different serdes
+create table partition_test_partitioned(key string, value string) partitioned by (dt string) stored as rcfile;
+insert overwrite table partition_test_partitioned partition(dt='1') select * from src;
+alter table partition_test_partitioned set serde 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe';
+insert overwrite table partition_test_partitioned partition(dt='2') select * from src;
+
+select * from partition_test_partitioned where dt is not null order by key, value, dt limit 20;
+select key+key as key, value, dt from partition_test_partitioned where dt is not null order by key, value, dt limit 20;
+
diff --git a/src/ql/src/test/results/clientpositive/bucketcontext_1.q.out b/src/ql/src/test/results/clientpositive/bucketcontext_1.q.out
index 9abca74..b58aa3f 100644
--- a/src/ql/src/test/results/clientpositive/bucketcontext_1.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketcontext_1.q.out
@@ -162,7 +162,6 @@ STAGE PLANS:
             partition values:
               ds 2008-04-08
             properties:
-              SORTBUCKETCOLSPREFIX TRUE
               bucket_count 4
               bucket_field_name key
               columns key,value
@@ -170,7 +169,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.bucket_big
               numFiles 4
-              numPartitions 2
               numRows 0
               partition_columns ds
               rawDataSize 0
@@ -212,7 +210,6 @@ STAGE PLANS:
             partition values:
               ds 2008-04-09
             properties:
-              SORTBUCKETCOLSPREFIX TRUE
               bucket_count 4
               bucket_field_name key
               columns key,value
@@ -220,7 +217,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.bucket_big
               numFiles 4
-              numPartitions 2
               numRows 0
               partition_columns ds
               rawDataSize 0
@@ -408,7 +404,6 @@ STAGE PLANS:
             partition values:
               ds 2008-04-08
             properties:
-              SORTBUCKETCOLSPREFIX TRUE
               bucket_count 4
               bucket_field_name key
               columns key,value
@@ -416,7 +411,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.bucket_big
               numFiles 4
-              numPartitions 2
               numRows 0
               partition_columns ds
               rawDataSize 0
@@ -458,7 +452,6 @@ STAGE PLANS:
             partition values:
               ds 2008-04-09
             properties:
-              SORTBUCKETCOLSPREFIX TRUE
               bucket_count 4
               bucket_field_name key
               columns key,value
@@ -466,7 +459,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.bucket_big
               numFiles 4
-              numPartitions 2
               numRows 0
               partition_columns ds
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/bucketcontext_2.q.out b/src/ql/src/test/results/clientpositive/bucketcontext_2.q.out
index 308f61f..8eabe0c 100644
--- a/src/ql/src/test/results/clientpositive/bucketcontext_2.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketcontext_2.q.out
@@ -150,7 +150,6 @@ STAGE PLANS:
             partition values:
               ds 2008-04-08
             properties:
-              SORTBUCKETCOLSPREFIX TRUE
               bucket_count 2
               bucket_field_name key
               columns key,value
@@ -158,7 +157,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.bucket_big
               numFiles 2
-              numPartitions 2
               numRows 0
               partition_columns ds
               rawDataSize 0
@@ -200,7 +198,6 @@ STAGE PLANS:
             partition values:
               ds 2008-04-09
             properties:
-              SORTBUCKETCOLSPREFIX TRUE
               bucket_count 2
               bucket_field_name key
               columns key,value
@@ -208,7 +205,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.bucket_big
               numFiles 2
-              numPartitions 2
               numRows 0
               partition_columns ds
               rawDataSize 0
@@ -396,7 +392,6 @@ STAGE PLANS:
             partition values:
               ds 2008-04-08
             properties:
-              SORTBUCKETCOLSPREFIX TRUE
               bucket_count 2
               bucket_field_name key
               columns key,value
@@ -404,7 +399,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.bucket_big
               numFiles 2
-              numPartitions 2
               numRows 0
               partition_columns ds
               rawDataSize 0
@@ -446,7 +440,6 @@ STAGE PLANS:
             partition values:
               ds 2008-04-09
             properties:
-              SORTBUCKETCOLSPREFIX TRUE
               bucket_count 2
               bucket_field_name key
               columns key,value
@@ -454,7 +447,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.bucket_big
               numFiles 2
-              numPartitions 2
               numRows 0
               partition_columns ds
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/bucketcontext_3.q.out b/src/ql/src/test/results/clientpositive/bucketcontext_3.q.out
index df6d6e3..5605e33 100644
--- a/src/ql/src/test/results/clientpositive/bucketcontext_3.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketcontext_3.q.out
@@ -150,7 +150,6 @@ STAGE PLANS:
             partition values:
               ds 2008-04-08
             properties:
-              SORTBUCKETCOLSPREFIX TRUE
               bucket_count 4
               bucket_field_name key
               columns key,value
@@ -158,7 +157,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.bucket_big
               numFiles 4
-              numPartitions 1
               numRows 0
               partition_columns ds
               rawDataSize 0
@@ -345,7 +343,6 @@ STAGE PLANS:
             partition values:
               ds 2008-04-08
             properties:
-              SORTBUCKETCOLSPREFIX TRUE
               bucket_count 4
               bucket_field_name key
               columns key,value
@@ -353,7 +350,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.bucket_big
               numFiles 4
-              numPartitions 1
               numRows 0
               partition_columns ds
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/bucketcontext_4.q.out b/src/ql/src/test/results/clientpositive/bucketcontext_4.q.out
index a524a0e..1707b89 100644
--- a/src/ql/src/test/results/clientpositive/bucketcontext_4.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketcontext_4.q.out
@@ -162,7 +162,6 @@ STAGE PLANS:
             partition values:
               ds 2008-04-08
             properties:
-              SORTBUCKETCOLSPREFIX TRUE
               bucket_count 2
               bucket_field_name key
               columns key,value
@@ -170,7 +169,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.bucket_big
               numFiles 2
-              numPartitions 1
               numRows 0
               partition_columns ds
               rawDataSize 0
@@ -357,7 +355,6 @@ STAGE PLANS:
             partition values:
               ds 2008-04-08
             properties:
-              SORTBUCKETCOLSPREFIX TRUE
               bucket_count 2
               bucket_field_name key
               columns key,value
@@ -365,7 +362,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.bucket_big
               numFiles 2
-              numPartitions 1
               numRows 0
               partition_columns ds
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/bucketcontext_6.q.out b/src/ql/src/test/results/clientpositive/bucketcontext_6.q.out
index 76461a2..2417bea 100644
--- a/src/ql/src/test/results/clientpositive/bucketcontext_6.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketcontext_6.q.out
@@ -149,7 +149,6 @@ STAGE PLANS:
             partition values:
               ds 2008-04-08
             properties:
-              SORTBUCKETCOLSPREFIX TRUE
               bucket_count 2
               bucket_field_name key
               columns key,value
@@ -157,7 +156,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.bucket_big
               numFiles 2
-              numPartitions 2
               numRows 0
               partition_columns ds
               rawDataSize 0
@@ -199,7 +197,6 @@ STAGE PLANS:
             partition values:
               ds 2008-04-09
             properties:
-              SORTBUCKETCOLSPREFIX TRUE
               bucket_count 2
               bucket_field_name key
               columns key,value
@@ -207,7 +204,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.bucket_big
               numFiles 2
-              numPartitions 2
               numRows 0
               partition_columns ds
               rawDataSize 0
@@ -393,7 +389,6 @@ STAGE PLANS:
             partition values:
               ds 2008-04-08
             properties:
-              SORTBUCKETCOLSPREFIX TRUE
               bucket_count 2
               bucket_field_name key
               columns key,value
@@ -401,7 +396,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.bucket_big
               numFiles 2
-              numPartitions 2
               numRows 0
               partition_columns ds
               rawDataSize 0
@@ -443,7 +437,6 @@ STAGE PLANS:
             partition values:
               ds 2008-04-09
             properties:
-              SORTBUCKETCOLSPREFIX TRUE
               bucket_count 2
               bucket_field_name key
               columns key,value
@@ -451,7 +444,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.bucket_big
               numFiles 2
-              numPartitions 2
               numRows 0
               partition_columns ds
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/bucketcontext_7.q.out b/src/ql/src/test/results/clientpositive/bucketcontext_7.q.out
index c9c4eed..f3d0f08 100644
--- a/src/ql/src/test/results/clientpositive/bucketcontext_7.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketcontext_7.q.out
@@ -175,7 +175,6 @@ STAGE PLANS:
             partition values:
               ds 2008-04-08
             properties:
-              SORTBUCKETCOLSPREFIX TRUE
               bucket_count 2
               bucket_field_name key
               columns key,value
@@ -183,7 +182,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.bucket_big
               numFiles 2
-              numPartitions 2
               numRows 0
               partition_columns ds
               rawDataSize 0
@@ -225,7 +223,6 @@ STAGE PLANS:
             partition values:
               ds 2008-04-09
             properties:
-              SORTBUCKETCOLSPREFIX TRUE
               bucket_count 2
               bucket_field_name key
               columns key,value
@@ -233,7 +230,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.bucket_big
               numFiles 2
-              numPartitions 2
               numRows 0
               partition_columns ds
               rawDataSize 0
@@ -423,7 +419,6 @@ STAGE PLANS:
             partition values:
               ds 2008-04-08
             properties:
-              SORTBUCKETCOLSPREFIX TRUE
               bucket_count 2
               bucket_field_name key
               columns key,value
@@ -431,7 +426,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.bucket_big
               numFiles 2
-              numPartitions 2
               numRows 0
               partition_columns ds
               rawDataSize 0
@@ -473,7 +467,6 @@ STAGE PLANS:
             partition values:
               ds 2008-04-09
             properties:
-              SORTBUCKETCOLSPREFIX TRUE
               bucket_count 2
               bucket_field_name key
               columns key,value
@@ -481,7 +474,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.bucket_big
               numFiles 2
-              numPartitions 2
               numRows 0
               partition_columns ds
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/bucketcontext_8.q.out b/src/ql/src/test/results/clientpositive/bucketcontext_8.q.out
index 9342f61..365b718 100644
--- a/src/ql/src/test/results/clientpositive/bucketcontext_8.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketcontext_8.q.out
@@ -175,7 +175,6 @@ STAGE PLANS:
             partition values:
               ds 2008-04-08
             properties:
-              SORTBUCKETCOLSPREFIX TRUE
               bucket_count 4
               bucket_field_name key
               columns key,value
@@ -183,7 +182,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.bucket_big
               numFiles 4
-              numPartitions 2
               numRows 0
               partition_columns ds
               rawDataSize 0
@@ -225,7 +223,6 @@ STAGE PLANS:
             partition values:
               ds 2008-04-09
             properties:
-              SORTBUCKETCOLSPREFIX TRUE
               bucket_count 4
               bucket_field_name key
               columns key,value
@@ -233,7 +230,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.bucket_big
               numFiles 4
-              numPartitions 2
               numRows 0
               partition_columns ds
               rawDataSize 0
@@ -423,7 +419,6 @@ STAGE PLANS:
             partition values:
               ds 2008-04-08
             properties:
-              SORTBUCKETCOLSPREFIX TRUE
               bucket_count 4
               bucket_field_name key
               columns key,value
@@ -431,7 +426,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.bucket_big
               numFiles 4
-              numPartitions 2
               numRows 0
               partition_columns ds
               rawDataSize 0
@@ -473,7 +467,6 @@ STAGE PLANS:
             partition values:
               ds 2008-04-09
             properties:
-              SORTBUCKETCOLSPREFIX TRUE
               bucket_count 4
               bucket_field_name key
               columns key,value
@@ -481,7 +474,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.bucket_big
               numFiles 4
-              numPartitions 2
               numRows 0
               partition_columns ds
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/bucketmapjoin1.q.out b/src/ql/src/test/results/clientpositive/bucketmapjoin1.q.out
index 30fe946..fdb1233 100644
--- a/src/ql/src/test/results/clientpositive/bucketmapjoin1.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketmapjoin1.q.out
@@ -982,7 +982,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcbucket_mapjoin_part
               numFiles 4
-              numPartitions 1
               numRows 0
               partition_columns ds
               rawDataSize 0
@@ -1226,7 +1225,6 @@ STAGE PLANS:
           hdfs directory: true
 #### A masked pattern was here ####
 
-
 PREHOOK: query: insert overwrite table bucketmapjoin_tmp_result 
 select /*+mapjoin(a)*/ a.key, a.value, b.value 
 from srcbucket_mapjoin a join srcbucket_mapjoin_part b 
diff --git a/src/ql/src/test/results/clientpositive/bucketmapjoin10.q.out b/src/ql/src/test/results/clientpositive/bucketmapjoin10.q.out
index 3be3fff..10192ca 100644
--- a/src/ql/src/test/results/clientpositive/bucketmapjoin10.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketmapjoin10.q.out
@@ -199,7 +199,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcbucket_mapjoin_part_1
               numFiles 2
-              numPartitions 2
               numRows 0
               partition_columns part
               rawDataSize 0
@@ -247,7 +246,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcbucket_mapjoin_part_1
               numFiles 3
-              numPartitions 2
               numRows 0
               partition_columns part
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/bucketmapjoin11.q.out b/src/ql/src/test/results/clientpositive/bucketmapjoin11.q.out
index 0beefd1..ab62300 100644
--- a/src/ql/src/test/results/clientpositive/bucketmapjoin11.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketmapjoin11.q.out
@@ -212,7 +212,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcbucket_mapjoin_part_1
               numFiles 2
-              numPartitions 2
               numRows 0
               partition_columns part
               rawDataSize 0
@@ -260,7 +259,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcbucket_mapjoin_part_1
               numFiles 4
-              numPartitions 2
               numRows 0
               partition_columns part
               rawDataSize 0
@@ -497,7 +495,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcbucket_mapjoin_part_1
               numFiles 2
-              numPartitions 2
               numRows 0
               partition_columns part
               rawDataSize 0
@@ -545,7 +542,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcbucket_mapjoin_part_1
               numFiles 4
-              numPartitions 2
               numRows 0
               partition_columns part
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/bucketmapjoin12.q.out b/src/ql/src/test/results/clientpositive/bucketmapjoin12.q.out
index 20fc43a..2a2c80a 100644
--- a/src/ql/src/test/results/clientpositive/bucketmapjoin12.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketmapjoin12.q.out
@@ -180,7 +180,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcbucket_mapjoin_part_1
               numFiles 2
-              numPartitions 1
               numRows 0
               partition_columns part
               rawDataSize 0
@@ -409,7 +408,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcbucket_mapjoin_part_1
               numFiles 2
-              numPartitions 1
               numRows 0
               partition_columns part
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/bucketmapjoin13.q.out b/src/ql/src/test/results/clientpositive/bucketmapjoin13.q.out
index 9e501c5..5a90ce3 100644
--- a/src/ql/src/test/results/clientpositive/bucketmapjoin13.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketmapjoin13.q.out
@@ -179,7 +179,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcbucket_mapjoin_part_1
               numFiles 2
-              numPartitions 2
               numRows 500
               partition_columns part
               rawDataSize 5312
@@ -227,7 +226,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcbucket_mapjoin_part_1
               numFiles 2
-              numPartitions 2
               numRows 500
               partition_columns part
               rawDataSize 5312
@@ -478,7 +476,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcbucket_mapjoin_part_1
               numFiles 2
-              numPartitions 2
               numRows 500
               partition_columns part
               rawDataSize 5312
@@ -740,7 +737,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcbucket_mapjoin_part_1
               numFiles 2
-              numPartitions 2
               numRows 500
               partition_columns part
               rawDataSize 5312
@@ -1004,7 +1000,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcbucket_mapjoin_part_1
               numFiles 2
-              numPartitions 2
               numRows 500
               partition_columns part
               rawDataSize 5312
diff --git a/src/ql/src/test/results/clientpositive/bucketmapjoin2.q.out b/src/ql/src/test/results/clientpositive/bucketmapjoin2.q.out
index 1d32277..4cbd281 100644
--- a/src/ql/src/test/results/clientpositive/bucketmapjoin2.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketmapjoin2.q.out
@@ -198,7 +198,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcbucket_mapjoin_part
               numFiles 4
-              numPartitions 1
               numRows 0
               partition_columns ds
               rawDataSize 0
@@ -709,7 +708,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcbucket_mapjoin_part_2
               numFiles 2
-              numPartitions 1
               numRows 0
               partition_columns ds
               rawDataSize 0
@@ -1414,7 +1412,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcbucket_mapjoin_part
               numFiles 4
-              numPartitions 1
               numRows 0
               partition_columns ds
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/bucketmapjoin3.q.out b/src/ql/src/test/results/clientpositive/bucketmapjoin3.q.out
index af00380..3eb22b5 100644
--- a/src/ql/src/test/results/clientpositive/bucketmapjoin3.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketmapjoin3.q.out
@@ -215,7 +215,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcbucket_mapjoin_part_2
               numFiles 2
-              numPartitions 1
               numRows 0
               partition_columns ds
               rawDataSize 0
@@ -726,7 +725,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcbucket_mapjoin_part
               numFiles 4
-              numPartitions 1
               numRows 0
               partition_columns ds
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/bucketmapjoin5.q.out b/src/ql/src/test/results/clientpositive/bucketmapjoin5.q.out
index 03b7811..9bb8671 100644
--- a/src/ql/src/test/results/clientpositive/bucketmapjoin5.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketmapjoin5.q.out
@@ -253,7 +253,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcbucket_mapjoin_part
               numFiles 4
-              numPartitions 2
               numRows 0
               partition_columns ds
               rawDataSize 0
@@ -301,7 +300,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcbucket_mapjoin_part
               numFiles 4
-              numPartitions 2
               numRows 0
               partition_columns ds
               rawDataSize 0
@@ -813,7 +811,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcbucket_mapjoin_part_2
               numFiles 2
-              numPartitions 2
               numRows 0
               partition_columns ds
               rawDataSize 0
@@ -861,7 +858,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcbucket_mapjoin_part_2
               numFiles 2
-              numPartitions 2
               numRows 0
               partition_columns ds
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/bucketmapjoin7.q.out b/src/ql/src/test/results/clientpositive/bucketmapjoin7.q.out
index 24566c2..5f37d10 100644
--- a/src/ql/src/test/results/clientpositive/bucketmapjoin7.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketmapjoin7.q.out
@@ -162,7 +162,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcbucket_mapjoin_part_1
               numFiles 2
-              numPartitions 1
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/bucketmapjoin8.q.out b/src/ql/src/test/results/clientpositive/bucketmapjoin8.q.out
index 63f8663..7a11805 100644
--- a/src/ql/src/test/results/clientpositive/bucketmapjoin8.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketmapjoin8.q.out
@@ -152,7 +152,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcbucket_mapjoin_part_1
               numFiles 2
-              numPartitions 1
               numRows 0
               partition_columns part
               rawDataSize 0
@@ -396,7 +395,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcbucket_mapjoin_part_1
               numFiles 2
-              numPartitions 1
               numRows 0
               partition_columns part
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/bucketmapjoin9.q.out b/src/ql/src/test/results/clientpositive/bucketmapjoin9.q.out
index 0abf9a9..a30a5b6 100644
--- a/src/ql/src/test/results/clientpositive/bucketmapjoin9.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketmapjoin9.q.out
@@ -151,7 +151,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcbucket_mapjoin_part_1
               numFiles 2
-              numPartitions 1
               numRows 0
               partition_columns part
               rawDataSize 0
@@ -417,7 +416,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcbucket_mapjoin_part_1
               numFiles 2
-              numPartitions 1
               numRows 0
               partition_columns part
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/columnstats_partlvl.q.out b/src/ql/src/test/results/clientpositive/columnstats_partlvl.q.out
index a846bb0..2c32730 100644
--- a/src/ql/src/test/results/clientpositive/columnstats_partlvl.q.out
+++ b/src/ql/src/test/results/clientpositive/columnstats_partlvl.q.out
@@ -145,7 +145,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.employee_part
               numFiles 1
-              numPartitions 2
               numRows 0
               partition_columns employeesalary
               rawDataSize 0
@@ -352,7 +351,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.employee_part
               numFiles 1
-              numPartitions 2
               numRows 0
               partition_columns employeesalary
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/combine2_hadoop20.q.out b/src/ql/src/test/results/clientpositive/combine2_hadoop20.q.out
index 6279021..5152ade 100644
--- a/src/ql/src/test/results/clientpositive/combine2_hadoop20.q.out
+++ b/src/ql/src/test/results/clientpositive/combine2_hadoop20.q.out
@@ -250,7 +250,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.combine2
               numFiles 1
-              numPartitions 8
               numRows 1
               partition_columns value
               rawDataSize 2
@@ -296,7 +295,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.combine2
               numFiles 1
-              numPartitions 8
               numRows 3
               partition_columns value
               rawDataSize 3
@@ -342,7 +340,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.combine2
               numFiles 1
-              numPartitions 8
               numRows 1
               partition_columns value
               rawDataSize 1
@@ -388,7 +385,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.combine2
               numFiles 1
-              numPartitions 8
               numRows 1
               partition_columns value
               rawDataSize 1
@@ -434,7 +430,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.combine2
               numFiles 1
-              numPartitions 8
               numRows 3
               partition_columns value
               rawDataSize 3
@@ -480,7 +475,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.combine2
               numFiles 1
-              numPartitions 8
               numRows 1
               partition_columns value
               rawDataSize 1
@@ -526,7 +520,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.combine2
               numFiles 1
-              numPartitions 8
               numRows 1
               partition_columns value
               rawDataSize 1
@@ -572,7 +565,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.combine2
               numFiles 1
-              numPartitions 8
               numRows 1
               partition_columns value
               rawDataSize 2
diff --git a/src/ql/src/test/results/clientpositive/filter_join_breaktask.q.out b/src/ql/src/test/results/clientpositive/filter_join_breaktask.q.out
index 63d7d07..3a75bcf 100644
--- a/src/ql/src/test/results/clientpositive/filter_join_breaktask.q.out
+++ b/src/ql/src/test/results/clientpositive/filter_join_breaktask.q.out
@@ -101,7 +101,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.filter_join_breaktask
               numFiles 1
-              numPartitions 1
               numRows 25
               partition_columns ds
               rawDataSize 211
@@ -231,7 +230,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.filter_join_breaktask
               numFiles 1
-              numPartitions 1
               numRows 25
               partition_columns ds
               rawDataSize 211
diff --git a/src/ql/src/test/results/clientpositive/groupby_map_ppr.q.out b/src/ql/src/test/results/clientpositive/groupby_map_ppr.q.out
index 2082630..ce1e14c 100644
--- a/src/ql/src/test/results/clientpositive/groupby_map_ppr.q.out
+++ b/src/ql/src/test/results/clientpositive/groupby_map_ppr.q.out
@@ -87,7 +87,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -134,7 +133,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/groupby_map_ppr_multi_distinct.q.out b/src/ql/src/test/results/clientpositive/groupby_map_ppr_multi_distinct.q.out
index b78c399..8d7bb7e 100644
--- a/src/ql/src/test/results/clientpositive/groupby_map_ppr_multi_distinct.q.out
+++ b/src/ql/src/test/results/clientpositive/groupby_map_ppr_multi_distinct.q.out
@@ -97,7 +97,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -144,7 +143,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/groupby_ppr.q.out b/src/ql/src/test/results/clientpositive/groupby_ppr.q.out
index 824c802..8879e3c 100644
--- a/src/ql/src/test/results/clientpositive/groupby_ppr.q.out
+++ b/src/ql/src/test/results/clientpositive/groupby_ppr.q.out
@@ -70,7 +70,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -117,7 +116,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/groupby_ppr_multi_distinct.q.out b/src/ql/src/test/results/clientpositive/groupby_ppr_multi_distinct.q.out
index 6ae3db9..83fb64c 100644
--- a/src/ql/src/test/results/clientpositive/groupby_ppr_multi_distinct.q.out
+++ b/src/ql/src/test/results/clientpositive/groupby_ppr_multi_distinct.q.out
@@ -72,7 +72,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -119,7 +118,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/groupby_sort_6.q.out b/src/ql/src/test/results/clientpositive/groupby_sort_6.q.out
new file mode 100644
index 0000000..54125d0
--- /dev/null
+++ b/src/ql/src/test/results/clientpositive/groupby_sort_6.q.out
@@ -0,0 +1,579 @@
+PREHOOK: query: CREATE TABLE T1(key STRING, val STRING) PARTITIONED BY (ds string)
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE T1(key STRING, val STRING) PARTITIONED BY (ds string)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@T1
+PREHOOK: query: CREATE TABLE outputTbl1(key int, cnt int)
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE outputTbl1(key int, cnt int)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@outputTbl1
+PREHOOK: query: -- The plan should not be converted to a map-side group since no partition is being accessed
+EXPLAIN EXTENDED
+INSERT OVERWRITE TABLE outputTbl1
+SELECT key, count(1) FROM T1 where ds = '1' GROUP BY key
+PREHOOK: type: QUERY
+POSTHOOK: query: -- The plan should not be converted to a map-side group since no partition is being accessed
+EXPLAIN EXTENDED
+INSERT OVERWRITE TABLE outputTbl1
+SELECT key, count(1) FROM T1 where ds = '1' GROUP BY key
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME T1))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB (TOK_TABNAME outputTbl1))) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_FUNCTION count 1))) (TOK_WHERE (= (TOK_TABLE_OR_COL ds) '1')) (TOK_GROUPBY (TOK_TABLE_OR_COL key))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+  Stage-2 depends on stages: Stage-0
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        t1 
+          TableScan
+            alias: t1
+            GatherStats: false
+            Filter Operator
+              isSamplingPred: false
+              predicate:
+                  expr: (ds = '1')
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: string
+                outputColumnNames: key
+                Group By Operator
+                  aggregations:
+                        expr: count(1)
+                  bucketGroup: false
+                  keys:
+                        expr: key
+                        type: string
+                  mode: hash
+                  outputColumnNames: _col0, _col1
+                  Reduce Output Operator
+                    key expressions:
+                          expr: _col0
+                          type: string
+                    sort order: +
+                    Map-reduce partition columns:
+                          expr: _col0
+                          type: string
+                    tag: -1
+                    value expressions:
+                          expr: _col1
+                          type: bigint
+      Needs Tagging: false
+      Reduce Operator Tree:
+        Group By Operator
+          aggregations:
+                expr: count(VALUE._col0)
+          bucketGroup: false
+          keys:
+                expr: KEY._col0
+                type: string
+          mode: mergepartial
+          outputColumnNames: _col0, _col1
+          Select Operator
+            expressions:
+                  expr: _col0
+                  type: string
+                  expr: _col1
+                  type: bigint
+            outputColumnNames: _col0, _col1
+            Select Operator
+              expressions:
+                    expr: UDFToInteger(_col0)
+                    type: int
+                    expr: UDFToInteger(_col1)
+                    type: int
+              outputColumnNames: _col0, _col1
+              File Output Operator
+                compressed: false
+                GlobalTableId: 1
+#### A masked pattern was here ####
+                NumFilesPerFileSink: 1
+#### A masked pattern was here ####
+                table:
+                    input format: org.apache.hadoop.mapred.TextInputFormat
+                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                    properties:
+                      bucket_count -1
+                      columns key,cnt
+                      columns.types int:int
+#### A masked pattern was here ####
+                      name default.outputtbl1
+                      serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
+                      serialization.format 1
+                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+#### A masked pattern was here ####
+                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                    name: default.outputtbl1
+                TotalFiles: 1
+                GatherStats: true
+                MultiFileSpray: false
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          replace: true
+#### A masked pattern was here ####
+          table:
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              properties:
+                bucket_count -1
+                columns key,cnt
+                columns.types int:int
+#### A masked pattern was here ####
+                name default.outputtbl1
+                serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
+                serialization.format 1
+                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+#### A masked pattern was here ####
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.outputtbl1
+#### A masked pattern was here ####
+
+  Stage: Stage-2
+    Stats-Aggr Operator
+#### A masked pattern was here ####
+
+
+PREHOOK: query: INSERT OVERWRITE TABLE outputTbl1
+SELECT key, count(1) FROM T1 where ds = '1' GROUP BY key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t1
+PREHOOK: Output: default@outputtbl1
+POSTHOOK: query: INSERT OVERWRITE TABLE outputTbl1
+SELECT key, count(1) FROM T1 where ds = '1' GROUP BY key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t1
+POSTHOOK: Output: default@outputtbl1
+POSTHOOK: Lineage: outputtbl1.cnt EXPRESSION [(t1)t1.null, ]
+POSTHOOK: Lineage: outputtbl1.key EXPRESSION [(t1)t1.FieldSchema(name:key, type:string, comment:null), ]
+PREHOOK: query: SELECT * FROM outputTbl1 ORDER BY key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@outputtbl1
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM outputTbl1 ORDER BY key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@outputtbl1
+#### A masked pattern was here ####
+POSTHOOK: Lineage: outputtbl1.cnt EXPRESSION [(t1)t1.null, ]
+POSTHOOK: Lineage: outputtbl1.key EXPRESSION [(t1)t1.FieldSchema(name:key, type:string, comment:null), ]
+PREHOOK: query: LOAD DATA LOCAL INPATH '../data/files/T1.txt' INTO TABLE T1 PARTITION (ds='2')
+PREHOOK: type: LOAD
+PREHOOK: Output: default@t1
+POSTHOOK: query: LOAD DATA LOCAL INPATH '../data/files/T1.txt' INTO TABLE T1 PARTITION (ds='2')
+POSTHOOK: type: LOAD
+POSTHOOK: Output: default@t1
+POSTHOOK: Output: default@t1@ds=2
+POSTHOOK: Lineage: outputtbl1.cnt EXPRESSION [(t1)t1.null, ]
+POSTHOOK: Lineage: outputtbl1.key EXPRESSION [(t1)t1.FieldSchema(name:key, type:string, comment:null), ]
+PREHOOK: query: -- The plan should not be converted to a map-side group since no partition is being accessed
+EXPLAIN EXTENDED
+INSERT OVERWRITE TABLE outputTbl1
+SELECT key, count(1) FROM T1 where ds = '1' GROUP BY key
+PREHOOK: type: QUERY
+POSTHOOK: query: -- The plan should not be converted to a map-side group since no partition is being accessed
+EXPLAIN EXTENDED
+INSERT OVERWRITE TABLE outputTbl1
+SELECT key, count(1) FROM T1 where ds = '1' GROUP BY key
+POSTHOOK: type: QUERY
+POSTHOOK: Lineage: outputtbl1.cnt EXPRESSION [(t1)t1.null, ]
+POSTHOOK: Lineage: outputtbl1.key EXPRESSION [(t1)t1.FieldSchema(name:key, type:string, comment:null), ]
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME T1))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB (TOK_TABNAME outputTbl1))) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_FUNCTION count 1))) (TOK_WHERE (= (TOK_TABLE_OR_COL ds) '1')) (TOK_GROUPBY (TOK_TABLE_OR_COL key))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+  Stage-2 depends on stages: Stage-0
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        t1 
+          TableScan
+            alias: t1
+            GatherStats: false
+            Filter Operator
+              isSamplingPred: false
+              predicate:
+                  expr: (ds = '1')
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: string
+                outputColumnNames: key
+                Group By Operator
+                  aggregations:
+                        expr: count(1)
+                  bucketGroup: false
+                  keys:
+                        expr: key
+                        type: string
+                  mode: hash
+                  outputColumnNames: _col0, _col1
+                  Reduce Output Operator
+                    key expressions:
+                          expr: _col0
+                          type: string
+                    sort order: +
+                    Map-reduce partition columns:
+                          expr: _col0
+                          type: string
+                    tag: -1
+                    value expressions:
+                          expr: _col1
+                          type: bigint
+      Needs Tagging: false
+      Reduce Operator Tree:
+        Group By Operator
+          aggregations:
+                expr: count(VALUE._col0)
+          bucketGroup: false
+          keys:
+                expr: KEY._col0
+                type: string
+          mode: mergepartial
+          outputColumnNames: _col0, _col1
+          Select Operator
+            expressions:
+                  expr: _col0
+                  type: string
+                  expr: _col1
+                  type: bigint
+            outputColumnNames: _col0, _col1
+            Select Operator
+              expressions:
+                    expr: UDFToInteger(_col0)
+                    type: int
+                    expr: UDFToInteger(_col1)
+                    type: int
+              outputColumnNames: _col0, _col1
+              File Output Operator
+                compressed: false
+                GlobalTableId: 1
+#### A masked pattern was here ####
+                NumFilesPerFileSink: 1
+#### A masked pattern was here ####
+                table:
+                    input format: org.apache.hadoop.mapred.TextInputFormat
+                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                    properties:
+                      bucket_count -1
+                      columns key,cnt
+                      columns.types int:int
+#### A masked pattern was here ####
+                      name default.outputtbl1
+                      numFiles 1
+                      numPartitions 0
+                      numRows 0
+                      rawDataSize 0
+                      serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
+                      serialization.format 1
+                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                      totalSize 0
+#### A masked pattern was here ####
+                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                    name: default.outputtbl1
+                TotalFiles: 1
+                GatherStats: true
+                MultiFileSpray: false
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          replace: true
+#### A masked pattern was here ####
+          table:
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              properties:
+                bucket_count -1
+                columns key,cnt
+                columns.types int:int
+#### A masked pattern was here ####
+                name default.outputtbl1
+                numFiles 1
+                numPartitions 0
+                numRows 0
+                rawDataSize 0
+                serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
+                serialization.format 1
+                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                totalSize 0
+#### A masked pattern was here ####
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.outputtbl1
+#### A masked pattern was here ####
+
+  Stage: Stage-2
+    Stats-Aggr Operator
+#### A masked pattern was here ####
+
+
+PREHOOK: query: INSERT OVERWRITE TABLE outputTbl1
+SELECT key, count(1) FROM T1 where ds = '1' GROUP BY key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t1
+PREHOOK: Output: default@outputtbl1
+POSTHOOK: query: INSERT OVERWRITE TABLE outputTbl1
+SELECT key, count(1) FROM T1 where ds = '1' GROUP BY key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t1
+POSTHOOK: Output: default@outputtbl1
+POSTHOOK: Lineage: outputtbl1.cnt EXPRESSION [(t1)t1.null, ]
+POSTHOOK: Lineage: outputtbl1.cnt EXPRESSION [(t1)t1.null, ]
+POSTHOOK: Lineage: outputtbl1.key EXPRESSION [(t1)t1.FieldSchema(name:key, type:string, comment:null), ]
+POSTHOOK: Lineage: outputtbl1.key EXPRESSION [(t1)t1.FieldSchema(name:key, type:string, comment:null), ]
+PREHOOK: query: SELECT * FROM outputTbl1 ORDER BY key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@outputtbl1
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM outputTbl1 ORDER BY key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@outputtbl1
+#### A masked pattern was here ####
+POSTHOOK: Lineage: outputtbl1.cnt EXPRESSION [(t1)t1.null, ]
+POSTHOOK: Lineage: outputtbl1.cnt EXPRESSION [(t1)t1.null, ]
+POSTHOOK: Lineage: outputtbl1.key EXPRESSION [(t1)t1.FieldSchema(name:key, type:string, comment:null), ]
+POSTHOOK: Lineage: outputtbl1.key EXPRESSION [(t1)t1.FieldSchema(name:key, type:string, comment:null), ]
+PREHOOK: query: -- The plan should not be converted to a map-side group since the partition being accessed
+-- is neither bucketed not sorted
+EXPLAIN EXTENDED
+INSERT OVERWRITE TABLE outputTbl1
+SELECT key, count(1) FROM T1 where ds = '2' GROUP BY key
+PREHOOK: type: QUERY
+POSTHOOK: query: -- The plan should not be converted to a map-side group since the partition being accessed
+-- is neither bucketed not sorted
+EXPLAIN EXTENDED
+INSERT OVERWRITE TABLE outputTbl1
+SELECT key, count(1) FROM T1 where ds = '2' GROUP BY key
+POSTHOOK: type: QUERY
+POSTHOOK: Lineage: outputtbl1.cnt EXPRESSION [(t1)t1.null, ]
+POSTHOOK: Lineage: outputtbl1.cnt EXPRESSION [(t1)t1.null, ]
+POSTHOOK: Lineage: outputtbl1.key EXPRESSION [(t1)t1.FieldSchema(name:key, type:string, comment:null), ]
+POSTHOOK: Lineage: outputtbl1.key EXPRESSION [(t1)t1.FieldSchema(name:key, type:string, comment:null), ]
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME T1))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB (TOK_TABNAME outputTbl1))) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_FUNCTION count 1))) (TOK_WHERE (= (TOK_TABLE_OR_COL ds) '2')) (TOK_GROUPBY (TOK_TABLE_OR_COL key))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+  Stage-2 depends on stages: Stage-0
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        t1 
+          TableScan
+            alias: t1
+            GatherStats: false
+            Select Operator
+              expressions:
+                    expr: key
+                    type: string
+              outputColumnNames: key
+              Group By Operator
+                aggregations:
+                      expr: count(1)
+                bucketGroup: false
+                keys:
+                      expr: key
+                      type: string
+                mode: hash
+                outputColumnNames: _col0, _col1
+                Reduce Output Operator
+                  key expressions:
+                        expr: _col0
+                        type: string
+                  sort order: +
+                  Map-reduce partition columns:
+                        expr: _col0
+                        type: string
+                  tag: -1
+                  value expressions:
+                        expr: _col1
+                        type: bigint
+      Needs Tagging: false
+      Path -> Alias:
+#### A masked pattern was here ####
+      Path -> Partition:
+#### A masked pattern was here ####
+          Partition
+            base file name: ds=2
+            input format: org.apache.hadoop.mapred.TextInputFormat
+            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+            partition values:
+              ds 2
+            properties:
+              bucket_count -1
+              columns key,val
+              columns.types string:string
+#### A masked pattern was here ####
+              name default.t1
+              numFiles 1
+              numRows 0
+              partition_columns ds
+              rawDataSize 0
+              serialization.ddl struct t1 { string key, string val}
+              serialization.format 1
+              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              totalSize 30
+#### A masked pattern was here ####
+            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+          
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              properties:
+                bucket_count -1
+                columns key,val
+                columns.types string:string
+#### A masked pattern was here ####
+                name default.t1
+                numFiles 1
+                numPartitions 1
+                numRows 0
+                partition_columns ds
+                rawDataSize 0
+                serialization.ddl struct t1 { string key, string val}
+                serialization.format 1
+                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                totalSize 30
+#### A masked pattern was here ####
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.t1
+            name: default.t1
+      Reduce Operator Tree:
+        Group By Operator
+          aggregations:
+                expr: count(VALUE._col0)
+          bucketGroup: false
+          keys:
+                expr: KEY._col0
+                type: string
+          mode: mergepartial
+          outputColumnNames: _col0, _col1
+          Select Operator
+            expressions:
+                  expr: _col0
+                  type: string
+                  expr: _col1
+                  type: bigint
+            outputColumnNames: _col0, _col1
+            Select Operator
+              expressions:
+                    expr: UDFToInteger(_col0)
+                    type: int
+                    expr: UDFToInteger(_col1)
+                    type: int
+              outputColumnNames: _col0, _col1
+              File Output Operator
+                compressed: false
+                GlobalTableId: 1
+#### A masked pattern was here ####
+                NumFilesPerFileSink: 1
+#### A masked pattern was here ####
+                table:
+                    input format: org.apache.hadoop.mapred.TextInputFormat
+                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                    properties:
+                      bucket_count -1
+                      columns key,cnt
+                      columns.types int:int
+#### A masked pattern was here ####
+                      name default.outputtbl1
+                      numFiles 1
+                      numPartitions 0
+                      numRows 0
+                      rawDataSize 0
+                      serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
+                      serialization.format 1
+                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                      totalSize 0
+#### A masked pattern was here ####
+                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                    name: default.outputtbl1
+                TotalFiles: 1
+                GatherStats: true
+                MultiFileSpray: false
+      Truncated Path -> Alias:
+        /t1/ds=2 [t1]
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          replace: true
+#### A masked pattern was here ####
+          table:
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              properties:
+                bucket_count -1
+                columns key,cnt
+                columns.types int:int
+#### A masked pattern was here ####
+                name default.outputtbl1
+                numFiles 1
+                numPartitions 0
+                numRows 0
+                rawDataSize 0
+                serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
+                serialization.format 1
+                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                totalSize 0
+#### A masked pattern was here ####
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.outputtbl1
+#### A masked pattern was here ####
+
+  Stage: Stage-2
+    Stats-Aggr Operator
+#### A masked pattern was here ####
+
+
+PREHOOK: query: INSERT OVERWRITE TABLE outputTbl1
+SELECT key, count(1) FROM T1 where ds = '2' GROUP BY key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t1
+PREHOOK: Input: default@t1@ds=2
+PREHOOK: Output: default@outputtbl1
+POSTHOOK: query: INSERT OVERWRITE TABLE outputTbl1
+SELECT key, count(1) FROM T1 where ds = '2' GROUP BY key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t1
+POSTHOOK: Input: default@t1@ds=2
+POSTHOOK: Output: default@outputtbl1
+POSTHOOK: Lineage: outputtbl1.cnt EXPRESSION [(t1)t1.null, ]
+POSTHOOK: Lineage: outputtbl1.cnt EXPRESSION [(t1)t1.null, ]
+POSTHOOK: Lineage: outputtbl1.cnt EXPRESSION [(t1)t1.null, ]
+POSTHOOK: Lineage: outputtbl1.key EXPRESSION [(t1)t1.FieldSchema(name:key, type:string, comment:null), ]
+POSTHOOK: Lineage: outputtbl1.key EXPRESSION [(t1)t1.FieldSchema(name:key, type:string, comment:null), ]
+POSTHOOK: Lineage: outputtbl1.key EXPRESSION [(t1)t1.FieldSchema(name:key, type:string, comment:null), ]
+PREHOOK: query: SELECT * FROM outputTbl1 ORDER BY key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@outputtbl1
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM outputTbl1 ORDER BY key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@outputtbl1
+#### A masked pattern was here ####
+POSTHOOK: Lineage: outputtbl1.cnt EXPRESSION [(t1)t1.null, ]
+POSTHOOK: Lineage: outputtbl1.cnt EXPRESSION [(t1)t1.null, ]
+POSTHOOK: Lineage: outputtbl1.cnt EXPRESSION [(t1)t1.null, ]
+POSTHOOK: Lineage: outputtbl1.key EXPRESSION [(t1)t1.FieldSchema(name:key, type:string, comment:null), ]
+POSTHOOK: Lineage: outputtbl1.key EXPRESSION [(t1)t1.FieldSchema(name:key, type:string, comment:null), ]
+POSTHOOK: Lineage: outputtbl1.key EXPRESSION [(t1)t1.FieldSchema(name:key, type:string, comment:null), ]
+1	1
+2	1
+3	1
+7	1
+8	2
diff --git a/src/ql/src/test/results/clientpositive/input23.q.out b/src/ql/src/test/results/clientpositive/input23.q.out
index 5f6eab0..f71a43f 100644
--- a/src/ql/src/test/results/clientpositive/input23.q.out
+++ b/src/ql/src/test/results/clientpositive/input23.q.out
@@ -71,7 +71,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/input42.q.out b/src/ql/src/test/results/clientpositive/input42.q.out
index 06fa1c0..67679af 100644
--- a/src/ql/src/test/results/clientpositive/input42.q.out
+++ b/src/ql/src/test/results/clientpositive/input42.q.out
@@ -66,7 +66,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -113,7 +112,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -1258,7 +1256,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -1305,7 +1302,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -1828,7 +1824,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -1875,7 +1870,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/input_part1.q.out b/src/ql/src/test/results/clientpositive/input_part1.q.out
index 304b976..fa24c53 100644
--- a/src/ql/src/test/results/clientpositive/input_part1.q.out
+++ b/src/ql/src/test/results/clientpositive/input_part1.q.out
@@ -102,7 +102,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/input_part2.q.out b/src/ql/src/test/results/clientpositive/input_part2.q.out
index 8deda18..1a40058 100644
--- a/src/ql/src/test/results/clientpositive/input_part2.q.out
+++ b/src/ql/src/test/results/clientpositive/input_part2.q.out
@@ -167,7 +167,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -214,7 +213,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/input_part7.q.out b/src/ql/src/test/results/clientpositive/input_part7.q.out
index 66ed3fd..538a742 100644
--- a/src/ql/src/test/results/clientpositive/input_part7.q.out
+++ b/src/ql/src/test/results/clientpositive/input_part7.q.out
@@ -150,7 +150,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -197,7 +196,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/input_part9.q.out b/src/ql/src/test/results/clientpositive/input_part9.q.out
index a3df4a1..91d1794 100644
--- a/src/ql/src/test/results/clientpositive/input_part9.q.out
+++ b/src/ql/src/test/results/clientpositive/input_part9.q.out
@@ -71,7 +71,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -118,7 +117,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/join26.q.out b/src/ql/src/test/results/clientpositive/join26.q.out
index 5d69f7b..80b373a 100644
--- a/src/ql/src/test/results/clientpositive/join26.q.out
+++ b/src/ql/src/test/results/clientpositive/join26.q.out
@@ -156,7 +156,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/join33.q.out b/src/ql/src/test/results/clientpositive/join33.q.out
index 2c51f44..c232702 100644
--- a/src/ql/src/test/results/clientpositive/join33.q.out
+++ b/src/ql/src/test/results/clientpositive/join33.q.out
@@ -210,7 +210,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/join9.q.out b/src/ql/src/test/results/clientpositive/join9.q.out
index 0609650..f25d7d5 100644
--- a/src/ql/src/test/results/clientpositive/join9.q.out
+++ b/src/ql/src/test/results/clientpositive/join9.q.out
@@ -120,7 +120,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/join_map_ppr.q.out b/src/ql/src/test/results/clientpositive/join_map_ppr.q.out
index b9e13db..f5a0f42 100644
--- a/src/ql/src/test/results/clientpositive/join_map_ppr.q.out
+++ b/src/ql/src/test/results/clientpositive/join_map_ppr.q.out
@@ -162,7 +162,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -729,7 +728,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/load_dyn_part8.q.out b/src/ql/src/test/results/clientpositive/load_dyn_part8.q.out
index 11a7c83..a86966f 100644
--- a/src/ql/src/test/results/clientpositive/load_dyn_part8.q.out
+++ b/src/ql/src/test/results/clientpositive/load_dyn_part8.q.out
@@ -149,7 +149,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -196,7 +195,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -243,7 +241,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -290,7 +287,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/louter_join_ppr.q.out b/src/ql/src/test/results/clientpositive/louter_join_ppr.q.out
index 417f11b..94936d8 100644
--- a/src/ql/src/test/results/clientpositive/louter_join_ppr.q.out
+++ b/src/ql/src/test/results/clientpositive/louter_join_ppr.q.out
@@ -134,7 +134,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -181,7 +180,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -441,7 +439,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -488,7 +485,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -535,7 +531,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -582,7 +577,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -853,7 +847,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -900,7 +893,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -947,7 +939,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -994,7 +985,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -1260,7 +1250,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -1307,7 +1296,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/merge3.q.out b/src/ql/src/test/results/clientpositive/merge3.q.out
index 996901e..d3c700b 100644
--- a/src/ql/src/test/results/clientpositive/merge3.q.out
+++ b/src/ql/src/test/results/clientpositive/merge3.q.out
@@ -2441,7 +2441,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.merge_src_part
               numFiles 2
-              numPartitions 2
               numRows 1000
               partition_columns ds
               rawDataSize 10624
@@ -2487,7 +2486,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.merge_src_part
               numFiles 2
-              numPartitions 2
               numRows 1000
               partition_columns ds
               rawDataSize 10624
@@ -4873,7 +4871,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.merge_src_part
               numFiles 2
-              numPartitions 2
               numRows 1000
               partition_columns ds
               rawDataSize 10624
@@ -4919,7 +4916,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.merge_src_part
               numFiles 2
-              numPartitions 2
               numRows 1000
               partition_columns ds
               rawDataSize 10624
diff --git a/src/ql/src/test/results/clientpositive/outer_join_ppr.q.out b/src/ql/src/test/results/clientpositive/outer_join_ppr.q.out
index a3c6d70..f311cce 100644
--- a/src/ql/src/test/results/clientpositive/outer_join_ppr.q.out
+++ b/src/ql/src/test/results/clientpositive/outer_join_ppr.q.out
@@ -126,7 +126,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -173,7 +172,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -220,7 +218,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -267,7 +264,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -528,7 +524,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -575,7 +570,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -622,7 +616,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -669,7 +662,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/partition_wise_fileformat10.q.out b/src/ql/src/test/results/clientpositive/partition_wise_fileformat10.q.out
new file mode 100644
index 0000000..4417e19
--- /dev/null
+++ b/src/ql/src/test/results/clientpositive/partition_wise_fileformat10.q.out
@@ -0,0 +1,79 @@
+PREHOOK: query: -- This tests that the schema can be changed for binary serde data
+create table prt(key string, value string) partitioned by (dt string)
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: -- This tests that the schema can be changed for binary serde data
+create table prt(key string, value string) partitioned by (dt string)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@prt
+PREHOOK: query: insert overwrite table prt partition(dt='1') select * from src where key = 238
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@prt@dt=1
+POSTHOOK: query: insert overwrite table prt partition(dt='1') select * from src where key = 238
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@prt@dt=1
+POSTHOOK: Lineage: prt PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: prt PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: select * from prt where dt is not null
+PREHOOK: type: QUERY
+PREHOOK: Input: default@prt@dt=1
+#### A masked pattern was here ####
+POSTHOOK: query: select * from prt where dt is not null
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@prt@dt=1
+#### A masked pattern was here ####
+POSTHOOK: Lineage: prt PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: prt PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+238	val_238	1
+238	val_238	1
+PREHOOK: query: select key+key, value from prt where dt is not null
+PREHOOK: type: QUERY
+PREHOOK: Input: default@prt
+PREHOOK: Input: default@prt@dt=1
+#### A masked pattern was here ####
+POSTHOOK: query: select key+key, value from prt where dt is not null
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@prt
+POSTHOOK: Input: default@prt@dt=1
+#### A masked pattern was here ####
+POSTHOOK: Lineage: prt PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: prt PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+476.0	val_238
+476.0	val_238
+PREHOOK: query: alter table prt add columns (value2 string)
+PREHOOK: type: ALTERTABLE_ADDCOLS
+PREHOOK: Input: default@prt
+PREHOOK: Output: default@prt
+POSTHOOK: query: alter table prt add columns (value2 string)
+POSTHOOK: type: ALTERTABLE_ADDCOLS
+POSTHOOK: Input: default@prt
+POSTHOOK: Output: default@prt
+POSTHOOK: Lineage: prt PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: prt PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: select key+key, value from prt where dt is not null
+PREHOOK: type: QUERY
+PREHOOK: Input: default@prt
+PREHOOK: Input: default@prt@dt=1
+#### A masked pattern was here ####
+POSTHOOK: query: select key+key, value from prt where dt is not null
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@prt
+POSTHOOK: Input: default@prt@dt=1
+#### A masked pattern was here ####
+POSTHOOK: Lineage: prt PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: prt PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+476.0	val_238
+476.0	val_238
+PREHOOK: query: select * from prt where dt is not null
+PREHOOK: type: QUERY
+PREHOOK: Input: default@prt@dt=1
+#### A masked pattern was here ####
+POSTHOOK: query: select * from prt where dt is not null
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@prt@dt=1
+#### A masked pattern was here ####
+POSTHOOK: Lineage: prt PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: prt PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+238	val_238	NULL	1
+238	val_238	NULL	1
diff --git a/src/ql/src/test/results/clientpositive/partition_wise_fileformat11.q.out b/src/ql/src/test/results/clientpositive/partition_wise_fileformat11.q.out
new file mode 100644
index 0000000..1774a90
--- /dev/null
+++ b/src/ql/src/test/results/clientpositive/partition_wise_fileformat11.q.out
@@ -0,0 +1,123 @@
+PREHOOK: query: -- This tests that the schema can be changed for binary serde data
+create table partition_test_partitioned(key string, value string) partitioned by (dt string) stored as rcfile
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: -- This tests that the schema can be changed for binary serde data
+create table partition_test_partitioned(key string, value string) partitioned by (dt string) stored as rcfile
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@partition_test_partitioned
+PREHOOK: query: alter table partition_test_partitioned set serde 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe'
+PREHOOK: type: ALTERTABLE_SERIALIZER
+PREHOOK: Input: default@partition_test_partitioned
+PREHOOK: Output: default@partition_test_partitioned
+POSTHOOK: query: alter table partition_test_partitioned set serde 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe'
+POSTHOOK: type: ALTERTABLE_SERIALIZER
+POSTHOOK: Input: default@partition_test_partitioned
+POSTHOOK: Output: default@partition_test_partitioned
+PREHOOK: query: insert overwrite table partition_test_partitioned partition(dt='1') select * from src where key = 238
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@partition_test_partitioned@dt=1
+POSTHOOK: query: insert overwrite table partition_test_partitioned partition(dt='1') select * from src where key = 238
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@partition_test_partitioned@dt=1
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: select * from partition_test_partitioned where dt is not null
+PREHOOK: type: QUERY
+PREHOOK: Input: default@partition_test_partitioned@dt=1
+#### A masked pattern was here ####
+POSTHOOK: query: select * from partition_test_partitioned where dt is not null
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@partition_test_partitioned@dt=1
+#### A masked pattern was here ####
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+238	val_238	1
+238	val_238	1
+PREHOOK: query: select key+key, value from partition_test_partitioned where dt is not null
+PREHOOK: type: QUERY
+PREHOOK: Input: default@partition_test_partitioned
+PREHOOK: Input: default@partition_test_partitioned@dt=1
+#### A masked pattern was here ####
+POSTHOOK: query: select key+key, value from partition_test_partitioned where dt is not null
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@partition_test_partitioned
+POSTHOOK: Input: default@partition_test_partitioned@dt=1
+#### A masked pattern was here ####
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+476.0	val_238
+476.0	val_238
+PREHOOK: query: alter table partition_test_partitioned change key key int
+PREHOOK: type: ALTERTABLE_RENAMECOL
+PREHOOK: Input: default@partition_test_partitioned
+PREHOOK: Output: default@partition_test_partitioned
+POSTHOOK: query: alter table partition_test_partitioned change key key int
+POSTHOOK: type: ALTERTABLE_RENAMECOL
+POSTHOOK: Input: default@partition_test_partitioned
+POSTHOOK: Output: default@partition_test_partitioned
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: select key+key, value from partition_test_partitioned where dt is not null
+PREHOOK: type: QUERY
+PREHOOK: Input: default@partition_test_partitioned
+PREHOOK: Input: default@partition_test_partitioned@dt=1
+#### A masked pattern was here ####
+POSTHOOK: query: select key+key, value from partition_test_partitioned where dt is not null
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@partition_test_partitioned
+POSTHOOK: Input: default@partition_test_partitioned@dt=1
+#### A masked pattern was here ####
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+476	val_238
+476	val_238
+PREHOOK: query: select * from partition_test_partitioned where dt is not null
+PREHOOK: type: QUERY
+PREHOOK: Input: default@partition_test_partitioned@dt=1
+#### A masked pattern was here ####
+POSTHOOK: query: select * from partition_test_partitioned where dt is not null
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@partition_test_partitioned@dt=1
+#### A masked pattern was here ####
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+238	val_238	1
+238	val_238	1
+PREHOOK: query: alter table partition_test_partitioned add columns (value2 string)
+PREHOOK: type: ALTERTABLE_ADDCOLS
+PREHOOK: Input: default@partition_test_partitioned
+PREHOOK: Output: default@partition_test_partitioned
+POSTHOOK: query: alter table partition_test_partitioned add columns (value2 string)
+POSTHOOK: type: ALTERTABLE_ADDCOLS
+POSTHOOK: Input: default@partition_test_partitioned
+POSTHOOK: Output: default@partition_test_partitioned
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: select key+key, value from partition_test_partitioned where dt is not null
+PREHOOK: type: QUERY
+PREHOOK: Input: default@partition_test_partitioned
+PREHOOK: Input: default@partition_test_partitioned@dt=1
+#### A masked pattern was here ####
+POSTHOOK: query: select key+key, value from partition_test_partitioned where dt is not null
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@partition_test_partitioned
+POSTHOOK: Input: default@partition_test_partitioned@dt=1
+#### A masked pattern was here ####
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+476	val_238
+476	val_238
+PREHOOK: query: select * from partition_test_partitioned where dt is not null
+PREHOOK: type: QUERY
+PREHOOK: Input: default@partition_test_partitioned@dt=1
+#### A masked pattern was here ####
+POSTHOOK: query: select * from partition_test_partitioned where dt is not null
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@partition_test_partitioned@dt=1
+#### A masked pattern was here ####
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+238	val_238	NULL	1
+238	val_238	NULL	1
diff --git a/src/ql/src/test/results/clientpositive/partition_wise_fileformat12.q.out b/src/ql/src/test/results/clientpositive/partition_wise_fileformat12.q.out
new file mode 100644
index 0000000..b107ecd
--- /dev/null
+++ b/src/ql/src/test/results/clientpositive/partition_wise_fileformat12.q.out
@@ -0,0 +1,216 @@
+PREHOOK: query: -- This tests that the schema can be changed for binary serde data
+create table partition_test_partitioned(key string, value string) partitioned by (dt string) stored as rcfile
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: -- This tests that the schema can be changed for binary serde data
+create table partition_test_partitioned(key string, value string) partitioned by (dt string) stored as rcfile
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@partition_test_partitioned
+PREHOOK: query: alter table partition_test_partitioned set serde 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe'
+PREHOOK: type: ALTERTABLE_SERIALIZER
+PREHOOK: Input: default@partition_test_partitioned
+PREHOOK: Output: default@partition_test_partitioned
+POSTHOOK: query: alter table partition_test_partitioned set serde 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe'
+POSTHOOK: type: ALTERTABLE_SERIALIZER
+POSTHOOK: Input: default@partition_test_partitioned
+POSTHOOK: Output: default@partition_test_partitioned
+PREHOOK: query: insert overwrite table partition_test_partitioned partition(dt='1') select * from src where key = 238
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@partition_test_partitioned@dt=1
+POSTHOOK: query: insert overwrite table partition_test_partitioned partition(dt='1') select * from src where key = 238
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@partition_test_partitioned@dt=1
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: select * from partition_test_partitioned where dt is not null
+PREHOOK: type: QUERY
+PREHOOK: Input: default@partition_test_partitioned@dt=1
+#### A masked pattern was here ####
+POSTHOOK: query: select * from partition_test_partitioned where dt is not null
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@partition_test_partitioned@dt=1
+#### A masked pattern was here ####
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+238	val_238	1
+238	val_238	1
+PREHOOK: query: select key+key, value from partition_test_partitioned where dt is not null
+PREHOOK: type: QUERY
+PREHOOK: Input: default@partition_test_partitioned
+PREHOOK: Input: default@partition_test_partitioned@dt=1
+#### A masked pattern was here ####
+POSTHOOK: query: select key+key, value from partition_test_partitioned where dt is not null
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@partition_test_partitioned
+POSTHOOK: Input: default@partition_test_partitioned@dt=1
+#### A masked pattern was here ####
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+476.0	val_238
+476.0	val_238
+PREHOOK: query: alter table partition_test_partitioned change key key int
+PREHOOK: type: ALTERTABLE_RENAMECOL
+PREHOOK: Input: default@partition_test_partitioned
+PREHOOK: Output: default@partition_test_partitioned
+POSTHOOK: query: alter table partition_test_partitioned change key key int
+POSTHOOK: type: ALTERTABLE_RENAMECOL
+POSTHOOK: Input: default@partition_test_partitioned
+POSTHOOK: Output: default@partition_test_partitioned
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: select key+key, value from partition_test_partitioned where dt is not null
+PREHOOK: type: QUERY
+PREHOOK: Input: default@partition_test_partitioned
+PREHOOK: Input: default@partition_test_partitioned@dt=1
+#### A masked pattern was here ####
+POSTHOOK: query: select key+key, value from partition_test_partitioned where dt is not null
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@partition_test_partitioned
+POSTHOOK: Input: default@partition_test_partitioned@dt=1
+#### A masked pattern was here ####
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+476	val_238
+476	val_238
+PREHOOK: query: select * from partition_test_partitioned where dt is not null
+PREHOOK: type: QUERY
+PREHOOK: Input: default@partition_test_partitioned@dt=1
+#### A masked pattern was here ####
+POSTHOOK: query: select * from partition_test_partitioned where dt is not null
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@partition_test_partitioned@dt=1
+#### A masked pattern was here ####
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+238	val_238	1
+238	val_238	1
+PREHOOK: query: insert overwrite table partition_test_partitioned partition(dt='2') select * from src where key = 97
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@partition_test_partitioned@dt=2
+POSTHOOK: query: insert overwrite table partition_test_partitioned partition(dt='2') select * from src where key = 97
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@partition_test_partitioned@dt=2
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: alter table partition_test_partitioned add columns (value2 string)
+PREHOOK: type: ALTERTABLE_ADDCOLS
+PREHOOK: Input: default@partition_test_partitioned
+PREHOOK: Output: default@partition_test_partitioned
+POSTHOOK: query: alter table partition_test_partitioned add columns (value2 string)
+POSTHOOK: type: ALTERTABLE_ADDCOLS
+POSTHOOK: Input: default@partition_test_partitioned
+POSTHOOK: Output: default@partition_test_partitioned
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: select key+key, value from partition_test_partitioned where dt is not null
+PREHOOK: type: QUERY
+PREHOOK: Input: default@partition_test_partitioned
+PREHOOK: Input: default@partition_test_partitioned@dt=1
+PREHOOK: Input: default@partition_test_partitioned@dt=2
+#### A masked pattern was here ####
+POSTHOOK: query: select key+key, value from partition_test_partitioned where dt is not null
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@partition_test_partitioned
+POSTHOOK: Input: default@partition_test_partitioned@dt=1
+POSTHOOK: Input: default@partition_test_partitioned@dt=2
+#### A masked pattern was here ####
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+476	val_238
+476	val_238
+194	val_97
+194	val_97
+PREHOOK: query: select * from partition_test_partitioned where dt is not null
+PREHOOK: type: QUERY
+PREHOOK: Input: default@partition_test_partitioned@dt=1
+PREHOOK: Input: default@partition_test_partitioned@dt=2
+#### A masked pattern was here ####
+POSTHOOK: query: select * from partition_test_partitioned where dt is not null
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@partition_test_partitioned@dt=1
+POSTHOOK: Input: default@partition_test_partitioned@dt=2
+#### A masked pattern was here ####
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+238	val_238	NULL	1
+238	val_238	NULL	1
+97	val_97	NULL	2
+97	val_97	NULL	2
+PREHOOK: query: insert overwrite table partition_test_partitioned partition(dt='3') select key, value, value from src where key = 200
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@partition_test_partitioned@dt=3
+POSTHOOK: query: insert overwrite table partition_test_partitioned partition(dt='3') select key, value, value from src where key = 200
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@partition_test_partitioned@dt=3
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=3).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=3).value2 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: select key+key, value, value2 from partition_test_partitioned where dt is not null
+PREHOOK: type: QUERY
+PREHOOK: Input: default@partition_test_partitioned
+PREHOOK: Input: default@partition_test_partitioned@dt=1
+PREHOOK: Input: default@partition_test_partitioned@dt=2
+PREHOOK: Input: default@partition_test_partitioned@dt=3
+#### A masked pattern was here ####
+POSTHOOK: query: select key+key, value, value2 from partition_test_partitioned where dt is not null
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@partition_test_partitioned
+POSTHOOK: Input: default@partition_test_partitioned@dt=1
+POSTHOOK: Input: default@partition_test_partitioned@dt=2
+POSTHOOK: Input: default@partition_test_partitioned@dt=3
+#### A masked pattern was here ####
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=3).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=3).value2 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+476	val_238	NULL
+476	val_238	NULL
+194	val_97	NULL
+194	val_97	NULL
+400	val_200	val_200
+400	val_200	val_200
+PREHOOK: query: select * from partition_test_partitioned where dt is not null
+PREHOOK: type: QUERY
+PREHOOK: Input: default@partition_test_partitioned@dt=1
+PREHOOK: Input: default@partition_test_partitioned@dt=2
+PREHOOK: Input: default@partition_test_partitioned@dt=3
+#### A masked pattern was here ####
+POSTHOOK: query: select * from partition_test_partitioned where dt is not null
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@partition_test_partitioned@dt=1
+POSTHOOK: Input: default@partition_test_partitioned@dt=2
+POSTHOOK: Input: default@partition_test_partitioned@dt=3
+#### A masked pattern was here ####
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=3).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=3).value2 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+238	val_238	NULL	1
+238	val_238	NULL	1
+97	val_97	NULL	2
+97	val_97	NULL	2
+200	val_200	val_200	3
+200	val_200	val_200	3
diff --git a/src/ql/src/test/results/clientpositive/partition_wise_fileformat13.q.out b/src/ql/src/test/results/clientpositive/partition_wise_fileformat13.q.out
new file mode 100644
index 0000000..6a85d74
--- /dev/null
+++ b/src/ql/src/test/results/clientpositive/partition_wise_fileformat13.q.out
@@ -0,0 +1,128 @@
+PREHOOK: query: -- This tests that the schema can be changed for partitioned tables for binary serde data for joins
+create table T1(key string, value string) partitioned by (dt string) stored as rcfile
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: -- This tests that the schema can be changed for partitioned tables for binary serde data for joins
+create table T1(key string, value string) partitioned by (dt string) stored as rcfile
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@T1
+PREHOOK: query: alter table T1 set serde 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe'
+PREHOOK: type: ALTERTABLE_SERIALIZER
+PREHOOK: Input: default@t1
+PREHOOK: Output: default@t1
+POSTHOOK: query: alter table T1 set serde 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe'
+POSTHOOK: type: ALTERTABLE_SERIALIZER
+POSTHOOK: Input: default@t1
+POSTHOOK: Output: default@t1
+PREHOOK: query: insert overwrite table T1 partition (dt='1') select * from src where key = 238 or key = 97
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@t1@dt=1
+POSTHOOK: query: insert overwrite table T1 partition (dt='1') select * from src where key = 238 or key = 97
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@t1@dt=1
+POSTHOOK: Lineage: t1 PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: t1 PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: alter table T1 change key key int
+PREHOOK: type: ALTERTABLE_RENAMECOL
+PREHOOK: Input: default@t1
+PREHOOK: Output: default@t1
+POSTHOOK: query: alter table T1 change key key int
+POSTHOOK: type: ALTERTABLE_RENAMECOL
+POSTHOOK: Input: default@t1
+POSTHOOK: Output: default@t1
+POSTHOOK: Lineage: t1 PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: t1 PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: insert overwrite table T1 partition (dt='2') select * from src where key = 238 or key = 97
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@t1@dt=2
+POSTHOOK: query: insert overwrite table T1 partition (dt='2') select * from src where key = 238 or key = 97
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@t1@dt=2
+POSTHOOK: Lineage: t1 PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: t1 PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: t1 PARTITION(dt=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: t1 PARTITION(dt=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: alter table T1 change key key string
+PREHOOK: type: ALTERTABLE_RENAMECOL
+PREHOOK: Input: default@t1
+PREHOOK: Output: default@t1
+POSTHOOK: query: alter table T1 change key key string
+POSTHOOK: type: ALTERTABLE_RENAMECOL
+POSTHOOK: Input: default@t1
+POSTHOOK: Output: default@t1
+POSTHOOK: Lineage: t1 PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: t1 PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: t1 PARTITION(dt=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: t1 PARTITION(dt=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: create table T2(key string, value string) partitioned by (dt string) stored as rcfile
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: create table T2(key string, value string) partitioned by (dt string) stored as rcfile
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@T2
+POSTHOOK: Lineage: t1 PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: t1 PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: t1 PARTITION(dt=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: t1 PARTITION(dt=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: insert overwrite table T2 partition (dt='1') select * from src where key = 238 or key = 97
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@t2@dt=1
+POSTHOOK: query: insert overwrite table T2 partition (dt='1') select * from src where key = 238 or key = 97
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@t2@dt=1
+POSTHOOK: Lineage: t1 PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: t1 PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: t1 PARTITION(dt=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: t1 PARTITION(dt=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: t2 PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: t2 PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: select /* + MAPJOIN(a) */ count(*) FROM T1 a JOIN T2 b ON a.key = b.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t1
+PREHOOK: Input: default@t1@dt=1
+PREHOOK: Input: default@t1@dt=2
+PREHOOK: Input: default@t2
+PREHOOK: Input: default@t2@dt=1
+#### A masked pattern was here ####
+POSTHOOK: query: select /* + MAPJOIN(a) */ count(*) FROM T1 a JOIN T2 b ON a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t1
+POSTHOOK: Input: default@t1@dt=1
+POSTHOOK: Input: default@t1@dt=2
+POSTHOOK: Input: default@t2
+POSTHOOK: Input: default@t2@dt=1
+#### A masked pattern was here ####
+POSTHOOK: Lineage: t1 PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: t1 PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: t1 PARTITION(dt=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: t1 PARTITION(dt=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: t2 PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: t2 PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+16
+PREHOOK: query: select count(*) FROM T1 a JOIN T2 b ON a.key = b.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t1
+PREHOOK: Input: default@t1@dt=1
+PREHOOK: Input: default@t1@dt=2
+PREHOOK: Input: default@t2
+PREHOOK: Input: default@t2@dt=1
+#### A masked pattern was here ####
+POSTHOOK: query: select count(*) FROM T1 a JOIN T2 b ON a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t1
+POSTHOOK: Input: default@t1@dt=1
+POSTHOOK: Input: default@t1@dt=2
+POSTHOOK: Input: default@t2
+POSTHOOK: Input: default@t2@dt=1
+#### A masked pattern was here ####
+POSTHOOK: Lineage: t1 PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: t1 PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: t1 PARTITION(dt=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: t1 PARTITION(dt=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: t2 PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: t2 PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+16
diff --git a/src/ql/src/test/results/clientpositive/partition_wise_fileformat14.q.out b/src/ql/src/test/results/clientpositive/partition_wise_fileformat14.q.out
new file mode 100644
index 0000000..1c52b08
--- /dev/null
+++ b/src/ql/src/test/results/clientpositive/partition_wise_fileformat14.q.out
@@ -0,0 +1,234 @@
+PREHOOK: query: CREATE TABLE tbl1(key int, value string) PARTITIONED by (ds string)
+CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS rcfile
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE tbl1(key int, value string) PARTITIONED by (ds string)
+CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS rcfile
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@tbl1
+PREHOOK: query: CREATE TABLE tbl2(key int, value string) PARTITIONED by (ds string)
+CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS rcfile
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE tbl2(key int, value string) PARTITIONED by (ds string)
+CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS rcfile
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@tbl2
+PREHOOK: query: alter table tbl1 set serde 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe'
+PREHOOK: type: ALTERTABLE_SERIALIZER
+PREHOOK: Input: default@tbl1
+PREHOOK: Output: default@tbl1
+POSTHOOK: query: alter table tbl1 set serde 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe'
+POSTHOOK: type: ALTERTABLE_SERIALIZER
+POSTHOOK: Input: default@tbl1
+POSTHOOK: Output: default@tbl1
+PREHOOK: query: alter table tbl2 set serde 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe'
+PREHOOK: type: ALTERTABLE_SERIALIZER
+PREHOOK: Input: default@tbl2
+PREHOOK: Output: default@tbl2
+POSTHOOK: query: alter table tbl2 set serde 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe'
+POSTHOOK: type: ALTERTABLE_SERIALIZER
+POSTHOOK: Input: default@tbl2
+POSTHOOK: Output: default@tbl2
+PREHOOK: query: insert overwrite table tbl1 partition (ds='1') select * from src where key < 10
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@tbl1@ds=1
+POSTHOOK: query: insert overwrite table tbl1 partition (ds='1') select * from src where key < 10
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@tbl1@ds=1
+POSTHOOK: Lineage: tbl1 PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl1 PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: insert overwrite table tbl2 partition (ds='1') select * from src where key < 10
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@tbl2@ds=1
+POSTHOOK: query: insert overwrite table tbl2 partition (ds='1') select * from src where key < 10
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@tbl2@ds=1
+POSTHOOK: Lineage: tbl1 PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl1 PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl2 PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl2 PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: alter table tbl1 change key key int
+PREHOOK: type: ALTERTABLE_RENAMECOL
+PREHOOK: Input: default@tbl1
+PREHOOK: Output: default@tbl1
+POSTHOOK: query: alter table tbl1 change key key int
+POSTHOOK: type: ALTERTABLE_RENAMECOL
+POSTHOOK: Input: default@tbl1
+POSTHOOK: Output: default@tbl1
+POSTHOOK: Lineage: tbl1 PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl1 PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl2 PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl2 PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: insert overwrite table tbl1 partition (ds='2') select * from src where key < 10
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@tbl1@ds=2
+POSTHOOK: query: insert overwrite table tbl1 partition (ds='2') select * from src where key < 10
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@tbl1@ds=2
+POSTHOOK: Lineage: tbl1 PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl1 PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl1 PARTITION(ds=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl1 PARTITION(ds=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl2 PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl2 PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: alter table tbl1 change key key string
+PREHOOK: type: ALTERTABLE_RENAMECOL
+PREHOOK: Input: default@tbl1
+PREHOOK: Output: default@tbl1
+POSTHOOK: query: alter table tbl1 change key key string
+POSTHOOK: type: ALTERTABLE_RENAMECOL
+POSTHOOK: Input: default@tbl1
+POSTHOOK: Output: default@tbl1
+POSTHOOK: Lineage: tbl1 PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl1 PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl1 PARTITION(ds=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl1 PARTITION(ds=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl2 PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl2 PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: -- The subquery itself is being map-joined. Multiple partitions of tbl1 with different schemas are being read for tbl2
+select /*+mapjoin(subq1)*/ count(*) from 
+  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1 
+    join
+  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
+  on subq1.key = subq2.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@tbl1
+PREHOOK: Input: default@tbl1@ds=1
+PREHOOK: Input: default@tbl1@ds=2
+PREHOOK: Input: default@tbl2
+PREHOOK: Input: default@tbl2@ds=1
+#### A masked pattern was here ####
+POSTHOOK: query: -- The subquery itself is being map-joined. Multiple partitions of tbl1 with different schemas are being read for tbl2
+select /*+mapjoin(subq1)*/ count(*) from 
+  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1 
+    join
+  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
+  on subq1.key = subq2.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@tbl1
+POSTHOOK: Input: default@tbl1@ds=1
+POSTHOOK: Input: default@tbl1@ds=2
+POSTHOOK: Input: default@tbl2
+POSTHOOK: Input: default@tbl2@ds=1
+#### A masked pattern was here ####
+POSTHOOK: Lineage: tbl1 PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl1 PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl1 PARTITION(ds=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl1 PARTITION(ds=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl2 PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl2 PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+40
+PREHOOK: query: -- The subquery itself is being map-joined. Since the sub-query only contains selects and filters, it should 
+-- be converted to a bucketized mapside join. Multiple partitions of tbl1 with different schemas are being read for each
+-- bucket of tbl2
+select /*+mapjoin(subq1)*/ count(*) from 
+  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1 
+    join
+  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
+  on subq1.key = subq2.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@tbl1
+PREHOOK: Input: default@tbl1@ds=1
+PREHOOK: Input: default@tbl1@ds=2
+PREHOOK: Input: default@tbl2
+PREHOOK: Input: default@tbl2@ds=1
+#### A masked pattern was here ####
+POSTHOOK: query: -- The subquery itself is being map-joined. Since the sub-query only contains selects and filters, it should 
+-- be converted to a bucketized mapside join. Multiple partitions of tbl1 with different schemas are being read for each
+-- bucket of tbl2
+select /*+mapjoin(subq1)*/ count(*) from 
+  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1 
+    join
+  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
+  on subq1.key = subq2.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@tbl1
+POSTHOOK: Input: default@tbl1@ds=1
+POSTHOOK: Input: default@tbl1@ds=2
+POSTHOOK: Input: default@tbl2
+POSTHOOK: Input: default@tbl2@ds=1
+#### A masked pattern was here ####
+POSTHOOK: Lineage: tbl1 PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl1 PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl1 PARTITION(ds=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl1 PARTITION(ds=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl2 PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl2 PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+40
+PREHOOK: query: -- The subquery itself is being map-joined. Since the sub-query only contains selects and filters, it should 
+-- be converted to a sort-merge join. Multiple partitions of tbl1 with different schemas are being read for a
+-- given file of tbl2
+select /*+mapjoin(subq1)*/ count(*) from 
+  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1 
+    join
+  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
+  on subq1.key = subq2.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@tbl1
+PREHOOK: Input: default@tbl1@ds=1
+PREHOOK: Input: default@tbl1@ds=2
+PREHOOK: Input: default@tbl2
+PREHOOK: Input: default@tbl2@ds=1
+#### A masked pattern was here ####
+POSTHOOK: query: -- The subquery itself is being map-joined. Since the sub-query only contains selects and filters, it should 
+-- be converted to a sort-merge join. Multiple partitions of tbl1 with different schemas are being read for a
+-- given file of tbl2
+select /*+mapjoin(subq1)*/ count(*) from 
+  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1 
+    join
+  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
+  on subq1.key = subq2.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@tbl1
+POSTHOOK: Input: default@tbl1@ds=1
+POSTHOOK: Input: default@tbl1@ds=2
+POSTHOOK: Input: default@tbl2
+POSTHOOK: Input: default@tbl2@ds=1
+#### A masked pattern was here ####
+POSTHOOK: Lineage: tbl1 PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl1 PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl1 PARTITION(ds=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl1 PARTITION(ds=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl2 PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl2 PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+40
+PREHOOK: query: -- Since the join key is modified by the sub-query, neither sort-merge join not bucketized map-side
+-- join should be performed.  Multiple partitions of tbl1 with different schemas are being read for tbl2
+select /*+mapjoin(subq1)*/ count(*) from 
+  (select a.key+1 as key, concat(a.value, a.value) as value from tbl1 a) subq1 
+    join
+  (select a.key+1 as key, concat(a.value, a.value) as value from tbl2 a) subq2
+  on subq1.key = subq2.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@tbl1
+PREHOOK: Input: default@tbl1@ds=1
+PREHOOK: Input: default@tbl1@ds=2
+PREHOOK: Input: default@tbl2
+PREHOOK: Input: default@tbl2@ds=1
+#### A masked pattern was here ####
+POSTHOOK: query: -- Since the join key is modified by the sub-query, neither sort-merge join not bucketized map-side
+-- join should be performed.  Multiple partitions of tbl1 with different schemas are being read for tbl2
+select /*+mapjoin(subq1)*/ count(*) from 
+  (select a.key+1 as key, concat(a.value, a.value) as value from tbl1 a) subq1 
+    join
+  (select a.key+1 as key, concat(a.value, a.value) as value from tbl2 a) subq2
+  on subq1.key = subq2.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@tbl1
+POSTHOOK: Input: default@tbl1@ds=1
+POSTHOOK: Input: default@tbl1@ds=2
+POSTHOOK: Input: default@tbl2
+POSTHOOK: Input: default@tbl2@ds=1
+#### A masked pattern was here ####
+POSTHOOK: Lineage: tbl1 PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl1 PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl1 PARTITION(ds=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl1 PARTITION(ds=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl2 PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: tbl2 PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+44
diff --git a/src/ql/src/test/results/clientpositive/partition_wise_fileformat8.q.out b/src/ql/src/test/results/clientpositive/partition_wise_fileformat8.q.out
new file mode 100644
index 0000000..94856c4
--- /dev/null
+++ b/src/ql/src/test/results/clientpositive/partition_wise_fileformat8.q.out
@@ -0,0 +1,147 @@
+PREHOOK: query: -- This tests that a query can span multiple partitions which can not only have different file formats, but
+-- also different serdes
+create table partition_test_partitioned(key string, value string) partitioned by (dt string) stored as rcfile
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: -- This tests that a query can span multiple partitions which can not only have different file formats, but
+-- also different serdes
+create table partition_test_partitioned(key string, value string) partitioned by (dt string) stored as rcfile
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@partition_test_partitioned
+PREHOOK: query: insert overwrite table partition_test_partitioned partition(dt='1') select * from src
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@partition_test_partitioned@dt=1
+POSTHOOK: query: insert overwrite table partition_test_partitioned partition(dt='1') select * from src
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@partition_test_partitioned@dt=1
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: alter table partition_test_partitioned set fileformat sequencefile
+PREHOOK: type: ALTERTABLE_FILEFORMAT
+PREHOOK: Input: default@partition_test_partitioned
+PREHOOK: Output: default@partition_test_partitioned
+POSTHOOK: query: alter table partition_test_partitioned set fileformat sequencefile
+POSTHOOK: type: ALTERTABLE_FILEFORMAT
+POSTHOOK: Input: default@partition_test_partitioned
+POSTHOOK: Output: default@partition_test_partitioned
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: insert overwrite table partition_test_partitioned partition(dt='2') select * from src
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@partition_test_partitioned@dt=2
+POSTHOOK: query: insert overwrite table partition_test_partitioned partition(dt='2') select * from src
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@partition_test_partitioned@dt=2
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=2).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: alter table partition_test_partitioned set serde 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
+PREHOOK: type: ALTERTABLE_SERIALIZER
+PREHOOK: Input: default@partition_test_partitioned
+PREHOOK: Output: default@partition_test_partitioned
+POSTHOOK: query: alter table partition_test_partitioned set serde 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
+POSTHOOK: type: ALTERTABLE_SERIALIZER
+POSTHOOK: Input: default@partition_test_partitioned
+POSTHOOK: Output: default@partition_test_partitioned
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=2).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: insert overwrite table partition_test_partitioned partition(dt='3') select * from src
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@partition_test_partitioned@dt=3
+POSTHOOK: query: insert overwrite table partition_test_partitioned partition(dt='3') select * from src
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@partition_test_partitioned@dt=3
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=2).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=3).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: select * from partition_test_partitioned where dt is not null order by key, value, dt limit 20
+PREHOOK: type: QUERY
+PREHOOK: Input: default@partition_test_partitioned
+PREHOOK: Input: default@partition_test_partitioned@dt=1
+PREHOOK: Input: default@partition_test_partitioned@dt=2
+PREHOOK: Input: default@partition_test_partitioned@dt=3
+#### A masked pattern was here ####
+POSTHOOK: query: select * from partition_test_partitioned where dt is not null order by key, value, dt limit 20
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@partition_test_partitioned
+POSTHOOK: Input: default@partition_test_partitioned@dt=1
+POSTHOOK: Input: default@partition_test_partitioned@dt=2
+POSTHOOK: Input: default@partition_test_partitioned@dt=3
+#### A masked pattern was here ####
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=2).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=3).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+0	val_0	1
+0	val_0	1
+0	val_0	1
+0	val_0	2
+0	val_0	2
+0	val_0	2
+0	val_0	3
+0	val_0	3
+0	val_0	3
+10	val_10	1
+10	val_10	2
+10	val_10	3
+100	val_100	1
+100	val_100	1
+100	val_100	2
+100	val_100	2
+100	val_100	3
+100	val_100	3
+103	val_103	1
+103	val_103	1
+PREHOOK: query: select key+key as key, value, dt from partition_test_partitioned where dt is not null order by key, value, dt limit 20
+PREHOOK: type: QUERY
+PREHOOK: Input: default@partition_test_partitioned
+PREHOOK: Input: default@partition_test_partitioned@dt=1
+PREHOOK: Input: default@partition_test_partitioned@dt=2
+PREHOOK: Input: default@partition_test_partitioned@dt=3
+#### A masked pattern was here ####
+POSTHOOK: query: select key+key as key, value, dt from partition_test_partitioned where dt is not null order by key, value, dt limit 20
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@partition_test_partitioned
+POSTHOOK: Input: default@partition_test_partitioned@dt=1
+POSTHOOK: Input: default@partition_test_partitioned@dt=2
+POSTHOOK: Input: default@partition_test_partitioned@dt=3
+#### A masked pattern was here ####
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=2).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=3).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+0.0	val_0	1
+0.0	val_0	1
+0.0	val_0	1
+0.0	val_0	2
+0.0	val_0	2
+0.0	val_0	2
+0.0	val_0	3
+0.0	val_0	3
+0.0	val_0	3
+4.0	val_2	1
+4.0	val_2	2
+4.0	val_2	3
+8.0	val_4	1
+8.0	val_4	2
+8.0	val_4	3
+10.0	val_5	1
+10.0	val_5	1
+10.0	val_5	1
+10.0	val_5	2
+10.0	val_5	2
diff --git a/src/ql/src/test/results/clientpositive/partition_wise_fileformat9.q.out b/src/ql/src/test/results/clientpositive/partition_wise_fileformat9.q.out
new file mode 100644
index 0000000..7dd8781
--- /dev/null
+++ b/src/ql/src/test/results/clientpositive/partition_wise_fileformat9.q.out
@@ -0,0 +1,113 @@
+PREHOOK: query: -- This tests that a query can span multiple partitions which can not only have different file formats, but
+-- also different serdes
+create table partition_test_partitioned(key string, value string) partitioned by (dt string) stored as rcfile
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: -- This tests that a query can span multiple partitions which can not only have different file formats, but
+-- also different serdes
+create table partition_test_partitioned(key string, value string) partitioned by (dt string) stored as rcfile
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@partition_test_partitioned
+PREHOOK: query: insert overwrite table partition_test_partitioned partition(dt='1') select * from src
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@partition_test_partitioned@dt=1
+POSTHOOK: query: insert overwrite table partition_test_partitioned partition(dt='1') select * from src
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@partition_test_partitioned@dt=1
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: alter table partition_test_partitioned set serde 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe'
+PREHOOK: type: ALTERTABLE_SERIALIZER
+PREHOOK: Input: default@partition_test_partitioned
+PREHOOK: Output: default@partition_test_partitioned
+POSTHOOK: query: alter table partition_test_partitioned set serde 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe'
+POSTHOOK: type: ALTERTABLE_SERIALIZER
+POSTHOOK: Input: default@partition_test_partitioned
+POSTHOOK: Output: default@partition_test_partitioned
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: insert overwrite table partition_test_partitioned partition(dt='2') select * from src
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@partition_test_partitioned@dt=2
+POSTHOOK: query: insert overwrite table partition_test_partitioned partition(dt='2') select * from src
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@partition_test_partitioned@dt=2
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=2).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: select * from partition_test_partitioned where dt is not null order by key, value, dt limit 20
+PREHOOK: type: QUERY
+PREHOOK: Input: default@partition_test_partitioned
+PREHOOK: Input: default@partition_test_partitioned@dt=1
+PREHOOK: Input: default@partition_test_partitioned@dt=2
+#### A masked pattern was here ####
+POSTHOOK: query: select * from partition_test_partitioned where dt is not null order by key, value, dt limit 20
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@partition_test_partitioned
+POSTHOOK: Input: default@partition_test_partitioned@dt=1
+POSTHOOK: Input: default@partition_test_partitioned@dt=2
+#### A masked pattern was here ####
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=2).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+0	val_0	1
+0	val_0	1
+0	val_0	1
+0	val_0	2
+0	val_0	2
+0	val_0	2
+10	val_10	1
+10	val_10	2
+100	val_100	1
+100	val_100	1
+100	val_100	2
+100	val_100	2
+103	val_103	1
+103	val_103	1
+103	val_103	2
+103	val_103	2
+104	val_104	1
+104	val_104	1
+104	val_104	2
+104	val_104	2
+PREHOOK: query: select key+key as key, value, dt from partition_test_partitioned where dt is not null order by key, value, dt limit 20
+PREHOOK: type: QUERY
+PREHOOK: Input: default@partition_test_partitioned
+PREHOOK: Input: default@partition_test_partitioned@dt=1
+PREHOOK: Input: default@partition_test_partitioned@dt=2
+#### A masked pattern was here ####
+POSTHOOK: query: select key+key as key, value, dt from partition_test_partitioned where dt is not null order by key, value, dt limit 20
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@partition_test_partitioned
+POSTHOOK: Input: default@partition_test_partitioned@dt=1
+POSTHOOK: Input: default@partition_test_partitioned@dt=2
+#### A masked pattern was here ####
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=2).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+0.0	val_0	1
+0.0	val_0	1
+0.0	val_0	1
+0.0	val_0	2
+0.0	val_0	2
+0.0	val_0	2
+4.0	val_2	1
+4.0	val_2	2
+8.0	val_4	1
+8.0	val_4	2
+10.0	val_5	1
+10.0	val_5	1
+10.0	val_5	1
+10.0	val_5	2
+10.0	val_5	2
+10.0	val_5	2
+16.0	val_8	1
+16.0	val_8	2
+18.0	val_9	1
+18.0	val_9	2
diff --git a/src/ql/src/test/results/clientpositive/pcr.q.out b/src/ql/src/test/results/clientpositive/pcr.q.out
index e170a31..cd3caff 100644
--- a/src/ql/src/test/results/clientpositive/pcr.q.out
+++ b/src/ql/src/test/results/clientpositive/pcr.q.out
@@ -123,7 +123,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 3
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -169,7 +168,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 3
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -323,7 +321,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 3
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -369,7 +366,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 3
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -415,7 +411,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 3
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -613,7 +608,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 3
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -659,7 +653,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 3
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -821,7 +814,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 3
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -867,7 +859,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 3
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -1031,7 +1022,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 3
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -1077,7 +1067,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 3
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -1123,7 +1112,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 3
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -1298,7 +1286,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 3
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -1344,7 +1331,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 3
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -1390,7 +1376,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 3
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -1569,7 +1554,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 3
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -1615,7 +1599,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 3
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -1756,7 +1739,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 3
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -1802,7 +1784,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 3
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -1983,7 +1964,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 3
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -2029,7 +2009,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 3
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -2075,7 +2054,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 3
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -2290,7 +2268,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 3
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -2336,7 +2313,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 3
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -2495,7 +2471,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 3
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -2773,7 +2748,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 3
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -2819,7 +2793,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 3
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -3112,7 +3085,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 4
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -3158,7 +3130,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 4
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -3204,7 +3175,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 4
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -3250,7 +3220,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 4
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -3454,7 +3423,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 4
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -3500,7 +3468,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 4
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -3546,7 +3513,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 4
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -3822,7 +3788,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 4
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -4382,7 +4347,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.pcr_t1
               numFiles 1
-              numPartitions 4
               numRows 20
               partition_columns ds
               rawDataSize 160
@@ -4938,7 +4902,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -5111,7 +5074,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -5158,7 +5120,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -5337,7 +5298,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -5384,7 +5344,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/ppd_union_view.q.out b/src/ql/src/test/results/clientpositive/ppd_union_view.q.out
index ac74d7a..6663943 100644
--- a/src/ql/src/test/results/clientpositive/ppd_union_view.q.out
+++ b/src/ql/src/test/results/clientpositive/ppd_union_view.q.out
@@ -260,7 +260,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.t1_mapping
               numFiles 1
-              numPartitions 2
               numRows 1
               partition_columns ds
               rawDataSize 12
@@ -306,7 +305,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.t1_old
               numFiles 1
-              numPartitions 2
               numRows 1
               partition_columns ds
               rawDataSize 14
@@ -803,7 +801,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.t1_new
               numFiles 1
-              numPartitions 2
               numRows 1
               partition_columns ds
               rawDataSize 11
diff --git a/src/ql/src/test/results/clientpositive/ppr_allchildsarenull.q.out b/src/ql/src/test/results/clientpositive/ppr_allchildsarenull.q.out
index 38e4558..2136a33 100644
--- a/src/ql/src/test/results/clientpositive/ppr_allchildsarenull.q.out
+++ b/src/ql/src/test/results/clientpositive/ppr_allchildsarenull.q.out
@@ -79,7 +79,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -126,7 +125,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -272,7 +270,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -319,7 +316,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -366,7 +362,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -413,7 +408,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/rand_partitionpruner2.q.out b/src/ql/src/test/results/clientpositive/rand_partitionpruner2.q.out
index 3828edb..3092947 100644
--- a/src/ql/src/test/results/clientpositive/rand_partitionpruner2.q.out
+++ b/src/ql/src/test/results/clientpositive/rand_partitionpruner2.q.out
@@ -95,7 +95,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -142,7 +141,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/rand_partitionpruner3.q.out b/src/ql/src/test/results/clientpositive/rand_partitionpruner3.q.out
index c830792..600a834 100644
--- a/src/ql/src/test/results/clientpositive/rand_partitionpruner3.q.out
+++ b/src/ql/src/test/results/clientpositive/rand_partitionpruner3.q.out
@@ -73,7 +73,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -201,7 +200,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/router_join_ppr.q.out b/src/ql/src/test/results/clientpositive/router_join_ppr.q.out
index 8c50519..189e795 100644
--- a/src/ql/src/test/results/clientpositive/router_join_ppr.q.out
+++ b/src/ql/src/test/results/clientpositive/router_join_ppr.q.out
@@ -136,7 +136,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -183,7 +182,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -230,7 +228,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -277,7 +274,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -546,7 +542,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -593,7 +588,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -853,7 +847,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -900,7 +893,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -1160,7 +1152,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -1207,7 +1198,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -1254,7 +1244,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -1301,7 +1290,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/sample1.q.out b/src/ql/src/test/results/clientpositive/sample1.q.out
index d9064f5..a88d2d4 100644
--- a/src/ql/src/test/results/clientpositive/sample1.q.out
+++ b/src/ql/src/test/results/clientpositive/sample1.q.out
@@ -106,7 +106,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/sample10.q.out b/src/ql/src/test/results/clientpositive/sample10.q.out
index 1fdb335..e4fecbe 100644
--- a/src/ql/src/test/results/clientpositive/sample10.q.out
+++ b/src/ql/src/test/results/clientpositive/sample10.q.out
@@ -115,7 +115,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpartbucket
               numFiles 4
-              numPartitions 4
               numRows 10
               partition_columns ds/hr
               rawDataSize 60
@@ -164,7 +163,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpartbucket
               numFiles 4
-              numPartitions 4
               numRows 10
               partition_columns ds/hr
               rawDataSize 60
@@ -213,7 +211,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpartbucket
               numFiles 4
-              numPartitions 4
               numRows 10
               partition_columns ds/hr
               rawDataSize 60
@@ -262,7 +259,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpartbucket
               numFiles 4
-              numPartitions 4
               numRows 10
               partition_columns ds/hr
               rawDataSize 60
diff --git a/src/ql/src/test/results/clientpositive/sample8.q.out b/src/ql/src/test/results/clientpositive/sample8.q.out
index 707a201..8f26dc8 100644
--- a/src/ql/src/test/results/clientpositive/sample8.q.out
+++ b/src/ql/src/test/results/clientpositive/sample8.q.out
@@ -85,7 +85,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -132,7 +131,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -179,7 +177,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -226,7 +223,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/smb_mapjoin_11.q.out b/src/ql/src/test/results/clientpositive/smb_mapjoin_11.q.out
index f978ff2..6f6a6a9 100644
--- a/src/ql/src/test/results/clientpositive/smb_mapjoin_11.q.out
+++ b/src/ql/src/test/results/clientpositive/smb_mapjoin_11.q.out
@@ -136,7 +136,6 @@ STAGE PLANS:
             partition values:
               ds 1
             properties:
-              SORTBUCKETCOLSPREFIX TRUE
               bucket_count 16
               bucket_field_name key
               columns key,value
@@ -144,7 +143,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.test_table1
               numFiles 16
-              numPartitions 1
               numRows 500
               partition_columns ds
               rawDataSize 5312
diff --git a/src/ql/src/test/results/clientpositive/smb_mapjoin_12.q.out b/src/ql/src/test/results/clientpositive/smb_mapjoin_12.q.out
index ed78a59..0764945 100644
--- a/src/ql/src/test/results/clientpositive/smb_mapjoin_12.q.out
+++ b/src/ql/src/test/results/clientpositive/smb_mapjoin_12.q.out
@@ -157,7 +157,6 @@ STAGE PLANS:
             partition values:
               ds 1
             properties:
-              SORTBUCKETCOLSPREFIX TRUE
               bucket_count 16
               bucket_field_name key
               columns key,value
@@ -165,7 +164,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.test_table1
               numFiles 16
-              numPartitions 1
               numRows 500
               partition_columns ds
               rawDataSize 5312
@@ -398,7 +396,6 @@ STAGE PLANS:
             partition values:
               ds 1
             properties:
-              SORTBUCKETCOLSPREFIX TRUE
               bucket_count 16
               bucket_field_name key
               columns key,value
@@ -406,7 +403,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.test_table3
               numFiles 16
-              numPartitions 1
               numRows 3084
               partition_columns ds
               rawDataSize 32904
diff --git a/src/ql/src/test/results/clientpositive/sort_merge_join_desc_5.q.out b/src/ql/src/test/results/clientpositive/sort_merge_join_desc_5.q.out
index ba038c4..9c8368d 100644
--- a/src/ql/src/test/results/clientpositive/sort_merge_join_desc_5.q.out
+++ b/src/ql/src/test/results/clientpositive/sort_merge_join_desc_5.q.out
@@ -120,7 +120,6 @@ STAGE PLANS:
             partition values:
               part 1
             properties:
-              SORTBUCKETCOLSPREFIX TRUE
               bucket_count 1
               bucket_field_name key
               columns key,value
@@ -128,7 +127,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcbucket_mapjoin_part_1
               numFiles 1
-              numPartitions 1
               numRows 500
               partition_columns part
               rawDataSize 5312
diff --git a/src/ql/src/test/results/clientpositive/sort_merge_join_desc_6.q.out b/src/ql/src/test/results/clientpositive/sort_merge_join_desc_6.q.out
index d0157bd..0394c7d 100644
--- a/src/ql/src/test/results/clientpositive/sort_merge_join_desc_6.q.out
+++ b/src/ql/src/test/results/clientpositive/sort_merge_join_desc_6.q.out
@@ -151,7 +151,6 @@ STAGE PLANS:
             partition values:
               part 1
             properties:
-              SORTBUCKETCOLSPREFIX TRUE
               bucket_count 2
               bucket_field_name key
               columns key,value
@@ -159,7 +158,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcbucket_mapjoin_part_1
               numFiles 2
-              numPartitions 1
               numRows 500
               partition_columns part
               rawDataSize 5312
diff --git a/src/ql/src/test/results/clientpositive/sort_merge_join_desc_7.q.out b/src/ql/src/test/results/clientpositive/sort_merge_join_desc_7.q.out
index 7be90e6..493867f 100644
--- a/src/ql/src/test/results/clientpositive/sort_merge_join_desc_7.q.out
+++ b/src/ql/src/test/results/clientpositive/sort_merge_join_desc_7.q.out
@@ -215,7 +215,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcbucket_mapjoin_part_1
               numFiles 2
-              numPartitions 2
               numRows 500
               partition_columns part
               rawDataSize 5312
@@ -263,7 +262,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcbucket_mapjoin_part_1
               numFiles 2
-              numPartitions 2
               numRows 500
               partition_columns part
               rawDataSize 5312
diff --git a/src/ql/src/test/results/clientpositive/stats11.q.out b/src/ql/src/test/results/clientpositive/stats11.q.out
index fda5796..f81ff07 100644
--- a/src/ql/src/test/results/clientpositive/stats11.q.out
+++ b/src/ql/src/test/results/clientpositive/stats11.q.out
@@ -909,7 +909,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcbucket_mapjoin_part
               numFiles 4
-              numPartitions 1
               numRows 0
               partition_columns ds
               rawDataSize 0
@@ -1153,7 +1152,6 @@ STAGE PLANS:
           hdfs directory: true
 #### A masked pattern was here ####
 
-
 PREHOOK: query: insert overwrite table bucketmapjoin_tmp_result 
 select /*+mapjoin(a)*/ a.key, a.value, b.value 
 from srcbucket_mapjoin a join srcbucket_mapjoin_part b 
diff --git a/src/ql/src/test/results/clientpositive/transform_ppr1.q.out b/src/ql/src/test/results/clientpositive/transform_ppr1.q.out
index 1d29048..740a931 100644
--- a/src/ql/src/test/results/clientpositive/transform_ppr1.q.out
+++ b/src/ql/src/test/results/clientpositive/transform_ppr1.q.out
@@ -90,7 +90,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -137,7 +136,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -184,7 +182,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -231,7 +228,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/transform_ppr2.q.out b/src/ql/src/test/results/clientpositive/transform_ppr2.q.out
index 4f547c0..fb8e039 100644
--- a/src/ql/src/test/results/clientpositive/transform_ppr2.q.out
+++ b/src/ql/src/test/results/clientpositive/transform_ppr2.q.out
@@ -92,7 +92,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -139,7 +138,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
diff --git a/src/ql/src/test/results/clientpositive/union22.q.out b/src/ql/src/test/results/clientpositive/union22.q.out
index 26e2616..a4c901a 100644
--- a/src/ql/src/test/results/clientpositive/union22.q.out
+++ b/src/ql/src/test/results/clientpositive/union22.q.out
@@ -194,7 +194,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.dst_union22
               numFiles 1
-              numPartitions 1
               numRows 500
               partition_columns ds
               rawDataSize 11124
@@ -439,7 +438,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.dst_union22_delta
               numFiles 1
-              numPartitions 1
               numRows 500
               partition_columns ds
               rawDataSize 16936
diff --git a/src/ql/src/test/results/clientpositive/union_ppr.q.out b/src/ql/src/test/results/clientpositive/union_ppr.q.out
index a2aefc7..756a9cd 100644
--- a/src/ql/src/test/results/clientpositive/union_ppr.q.out
+++ b/src/ql/src/test/results/clientpositive/union_ppr.q.out
@@ -152,7 +152,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
@@ -199,7 +198,6 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.srcpart
               numFiles 1
-              numPartitions 4
               numRows 0
               partition_columns ds/hr
               rawDataSize 0
diff --git a/src/ql/src/test/results/compiler/plan/case_sensitivity.q.xml b/src/ql/src/test/results/compiler/plan/case_sensitivity.q.xml
index afce19c..8db407f 100644
--- a/src/ql/src/test/results/compiler/plan/case_sensitivity.q.xml
+++ b/src/ql/src/test/results/compiler/plan/case_sensitivity.q.xml
@@ -532,10 +532,6 @@
           <string>default.src_thrift</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string></string> 
          </void> 
@@ -552,18 +548,6 @@
           <string>org.apache.thrift.protocol.TBinaryProtocol</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>serialization.class</string> 
           <string>org.apache.hadoop.hive.serde2.thrift.test.Complex</string> 
          </void> 
@@ -580,10 +564,6 @@
           <string>org.apache.hadoop.mapred.SequenceFileInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>1606</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</string> 
          </void> 
@@ -591,10 +571,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/cast1.q.xml b/src/ql/src/test/results/compiler/plan/cast1.q.xml
index e9deaa4..4bb6df9 100644
--- a/src/ql/src/test/results/compiler/plan/cast1.q.xml
+++ b/src/ql/src/test/results/compiler/plan/cast1.q.xml
@@ -29,10 +29,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -49,18 +45,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -73,10 +57,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -84,10 +64,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/groupby1.q.xml b/src/ql/src/test/results/compiler/plan/groupby1.q.xml
index cd0d6e4..9478ee0 100755
--- a/src/ql/src/test/results/compiler/plan/groupby1.q.xml
+++ b/src/ql/src/test/results/compiler/plan/groupby1.q.xml
@@ -163,10 +163,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -183,18 +179,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -207,10 +191,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -218,10 +198,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/groupby2.q.xml b/src/ql/src/test/results/compiler/plan/groupby2.q.xml
index 7b07f02..abb394c 100755
--- a/src/ql/src/test/results/compiler/plan/groupby2.q.xml
+++ b/src/ql/src/test/results/compiler/plan/groupby2.q.xml
@@ -29,10 +29,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -49,18 +45,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -73,10 +57,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -84,10 +64,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/groupby3.q.xml b/src/ql/src/test/results/compiler/plan/groupby3.q.xml
index a6a1986..e20c3bd 100644
--- a/src/ql/src/test/results/compiler/plan/groupby3.q.xml
+++ b/src/ql/src/test/results/compiler/plan/groupby3.q.xml
@@ -29,10 +29,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -49,18 +45,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -73,10 +57,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -84,10 +64,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/groupby4.q.xml b/src/ql/src/test/results/compiler/plan/groupby4.q.xml
index eab8b68..1b1487e 100644
--- a/src/ql/src/test/results/compiler/plan/groupby4.q.xml
+++ b/src/ql/src/test/results/compiler/plan/groupby4.q.xml
@@ -29,10 +29,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -49,18 +45,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -73,10 +57,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -84,10 +64,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/groupby5.q.xml b/src/ql/src/test/results/compiler/plan/groupby5.q.xml
index 25e3583..f73e9f1 100644
--- a/src/ql/src/test/results/compiler/plan/groupby5.q.xml
+++ b/src/ql/src/test/results/compiler/plan/groupby5.q.xml
@@ -29,10 +29,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -49,18 +45,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -73,10 +57,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -84,10 +64,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/groupby6.q.xml b/src/ql/src/test/results/compiler/plan/groupby6.q.xml
index 9617836..fdd18fd 100644
--- a/src/ql/src/test/results/compiler/plan/groupby6.q.xml
+++ b/src/ql/src/test/results/compiler/plan/groupby6.q.xml
@@ -29,10 +29,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -49,18 +45,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -73,10 +57,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -84,10 +64,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/input1.q.xml b/src/ql/src/test/results/compiler/plan/input1.q.xml
index 2213616..b147e8b 100755
--- a/src/ql/src/test/results/compiler/plan/input1.q.xml
+++ b/src/ql/src/test/results/compiler/plan/input1.q.xml
@@ -532,10 +532,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -552,18 +548,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -576,10 +560,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -587,10 +567,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/input2.q.xml b/src/ql/src/test/results/compiler/plan/input2.q.xml
index b623e75..1e9795a 100755
--- a/src/ql/src/test/results/compiler/plan/input2.q.xml
+++ b/src/ql/src/test/results/compiler/plan/input2.q.xml
@@ -1535,10 +1535,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -1555,18 +1551,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -1579,10 +1563,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -1590,10 +1570,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/input20.q.xml b/src/ql/src/test/results/compiler/plan/input20.q.xml
index 46ed989..7e3b577 100644
--- a/src/ql/src/test/results/compiler/plan/input20.q.xml
+++ b/src/ql/src/test/results/compiler/plan/input20.q.xml
@@ -29,10 +29,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -49,18 +45,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -73,10 +57,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -84,10 +64,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/input3.q.xml b/src/ql/src/test/results/compiler/plan/input3.q.xml
index 3753e87..1b822a8 100755
--- a/src/ql/src/test/results/compiler/plan/input3.q.xml
+++ b/src/ql/src/test/results/compiler/plan/input3.q.xml
@@ -1912,10 +1912,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -1932,18 +1928,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -1956,10 +1940,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -1967,10 +1947,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/input4.q.xml b/src/ql/src/test/results/compiler/plan/input4.q.xml
index 40db98b..2e9d732 100755
--- a/src/ql/src/test/results/compiler/plan/input4.q.xml
+++ b/src/ql/src/test/results/compiler/plan/input4.q.xml
@@ -163,10 +163,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -183,18 +179,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -207,10 +191,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -218,10 +198,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/input5.q.xml b/src/ql/src/test/results/compiler/plan/input5.q.xml
index 5c68ca9..0f4440e 100644
--- a/src/ql/src/test/results/compiler/plan/input5.q.xml
+++ b/src/ql/src/test/results/compiler/plan/input5.q.xml
@@ -163,10 +163,6 @@
           <string>default.src_thrift</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string></string> 
          </void> 
@@ -183,18 +179,6 @@
           <string>org.apache.thrift.protocol.TBinaryProtocol</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>serialization.class</string> 
           <string>org.apache.hadoop.hive.serde2.thrift.test.Complex</string> 
          </void> 
@@ -211,10 +195,6 @@
           <string>org.apache.hadoop.mapred.SequenceFileInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>1606</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</string> 
          </void> 
@@ -222,10 +202,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/input6.q.xml b/src/ql/src/test/results/compiler/plan/input6.q.xml
index 03f2d46..9da42b6 100644
--- a/src/ql/src/test/results/compiler/plan/input6.q.xml
+++ b/src/ql/src/test/results/compiler/plan/input6.q.xml
@@ -532,10 +532,6 @@
           <string>default.src1</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -552,18 +548,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -576,10 +560,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>216</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -587,10 +567,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/input7.q.xml b/src/ql/src/test/results/compiler/plan/input7.q.xml
index b556d4b..4dd4162 100644
--- a/src/ql/src/test/results/compiler/plan/input7.q.xml
+++ b/src/ql/src/test/results/compiler/plan/input7.q.xml
@@ -532,10 +532,6 @@
           <string>default.src1</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -552,18 +548,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -576,10 +560,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>216</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -587,10 +567,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/input8.q.xml b/src/ql/src/test/results/compiler/plan/input8.q.xml
index 3a07ff6..9767451 100644
--- a/src/ql/src/test/results/compiler/plan/input8.q.xml
+++ b/src/ql/src/test/results/compiler/plan/input8.q.xml
@@ -29,10 +29,6 @@
           <string>default.src1</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -49,18 +45,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -73,10 +57,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>216</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -84,10 +64,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/input9.q.xml b/src/ql/src/test/results/compiler/plan/input9.q.xml
index 70ccf3b..a431ce0 100644
--- a/src/ql/src/test/results/compiler/plan/input9.q.xml
+++ b/src/ql/src/test/results/compiler/plan/input9.q.xml
@@ -532,10 +532,6 @@
           <string>default.src1</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -552,18 +548,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -576,10 +560,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>216</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -587,10 +567,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/input_part1.q.xml b/src/ql/src/test/results/compiler/plan/input_part1.q.xml
index 8375c9d..05ebbc5 100644
--- a/src/ql/src/test/results/compiler/plan/input_part1.q.xml
+++ b/src/ql/src/test/results/compiler/plan/input_part1.q.xml
@@ -39,7 +39,7 @@
          </void> 
          <void method="put"> 
           <string>numFiles</string> 
-          <string>4</string> 
+          <string>1</string> 
          </void> 
          <void method="put"> 
           <string>columns.types</string> 
@@ -66,10 +66,6 @@
           <string>0</string> 
          </void> 
          <void method="put"> 
-          <string>numPartitions</string> 
-          <string>4</string> 
-         </void> 
-         <void method="put"> 
           <string>partition_columns</string> 
           <string>ds/hr</string> 
          </void> 
@@ -87,7 +83,7 @@
          </void> 
          <void method="put"> 
           <string>totalSize</string> 
-          <string>23248</string> 
+          <string>5812</string> 
          </void> 
          <void method="put"> 
           <string>file.outputformat</string> 
@@ -865,10 +861,6 @@
            <string>0</string> 
           </void> 
           <void method="put"> 
-           <string>numPartitions</string> 
-           <string>4</string> 
-          </void> 
-          <void method="put"> 
            <string>partition_columns</string> 
            <string>ds/hr</string> 
           </void> 
diff --git a/src/ql/src/test/results/compiler/plan/input_testsequencefile.q.xml b/src/ql/src/test/results/compiler/plan/input_testsequencefile.q.xml
index 5eed0f4..8b04931 100644
--- a/src/ql/src/test/results/compiler/plan/input_testsequencefile.q.xml
+++ b/src/ql/src/test/results/compiler/plan/input_testsequencefile.q.xml
@@ -532,10 +532,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -552,18 +548,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -576,10 +560,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -587,10 +567,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/input_testxpath.q.xml b/src/ql/src/test/results/compiler/plan/input_testxpath.q.xml
index 327e260..a055bbf 100644
--- a/src/ql/src/test/results/compiler/plan/input_testxpath.q.xml
+++ b/src/ql/src/test/results/compiler/plan/input_testxpath.q.xml
@@ -29,10 +29,6 @@
           <string>default.src_thrift</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string></string> 
          </void> 
@@ -49,18 +45,6 @@
           <string>org.apache.thrift.protocol.TBinaryProtocol</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>serialization.class</string> 
           <string>org.apache.hadoop.hive.serde2.thrift.test.Complex</string> 
          </void> 
@@ -77,10 +61,6 @@
           <string>org.apache.hadoop.mapred.SequenceFileInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>1606</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</string> 
          </void> 
@@ -88,10 +68,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/input_testxpath2.q.xml b/src/ql/src/test/results/compiler/plan/input_testxpath2.q.xml
index 476e442..27b4ed6 100644
--- a/src/ql/src/test/results/compiler/plan/input_testxpath2.q.xml
+++ b/src/ql/src/test/results/compiler/plan/input_testxpath2.q.xml
@@ -29,10 +29,6 @@
           <string>default.src_thrift</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string></string> 
          </void> 
@@ -49,18 +45,6 @@
           <string>org.apache.thrift.protocol.TBinaryProtocol</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>serialization.class</string> 
           <string>org.apache.hadoop.hive.serde2.thrift.test.Complex</string> 
          </void> 
@@ -77,10 +61,6 @@
           <string>org.apache.hadoop.mapred.SequenceFileInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>1606</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</string> 
          </void> 
@@ -88,10 +68,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/join1.q.xml b/src/ql/src/test/results/compiler/plan/join1.q.xml
index 9055425..223e18e 100644
--- a/src/ql/src/test/results/compiler/plan/join1.q.xml
+++ b/src/ql/src/test/results/compiler/plan/join1.q.xml
@@ -163,10 +163,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -183,18 +179,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -207,10 +191,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -218,10 +198,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
@@ -335,10 +311,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -355,18 +327,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -379,10 +339,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -390,10 +346,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/join2.q.xml b/src/ql/src/test/results/compiler/plan/join2.q.xml
index 6a03abf..690fcc5 100644
--- a/src/ql/src/test/results/compiler/plan/join2.q.xml
+++ b/src/ql/src/test/results/compiler/plan/join2.q.xml
@@ -174,10 +174,6 @@
               <string>default.src</string> 
              </void> 
              <void method="put"> 
-              <string>numFiles</string> 
-              <string>1</string> 
-             </void> 
-             <void method="put"> 
               <string>columns.types</string> 
               <string>string:string</string> 
              </void> 
@@ -194,18 +190,6 @@
               <string>key,value</string> 
              </void> 
              <void method="put"> 
-              <string>rawDataSize</string> 
-              <string>0</string> 
-             </void> 
-             <void method="put"> 
-              <string>numRows</string> 
-              <string>0</string> 
-             </void> 
-             <void method="put"> 
-              <string>numPartitions</string> 
-              <string>0</string> 
-             </void> 
-             <void method="put"> 
               <string>bucket_count</string> 
               <string>-1</string> 
              </void> 
@@ -218,10 +202,6 @@
               <string>org.apache.hadoop.mapred.TextInputFormat</string> 
              </void> 
              <void method="put"> 
-              <string>totalSize</string> 
-              <string>5812</string> 
-             </void> 
-             <void method="put"> 
               <string>file.outputformat</string> 
               <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
              </void> 
@@ -229,10 +209,6 @@
               <string>location</string> 
               #### A masked pattern was here #### 
              </void> 
-             <void method="put"> 
-              <string>transient_lastDdlTime</string> 
-              #### A masked pattern was here #### 
-             </void> 
             </object> 
            </void> 
            <void property="serdeClassName"> 
@@ -1258,6 +1234,12 @@
                     <void property="gatherStats"> 
                      <boolean>true</boolean> 
                     </void> 
+                    <void property="lbCtx"> 
+                     <object idref="ListBucketingCtx0"/> 
+                    </void> 
+                    <void property="maxStatsKeyPrefixLength"> 
+                     <int>200</int> 
+                    </void> 
                     <void property="numFiles"> 
                      <int>1</int> 
                     </void> 
@@ -1748,10 +1730,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -1768,18 +1746,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -1792,10 +1758,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -1803,10 +1765,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
@@ -1920,10 +1878,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -1940,18 +1894,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -1964,10 +1906,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -1975,10 +1913,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/join3.q.xml b/src/ql/src/test/results/compiler/plan/join3.q.xml
index 89be4a8..032b3cf 100644
--- a/src/ql/src/test/results/compiler/plan/join3.q.xml
+++ b/src/ql/src/test/results/compiler/plan/join3.q.xml
@@ -163,10 +163,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -183,18 +179,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -207,10 +191,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -218,10 +198,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
@@ -335,10 +311,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -355,18 +327,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -379,10 +339,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -390,10 +346,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
@@ -507,10 +459,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -527,18 +475,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -551,10 +487,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -562,10 +494,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/join4.q.xml b/src/ql/src/test/results/compiler/plan/join4.q.xml
index 105ac89..138a8db 100644
--- a/src/ql/src/test/results/compiler/plan/join4.q.xml
+++ b/src/ql/src/test/results/compiler/plan/join4.q.xml
@@ -29,10 +29,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -49,18 +45,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -73,10 +57,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -84,10 +64,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
@@ -201,10 +177,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -221,18 +193,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -245,10 +205,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -256,10 +212,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/join5.q.xml b/src/ql/src/test/results/compiler/plan/join5.q.xml
index d6963a7..59a64fd 100644
--- a/src/ql/src/test/results/compiler/plan/join5.q.xml
+++ b/src/ql/src/test/results/compiler/plan/join5.q.xml
@@ -29,10 +29,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -49,18 +45,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -73,10 +57,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -84,10 +64,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
@@ -201,10 +177,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -221,18 +193,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -245,10 +205,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -256,10 +212,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/join6.q.xml b/src/ql/src/test/results/compiler/plan/join6.q.xml
index c4d5b58..9602cbd 100644
--- a/src/ql/src/test/results/compiler/plan/join6.q.xml
+++ b/src/ql/src/test/results/compiler/plan/join6.q.xml
@@ -29,10 +29,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -49,18 +45,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -73,10 +57,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -84,10 +64,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
@@ -201,10 +177,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -221,18 +193,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -245,10 +205,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -256,10 +212,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/join7.q.xml b/src/ql/src/test/results/compiler/plan/join7.q.xml
index b24e0ca..9db5901 100644
--- a/src/ql/src/test/results/compiler/plan/join7.q.xml
+++ b/src/ql/src/test/results/compiler/plan/join7.q.xml
@@ -29,10 +29,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -49,18 +45,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -73,10 +57,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -84,10 +64,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
@@ -201,10 +177,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -221,18 +193,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -245,10 +205,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -256,10 +212,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
@@ -373,10 +325,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -393,18 +341,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -417,10 +353,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -428,10 +360,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/join8.q.xml b/src/ql/src/test/results/compiler/plan/join8.q.xml
index c5ed73d..a7ea611 100644
--- a/src/ql/src/test/results/compiler/plan/join8.q.xml
+++ b/src/ql/src/test/results/compiler/plan/join8.q.xml
@@ -29,10 +29,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -49,18 +45,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -73,10 +57,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -84,10 +64,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
@@ -201,10 +177,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -221,18 +193,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -245,10 +205,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -256,10 +212,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/sample1.q.xml b/src/ql/src/test/results/compiler/plan/sample1.q.xml
index a7a94ad..3374432 100644
--- a/src/ql/src/test/results/compiler/plan/sample1.q.xml
+++ b/src/ql/src/test/results/compiler/plan/sample1.q.xml
@@ -39,7 +39,7 @@
          </void> 
          <void method="put"> 
           <string>numFiles</string> 
-          <string>4</string> 
+          <string>1</string> 
          </void> 
          <void method="put"> 
           <string>columns.types</string> 
@@ -66,10 +66,6 @@
           <string>0</string> 
          </void> 
          <void method="put"> 
-          <string>numPartitions</string> 
-          <string>4</string> 
-         </void> 
-         <void method="put"> 
           <string>partition_columns</string> 
           <string>ds/hr</string> 
          </void> 
@@ -87,7 +83,7 @@
          </void> 
          <void method="put"> 
           <string>totalSize</string> 
-          <string>23248</string> 
+          <string>5812</string> 
          </void> 
          <void method="put"> 
           <string>file.outputformat</string> 
@@ -986,10 +982,6 @@
            <string>0</string> 
           </void> 
           <void method="put"> 
-           <string>numPartitions</string> 
-           <string>4</string> 
-          </void> 
-          <void method="put"> 
            <string>partition_columns</string> 
            <string>ds/hr</string> 
           </void> 
diff --git a/src/ql/src/test/results/compiler/plan/sample2.q.xml b/src/ql/src/test/results/compiler/plan/sample2.q.xml
index d6bc00c..10dd4f0 100644
--- a/src/ql/src/test/results/compiler/plan/sample2.q.xml
+++ b/src/ql/src/test/results/compiler/plan/sample2.q.xml
@@ -532,10 +532,6 @@
           <string>default.srcbucket</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>2</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>int:string</string> 
          </void> 
@@ -556,18 +552,6 @@
           <string>1</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>2</string> 
          </void> 
@@ -580,10 +564,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>11603</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -591,10 +571,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/sample3.q.xml b/src/ql/src/test/results/compiler/plan/sample3.q.xml
index a3950e9..933926c 100644
--- a/src/ql/src/test/results/compiler/plan/sample3.q.xml
+++ b/src/ql/src/test/results/compiler/plan/sample3.q.xml
@@ -532,10 +532,6 @@
           <string>default.srcbucket</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>2</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>int:string</string> 
          </void> 
@@ -556,18 +552,6 @@
           <string>1</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>2</string> 
          </void> 
@@ -580,10 +564,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>11603</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -591,10 +571,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/sample4.q.xml b/src/ql/src/test/results/compiler/plan/sample4.q.xml
index d6bc00c..10dd4f0 100644
--- a/src/ql/src/test/results/compiler/plan/sample4.q.xml
+++ b/src/ql/src/test/results/compiler/plan/sample4.q.xml
@@ -532,10 +532,6 @@
           <string>default.srcbucket</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>2</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>int:string</string> 
          </void> 
@@ -556,18 +552,6 @@
           <string>1</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>2</string> 
          </void> 
@@ -580,10 +564,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>11603</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -591,10 +571,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/sample5.q.xml b/src/ql/src/test/results/compiler/plan/sample5.q.xml
index 08455f9..c4137d1 100644
--- a/src/ql/src/test/results/compiler/plan/sample5.q.xml
+++ b/src/ql/src/test/results/compiler/plan/sample5.q.xml
@@ -532,10 +532,6 @@
           <string>default.srcbucket</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>2</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>int:string</string> 
          </void> 
@@ -556,18 +552,6 @@
           <string>1</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>2</string> 
          </void> 
@@ -580,10 +564,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>11603</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -591,10 +571,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/sample6.q.xml b/src/ql/src/test/results/compiler/plan/sample6.q.xml
index 4ba57e8..bbfdc93 100644
--- a/src/ql/src/test/results/compiler/plan/sample6.q.xml
+++ b/src/ql/src/test/results/compiler/plan/sample6.q.xml
@@ -532,10 +532,6 @@
           <string>default.srcbucket</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>2</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>int:string</string> 
          </void> 
@@ -556,18 +552,6 @@
           <string>1</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>2</string> 
          </void> 
@@ -580,10 +564,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>11603</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -591,10 +571,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/sample7.q.xml b/src/ql/src/test/results/compiler/plan/sample7.q.xml
index 2153d4c..37a9f14 100644
--- a/src/ql/src/test/results/compiler/plan/sample7.q.xml
+++ b/src/ql/src/test/results/compiler/plan/sample7.q.xml
@@ -532,10 +532,6 @@
           <string>default.srcbucket</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>2</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>int:string</string> 
          </void> 
@@ -556,18 +552,6 @@
           <string>1</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>2</string> 
          </void> 
@@ -580,10 +564,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>11603</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -591,10 +571,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/subq.q.xml b/src/ql/src/test/results/compiler/plan/subq.q.xml
index 64f8dca..61a179c 100644
--- a/src/ql/src/test/results/compiler/plan/subq.q.xml
+++ b/src/ql/src/test/results/compiler/plan/subq.q.xml
@@ -427,10 +427,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -447,18 +443,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -471,10 +455,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -482,10 +462,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/udf1.q.xml b/src/ql/src/test/results/compiler/plan/udf1.q.xml
index 0c23042..5a81d07 100644
--- a/src/ql/src/test/results/compiler/plan/udf1.q.xml
+++ b/src/ql/src/test/results/compiler/plan/udf1.q.xml
@@ -29,10 +29,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -49,18 +45,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -73,10 +57,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -84,10 +64,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/udf4.q.xml b/src/ql/src/test/results/compiler/plan/udf4.q.xml
index 4ed9a8d..091b16e 100644
--- a/src/ql/src/test/results/compiler/plan/udf4.q.xml
+++ b/src/ql/src/test/results/compiler/plan/udf4.q.xml
@@ -64,10 +64,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/udf6.q.xml b/src/ql/src/test/results/compiler/plan/udf6.q.xml
index 2e47307..9bf5e19 100644
--- a/src/ql/src/test/results/compiler/plan/udf6.q.xml
+++ b/src/ql/src/test/results/compiler/plan/udf6.q.xml
@@ -29,10 +29,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -49,18 +45,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -73,10 +57,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -84,10 +64,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/udf_case.q.xml b/src/ql/src/test/results/compiler/plan/udf_case.q.xml
index c4dbc2f..aba590c 100644
--- a/src/ql/src/test/results/compiler/plan/udf_case.q.xml
+++ b/src/ql/src/test/results/compiler/plan/udf_case.q.xml
@@ -29,10 +29,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -49,18 +45,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -73,10 +57,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -84,10 +64,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/udf_when.q.xml b/src/ql/src/test/results/compiler/plan/udf_when.q.xml
index af1ae51..7c0e3aa 100644
--- a/src/ql/src/test/results/compiler/plan/udf_when.q.xml
+++ b/src/ql/src/test/results/compiler/plan/udf_when.q.xml
@@ -29,10 +29,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -49,18 +45,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -73,10 +57,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -84,10 +64,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/ql/src/test/results/compiler/plan/union.q.xml b/src/ql/src/test/results/compiler/plan/union.q.xml
index bc0e04b..22f2f0e 100644
--- a/src/ql/src/test/results/compiler/plan/union.q.xml
+++ b/src/ql/src/test/results/compiler/plan/union.q.xml
@@ -427,10 +427,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -447,18 +443,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -471,10 +455,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -482,10 +462,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
@@ -599,10 +575,6 @@
           <string>default.src</string> 
          </void> 
          <void method="put"> 
-          <string>numFiles</string> 
-          <string>1</string> 
-         </void> 
-         <void method="put"> 
           <string>columns.types</string> 
           <string>string:string</string> 
          </void> 
@@ -619,18 +591,6 @@
           <string>key,value</string> 
          </void> 
          <void method="put"> 
-          <string>rawDataSize</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numRows</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
-          <string>numPartitions</string> 
-          <string>0</string> 
-         </void> 
-         <void method="put"> 
           <string>bucket_count</string> 
           <string>-1</string> 
          </void> 
@@ -643,10 +603,6 @@
           <string>org.apache.hadoop.mapred.TextInputFormat</string> 
          </void> 
          <void method="put"> 
-          <string>totalSize</string> 
-          <string>5812</string> 
-         </void> 
-         <void method="put"> 
           <string>file.outputformat</string> 
           <string>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</string> 
          </void> 
@@ -654,10 +610,6 @@
           <string>location</string> 
           #### A masked pattern was here #### 
          </void> 
-         <void method="put"> 
-          <string>transient_lastDdlTime</string> 
-          #### A masked pattern was here #### 
-         </void> 
         </object> 
        </void> 
        <void property="serdeClassName"> 
diff --git a/src/serde/src/java/org/apache/hadoop/hive/serde2/NullStructSerDe.java b/src/serde/src/java/org/apache/hadoop/hive/serde2/NullStructSerDe.java
index c279e92..cd7c539 100644
--- a/src/serde/src/java/org/apache/hadoop/hive/serde2/NullStructSerDe.java
+++ b/src/serde/src/java/org/apache/hadoop/hive/serde2/NullStructSerDe.java
@@ -56,32 +56,11 @@ public class NullStructSerDe implements SerDe {
     return null;
   }
 
+  private static ObjectInspector nullStructOI = new NullStructSerDeObjectInspector();
+
   @Override
   public ObjectInspector getObjectInspector() throws SerDeException {
-    return new StructObjectInspector() {
-      public String getTypeName() {
-        return "null";
-      }
-      public Category getCategory() {
-        return Category.PRIMITIVE;
-      }
-      @Override
-      public StructField getStructFieldRef(String fieldName) {
-        return null;
-      }
-      @Override
-      public List<NullStructField> getAllStructFieldRefs() {
-        return new ArrayList<NullStructField>();
-      }
-      @Override
-      public Object getStructFieldData(Object data, StructField fieldRef) {
-        return null;
-      }
-      @Override
-      public List<Object> getStructFieldsDataAsList(Object data) {
-        return new ArrayList<Object>();
-      }
-    };
+    return nullStructOI;
   }
 
   @Override
@@ -103,4 +82,38 @@ public class NullStructSerDe implements SerDe {
     return NullWritable.get();
   }
 
+
+  /**
+   * A object inspector for null struct serde.
+   */
+  public static class NullStructSerDeObjectInspector extends StructObjectInspector {
+    public String getTypeName() {
+      return "null";
+    }
+
+    public Category getCategory() {
+      return Category.PRIMITIVE;
+    }
+
+    @Override
+    public StructField getStructFieldRef(String fieldName) {
+      return null;
+    }
+
+    @Override
+    public List<NullStructField> getAllStructFieldRefs() {
+      return new ArrayList<NullStructField>();
+    }
+
+    @Override
+    public Object getStructFieldData(Object data, StructField fieldRef) {
+      return null;
+    }
+
+    @Override
+    public List<Object> getStructFieldsDataAsList(Object data) {
+      return new ArrayList<Object>();
+    }
+  }
+
 }
diff --git a/src/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorConverters.java b/src/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorConverters.java
index 6e1aac0..87b1b28 100644
--- a/src/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorConverters.java
+++ b/src/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorConverters.java
@@ -24,9 +24,9 @@ import java.util.Map;
 
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaStringObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableBooleanObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableBigDecimalObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableBinaryObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableBooleanObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableByteObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableDoubleObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableFloatObjectInspector;
@@ -34,6 +34,7 @@ import org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableIntObject
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableLongObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableShortObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableTimestampObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.VoidObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector;
 
 /**
@@ -59,6 +60,64 @@ public final class ObjectInspectorConverters {
     }
   }
 
+  private static Converter getConverter(PrimitiveObjectInspector inputOI,
+      PrimitiveObjectInspector outputOI) {
+    switch (outputOI.getPrimitiveCategory()) {
+    case BOOLEAN:
+      return new PrimitiveObjectInspectorConverter.BooleanConverter(
+          inputOI,
+          (SettableBooleanObjectInspector) outputOI);
+    case BYTE:
+      return new PrimitiveObjectInspectorConverter.ByteConverter(
+          inputOI,
+          (SettableByteObjectInspector) outputOI);
+    case SHORT:
+      return new PrimitiveObjectInspectorConverter.ShortConverter(
+          inputOI,
+          (SettableShortObjectInspector) outputOI);
+    case INT:
+      return new PrimitiveObjectInspectorConverter.IntConverter(
+          inputOI,
+          (SettableIntObjectInspector) outputOI);
+    case LONG:
+      return new PrimitiveObjectInspectorConverter.LongConverter(
+          inputOI,
+          (SettableLongObjectInspector) outputOI);
+    case FLOAT:
+      return new PrimitiveObjectInspectorConverter.FloatConverter(
+          inputOI,
+          (SettableFloatObjectInspector) outputOI);
+    case DOUBLE:
+      return new PrimitiveObjectInspectorConverter.DoubleConverter(
+          inputOI,
+          (SettableDoubleObjectInspector) outputOI);
+    case STRING:
+      if (outputOI instanceof WritableStringObjectInspector) {
+        return new PrimitiveObjectInspectorConverter.TextConverter(
+            inputOI);
+      } else if (outputOI instanceof JavaStringObjectInspector) {
+        return new PrimitiveObjectInspectorConverter.StringConverter(
+            inputOI);
+      }
+    case TIMESTAMP:
+      return new PrimitiveObjectInspectorConverter.TimestampConverter(
+          inputOI,
+          (SettableTimestampObjectInspector) outputOI);
+    case BINARY:
+      return new PrimitiveObjectInspectorConverter.BinaryConverter(
+          inputOI,
+          (SettableBinaryObjectInspector)outputOI);
+    case DECIMAL:
+      return new PrimitiveObjectInspectorConverter.BigDecimalConverter(
+          (PrimitiveObjectInspector) inputOI,
+          (SettableBigDecimalObjectInspector) outputOI);
+    default:
+      throw new RuntimeException("Hive internal error: conversion of "
+          + inputOI.getTypeName() + " to " + outputOI.getTypeName()
+          + " not supported yet.");
+    }
+  }
+
   /**
    * Returns a converter that converts objects from one OI to another OI. The
    * returned (converted) object belongs to this converter, so that it can be
@@ -68,66 +127,12 @@ public final class ObjectInspectorConverters {
       ObjectInspector outputOI) {
     // If the inputOI is the same as the outputOI, just return an
     // IdentityConverter.
-    if (inputOI == outputOI) {
+    if (inputOI.equals(outputOI)) {
       return new IdentityConverter();
     }
     switch (outputOI.getCategory()) {
     case PRIMITIVE:
-      switch (((PrimitiveObjectInspector) outputOI).getPrimitiveCategory()) {
-      case BOOLEAN:
-        return new PrimitiveObjectInspectorConverter.BooleanConverter(
-            (PrimitiveObjectInspector) inputOI,
-            (SettableBooleanObjectInspector) outputOI);
-      case BYTE:
-        return new PrimitiveObjectInspectorConverter.ByteConverter(
-            (PrimitiveObjectInspector) inputOI,
-            (SettableByteObjectInspector) outputOI);
-      case SHORT:
-        return new PrimitiveObjectInspectorConverter.ShortConverter(
-            (PrimitiveObjectInspector) inputOI,
-            (SettableShortObjectInspector) outputOI);
-      case INT:
-        return new PrimitiveObjectInspectorConverter.IntConverter(
-            (PrimitiveObjectInspector) inputOI,
-            (SettableIntObjectInspector) outputOI);
-      case LONG:
-        return new PrimitiveObjectInspectorConverter.LongConverter(
-            (PrimitiveObjectInspector) inputOI,
-            (SettableLongObjectInspector) outputOI);
-      case FLOAT:
-        return new PrimitiveObjectInspectorConverter.FloatConverter(
-            (PrimitiveObjectInspector) inputOI,
-            (SettableFloatObjectInspector) outputOI);
-      case DOUBLE:
-        return new PrimitiveObjectInspectorConverter.DoubleConverter(
-            (PrimitiveObjectInspector) inputOI,
-            (SettableDoubleObjectInspector) outputOI);
-      case STRING:
-        if (outputOI instanceof WritableStringObjectInspector) {
-          return new PrimitiveObjectInspectorConverter.TextConverter(
-              (PrimitiveObjectInspector) inputOI);
-        } else if (outputOI instanceof JavaStringObjectInspector) {
-          return new PrimitiveObjectInspectorConverter.StringConverter(
-              (PrimitiveObjectInspector) inputOI);
-        }
-      case TIMESTAMP:
-        return new PrimitiveObjectInspectorConverter.TimestampConverter(
-            (PrimitiveObjectInspector) inputOI,
-            (SettableTimestampObjectInspector) outputOI);
-      case BINARY:
-        return new PrimitiveObjectInspectorConverter.BinaryConverter(
-            (PrimitiveObjectInspector)inputOI,
-            (SettableBinaryObjectInspector)outputOI);
-      case DECIMAL:
-        return new PrimitiveObjectInspectorConverter.BigDecimalConverter(
-            (PrimitiveObjectInspector) inputOI,
-            (SettableBigDecimalObjectInspector) outputOI);
-
-      default:
-        throw new RuntimeException("Hive internal error: conversion of "
-            + inputOI.getTypeName() + " to " + outputOI.getTypeName()
-            + " not supported yet.");
-      }
+      return getConverter((PrimitiveObjectInspector) inputOI, (PrimitiveObjectInspector) outputOI);
     case STRUCT:
       return new StructConverter((StructObjectInspector) inputOI,
           (SettableStructObjectInspector) outputOI);
@@ -144,6 +149,50 @@ public final class ObjectInspectorConverters {
     }
   }
 
+  public static ObjectInspector getConvertedOI(
+      ObjectInspector inputOI,
+      ObjectInspector outputOI) {
+    // If the inputOI is the same as the outputOI, just return it
+    if (inputOI.equals(outputOI)) {
+      return outputOI;
+    }
+    switch (outputOI.getCategory()) {
+    case PRIMITIVE:
+      return outputOI;
+    case STRUCT:
+      StructObjectInspector structOutputOI = (StructObjectInspector) outputOI;
+      if (structOutputOI.isSettable()) {
+        return outputOI;
+      }
+      else {
+        // create a standard settable struct object inspector
+        List<? extends StructField> listFields = structOutputOI.getAllStructFieldRefs();
+        List<String> structFieldNames = new ArrayList<String>(listFields.size());
+        List<ObjectInspector> structFieldObjectInspectors = new ArrayList<ObjectInspector>(
+            listFields.size());
+
+        for (StructField listField : listFields) {
+          structFieldNames.add(listField.getFieldName());
+          structFieldObjectInspectors.add(listField.getFieldObjectInspector());
+        }
+
+        StandardStructObjectInspector structStandardOutputOI = ObjectInspectorFactory
+            .getStandardStructObjectInspector(
+                structFieldNames,
+                structFieldObjectInspectors);
+        return structStandardOutputOI;
+      }
+    case LIST:
+      return outputOI;
+    case MAP:
+      return outputOI;
+    default:
+      throw new RuntimeException("Hive internal error: conversion of "
+          + inputOI.getTypeName() + " to " + outputOI.getTypeName()
+          + " not supported yet.");
+    }
+  }
+
   /**
    * A converter class for List.
    */
@@ -214,18 +263,25 @@ public final class ObjectInspectorConverters {
 
     public StructConverter(StructObjectInspector inputOI,
         SettableStructObjectInspector outputOI) {
-
-      this.inputOI = inputOI;
-      this.outputOI = outputOI;
-      inputFields = inputOI.getAllStructFieldRefs();
-      outputFields = outputOI.getAllStructFieldRefs();
-      assert (inputFields.size() == outputFields.size());
-
-      fieldConverters = new ArrayList<Converter>(inputFields.size());
-      for (int f = 0; f < inputFields.size(); f++) {
-        fieldConverters.add(getConverter(inputFields.get(f)
-            .getFieldObjectInspector(), outputFields.get(f)
-            .getFieldObjectInspector()));
+      if (inputOI instanceof StructObjectInspector) {
+        this.inputOI = (StructObjectInspector)inputOI;
+        this.outputOI = outputOI;
+        inputFields = this.inputOI.getAllStructFieldRefs();
+        outputFields = outputOI.getAllStructFieldRefs();
+
+        // If the output has some extra fields, set them to NULL.
+        int minFields = Math.min(inputFields.size(), outputFields.size());
+        fieldConverters = new ArrayList<Converter>(minFields);
+        for (int f = 0; f < minFields; f++) {
+          fieldConverters.add(getConverter(inputFields.get(f)
+              .getFieldObjectInspector(), outputFields.get(f)
+              .getFieldObjectInspector()));
+        }
+        output = outputOI.create();
+      } else if (!(inputOI instanceof VoidObjectInspector)) {
+        throw new RuntimeException("Hive internal error: conversion of " +
+            inputOI.getTypeName() + " to " + outputOI.getTypeName() +
+            "not supported yet.");
       }
       output = outputOI.create();
     }
@@ -236,15 +292,19 @@ public final class ObjectInspectorConverters {
         return null;
       }
 
+      int minFields = Math.min(inputFields.size(), outputFields.size());
       // Convert the fields
-      for (int f = 0; f < inputFields.size(); f++) {
-        Object inputFieldValue = inputOI.getStructFieldData(input, inputFields
-            .get(f));
-        Object outputFieldValue = fieldConverters.get(f).convert(
-            inputFieldValue);
-        outputOI.setStructFieldData(output, outputFields.get(f),
-            outputFieldValue);
+      for (int f = 0; f < minFields; f++) {
+        Object inputFieldValue = inputOI.getStructFieldData(input, inputFields.get(f));
+        Object outputFieldValue = fieldConverters.get(f).convert(inputFieldValue);
+        outputOI.setStructFieldData(output, outputFields.get(f), outputFieldValue);
+      }
+
+      // set the extra fields to null
+      for (int f = minFields; f < outputFields.size(); f++) {
+        outputOI.setStructFieldData(output, outputFields.get(f), null);
       }
+
       return output;
     }
   }
diff --git a/src/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/SettableStructObjectInspector.java b/src/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/SettableStructObjectInspector.java
index 39155ee..616699e 100644
--- a/src/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/SettableStructObjectInspector.java
+++ b/src/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/SettableStructObjectInspector.java
@@ -34,4 +34,9 @@ public abstract class SettableStructObjectInspector extends
    */
   public abstract Object setStructFieldData(Object struct, StructField field,
       Object fieldValue);
+
+  @Override
+  public boolean isSettable() {
+    return true;
+  }
 }
diff --git a/src/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/StructObjectInspector.java b/src/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/StructObjectInspector.java
index b1b63da..0f58293 100644
--- a/src/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/StructObjectInspector.java
+++ b/src/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/StructObjectInspector.java
@@ -47,6 +47,10 @@ public abstract class StructObjectInspector implements ObjectInspector {
    */
   public abstract List<Object> getStructFieldsDataAsList(Object data);
 
+  public boolean isSettable() {
+    return false;
+  }
+
   @Override
   public String toString() {
     StringBuilder sb = new StringBuilder();
-- 
1.7.0.4

