From da9708e956a63008e8c1f9f2d2fef352dbba4088 Mon Sep 17 00:00:00 2001
From: Prasad Mujumdar <prasadm@cloudera.com>
Date: Mon, 15 Apr 2013 11:24:15 -0700
Subject: [PATCH 091/121] CDH-11554: Enable capturing access file/directory entities for Hive DML statements

---
 .../java/org/apache/hadoop/hive/conf/HiveConf.java |    1 +
 .../apache/hadoop/hive/ql/hooks/ReadEntity.java    |   17 +++++++++++++++++
 .../hadoop/hive/ql/parse/DDLSemanticAnalyzer.java  |   17 +++++++++++------
 .../hive/ql/parse/ImportSemanticAnalyzer.java      |    7 +++++--
 .../hadoop/hive/ql/parse/LoadSemanticAnalyzer.java |    5 +++++
 .../hadoop/hive/ql/parse/SemanticAnalyzer.java     |    4 ++++
 6 files changed, 43 insertions(+), 8 deletions(-)

diff --git a/src/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/src/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index 65021e5..8f0fb58 100644
--- a/src/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/src/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -677,6 +677,7 @@ public class HiveConf extends Configuration {
     HIVE_DRIVER_RUN_HOOKS("hive.exec.driver.run.hooks", ""),
     HIVE_DDL_OUTPUT_FORMAT("hive.ddl.output.format", null),
     HIVE_ENTITY_SEPARATOR("hive.entity.separator", "@"),
+    HIVE_ENITITY_CAPTURE_INPUT_URI("hive.entity.capture.input.URI", false),
 
     HIVE_SERVER2_THRIFT_MIN_WORKER_THREADS("hive.server2.thrift.min.worker.threads", 5),
     HIVE_SERVER2_THRIFT_MAX_WORKER_THREADS("hive.server2.thrift.max.worker.threads", 100),
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/hooks/ReadEntity.java b/src/ql/src/java/org/apache/hadoop/hive/ql/hooks/ReadEntity.java
index 8676490..00a9c76 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/hooks/ReadEntity.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/hooks/ReadEntity.java
@@ -87,6 +87,23 @@ public class ReadEntity extends Entity implements Serializable {
   }
 
   /**
+   * Constructor for URI readentity
+   * @param path
+   * @param isLocal
+   */
+  public ReadEntity (String path, boolean isLocal) {
+    super(path, isLocal);
+  }
+
+  /**
+   * Constructor for URI readentity
+   * @param path
+   */
+  public ReadEntity (String path) {
+    super(path, false);
+  }
+
+  /**
    * Equals function.
    */
   @Override
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
index 2a6e829..fab1bd2 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
@@ -18,13 +18,8 @@
 
 package org.apache.hadoop.hive.ql.parse;
 
-import static org.apache.hadoop.hive.ql.parse.HiveParser.TOK_CASCADE;
-import static org.apache.hadoop.hive.ql.parse.HiveParser.TOK_DATABASECOMMENT;
 import static org.apache.hadoop.hive.ql.parse.HiveParser.TOK_DATABASELOCATION;
 import static org.apache.hadoop.hive.ql.parse.HiveParser.TOK_DATABASEPROPERTIES;
-import static org.apache.hadoop.hive.ql.parse.HiveParser.TOK_IFEXISTS;
-import static org.apache.hadoop.hive.ql.parse.HiveParser.TOK_IFNOTEXISTS;
-import static org.apache.hadoop.hive.ql.parse.HiveParser.TOK_SHOWDATABASES;
 
 import java.io.Serializable;
 import java.net.URI;
@@ -687,7 +682,7 @@ public class DDLSemanticAnalyzer extends BaseSemanticAnalyzer {
     if (dbProps != null) {
       createDatabaseDesc.setDatabaseProperties(dbProps);
     }
-
+    saveInputLocationEntity(dbLocation);
     rootTasks.add(TaskFactory.get(new DDLWork(getInputs(), getOutputs(),
         createDatabaseDesc), conf));
   }
@@ -819,6 +814,7 @@ public class DDLSemanticAnalyzer extends BaseSemanticAnalyzer {
         shared.serde, shared.serdeProps, rowFormatParams.collItemDelim,
         rowFormatParams.fieldDelim, rowFormatParams.fieldEscape,
         rowFormatParams.lineDelim, rowFormatParams.mapKeyDelim, indexComment);
+    saveInputLocationEntity(location);
     Task<?> createIndex =
         TaskFactory.get(new DDLWork(getInputs(), getOutputs(), crtIndexDesc), conf);
     rootTasks.add(createIndex);
@@ -1154,6 +1150,7 @@ public class DDLSemanticAnalyzer extends BaseSemanticAnalyzer {
     AlterTableDesc alterTblDesc = new AlterTableDesc(tableName, newLocation, partSpec);
 
     addInputsOutputsAlterTable(tableName, partSpec, alterTblDesc);
+    saveInputLocationEntity(newLocation);
     rootTasks.add(TaskFactory.get(new DDLWork(getInputs(), getOutputs(),
         alterTblDesc), conf));
   }
@@ -2319,6 +2316,7 @@ public class DDLSemanticAnalyzer extends BaseSemanticAnalyzer {
     // add the last one
     if (currentPart != null) {
       validatePartitionValues(currentPart);
+      saveInputLocationEntity(currentLocation);
       AddPartitionDesc addPartitionDesc = new AddPartitionDesc(
           db.getCurrentDatabase(), tblName, currentPart,
           currentLocation, ifNotExists, expectView);
@@ -2972,4 +2970,11 @@ public class DDLSemanticAnalyzer extends BaseSemanticAnalyzer {
     }
   }
 
+  private void saveInputLocationEntity(String location) {
+    if (conf.getBoolVar(ConfVars.HIVE_ENITITY_CAPTURE_INPUT_URI) &&
+        (location != null)) {
+      inputs.add(new ReadEntity(location));
+    }
+  }
+
 }
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java
index 70975c6..461844b 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java
@@ -36,6 +36,7 @@ import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hadoop.hive.metastore.TableType;
 import org.apache.hadoop.hive.metastore.Warehouse;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
@@ -46,6 +47,7 @@ import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.exec.TaskFactory;
 import org.apache.hadoop.hive.ql.exec.Utilities;
+import org.apache.hadoop.hive.ql.hooks.ReadEntity;
 import org.apache.hadoop.hive.ql.hooks.WriteEntity;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.metadata.InvalidTableException;
@@ -265,8 +267,9 @@ public class ImportSemanticAnalyzer extends BaseSemanticAnalyzer {
           }
         }
         rootTasks.add(t);
-        //inputs.add(new ReadEntity(fromURI.toString(),
-        //  fromURI.getScheme().equals("hdfs") ? true : false));
+        if (conf.getBoolVar(ConfVars.HIVE_ENITITY_CAPTURE_INPUT_URI)) {
+          inputs.add(new ReadEntity(fromURI.toString(), "hdfs".equals(fromURI.getScheme())));
+        }
       }
     } catch (SemanticException e) {
       throw e;
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java
index bd8d252..4582e6d 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java
@@ -32,11 +32,13 @@ import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.exec.TaskFactory;
 import org.apache.hadoop.hive.ql.exec.Utilities;
+import org.apache.hadoop.hive.ql.hooks.ReadEntity;
 import org.apache.hadoop.hive.ql.hooks.WriteEntity;
 import org.apache.hadoop.hive.ql.metadata.Hive;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
@@ -189,6 +191,9 @@ public class LoadSemanticAnalyzer extends BaseSemanticAnalyzer {
       throw new SemanticException(ErrorMsg.INVALID_PATH.getMsg(fromTree, e
           .getMessage()), e);
     }
+    if (conf.getBoolVar(ConfVars.HIVE_ENITITY_CAPTURE_INPUT_URI)) {
+      inputs.add(new ReadEntity(fromURI.toString(), isLocal));
+    }
 
     // initialize destination table/partition
     tableSpec ts = new tableSpec(db, conf, (ASTNode) tableTree);
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
index 991c644..5fec96e 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
@@ -8764,6 +8764,10 @@ public class SemanticAnalyzer extends BaseSemanticAnalyzer {
       }
     }
 
+    if (conf.getBoolVar(ConfVars.HIVE_ENITITY_CAPTURE_INPUT_URI) &&
+          (location != null)) {
+      inputs.add(new ReadEntity(location));
+    }
     // Handle different types of CREATE TABLE command
     CreateTableDesc crtTblDesc = null;
     switch (command_type) {
-- 
1.7.0.4

