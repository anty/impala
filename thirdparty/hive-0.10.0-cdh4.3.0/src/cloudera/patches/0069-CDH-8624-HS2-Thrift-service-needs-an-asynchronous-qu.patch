From f94211c466331dce6ac763f278be51e1c3810429 Mon Sep 17 00:00:00 2001
From: Prasad Mujumdar <prasadm@cloudera.com>
Date: Wed, 27 Feb 2013 22:28:35 -0800
Subject: [PATCH 069/121] CDH-8624: HS2 Thrift service needs an asynchronous query interface

---
 .../java/org/apache/hadoop/hive/conf/HiveConf.java |    1 +
 .../apache/hive/jdbc/HivePreparedStatement.java    |   32 ++-
 .../java/org/apache/hive/jdbc/HiveStatement.java   |   18 +-
 jdbc/src/java/org/apache/hive/jdbc/Utils.java      |   10 +
 .../test/org/apache/hive/jdbc/TestJdbcDriver2.java |  280 ++++++++++++++++++++
 ql/src/java/org/apache/hadoop/hive/ql/Driver.java  |   30 ++-
 .../cli/operation/AsyncExecStmtOperation.java      |  166 ++++++++++++
 .../cli/operation/ExecuteStatementOperation.java   |   27 ++-
 .../hive/service/cli/operation/Operation.java      |   19 ++
 .../hive/service/cli/operation/SQLOperation.java   |   79 ++++--
 .../hive/service/cli/session/HiveSessionImpl.java  |    1 +
 .../hive/service/cli/thrift/ThriftCLIService.java  |    7 +-
 12 files changed, 618 insertions(+), 52 deletions(-)
 create mode 100644 service/src/java/org/apache/hive/service/cli/operation/AsyncExecStmtOperation.java

diff --git a/src/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/src/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index d26dcbe..36af5d6 100644
--- a/src/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/src/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -693,6 +693,7 @@ public class HiveConf extends Configuration {
     HIVE_SERVER2_PLAIN_LDAP_BASEDN("hive.server2.authentication.ldap.baseDN", null),
     HIVE_SERVER2_KERBEROS_IMPERSONATION("hive.server2.enable.impersonation", false),
     HIVE_SERVER2_CUSTOM_AUTHENTICATION_CLASS("hive.server2.custom.authentication.class", null),
+    HIVE_SERVER2_BLOCKING_QUERY("hive.server2.blocking.query", true),
 
     HIVE_CONF_RESTRICTED_LIST("hive.conf.restricted.list", null),
 
diff --git a/src/jdbc/src/java/org/apache/hive/jdbc/HivePreparedStatement.java b/src/jdbc/src/java/org/apache/hive/jdbc/HivePreparedStatement.java
index 0d91340..3bb8b84 100644
--- a/src/jdbc/src/java/org/apache/hive/jdbc/HivePreparedStatement.java
+++ b/src/jdbc/src/java/org/apache/hive/jdbc/HivePreparedStatement.java
@@ -43,11 +43,13 @@ import java.util.Calendar;
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.hive.service.cli.thrift.TCLIService;
+import org.apache.hive.service.cli.thrift.TCloseOperationReq;
 import org.apache.hive.service.cli.thrift.TExecuteStatementReq;
 import org.apache.hive.service.cli.thrift.TExecuteStatementResp;
 import org.apache.hive.service.cli.thrift.TOperationHandle;
-import org.apache.hive.service.cli.thrift.TCLIService;
 import org.apache.hive.service.cli.thrift.TSessionHandle;
+import org.apache.hive.service.cli.thrift.TStatusCode;
 
 /**
  * HivePreparedStatement.
@@ -172,7 +174,7 @@ public class HivePreparedStatement implements PreparedStatement {
 
   protected ResultSet executeImmediate(String sql) throws SQLException {
     if (isClosed) {
-      throw new SQLException("Can't execute after statement has been closed");
+      throw new SQLException("Can't execute after statement has been closed", "24000");
     }
 
     try {
@@ -184,7 +186,12 @@ public class HivePreparedStatement implements PreparedStatement {
       TExecuteStatementReq execReq = new TExecuteStatementReq(sessHandle, sql);
       execReq.setConfOverlay(sessConf);
       TExecuteStatementResp execResp = client.ExecuteStatement(execReq);
-      Utils.verifySuccessWithInfo(execResp.getStatus());
+      if (execResp.getStatus().getStatusCode().equals(TStatusCode.STILL_EXECUTING_STATUS)) {
+        warningChain = Utils.addWarning(warningChain, new SQLWarning("Query execuing asynchronously"));
+      } else {
+        Utils.verifySuccessWithInfo(execResp.getStatus());
+      }
+
       stmtHandle = execResp.getOperationHandle();
     } catch (SQLException es) {
       throw es;
@@ -858,6 +865,22 @@ public class HivePreparedStatement implements PreparedStatement {
      warningChain=null;
   }
 
+  private void closeClientOperation() throws SQLException {
+    try {
+      clearWarnings();
+      if (stmtHandle != null) {
+        TCloseOperationReq closeReq = new TCloseOperationReq();
+        closeReq.setOperationHandle(stmtHandle);
+        client.CloseOperation(closeReq);
+      }
+    } catch (SQLException e) {
+      throw e;
+    } catch (Exception e) {
+      throw new SQLException(e.getMessage(), "08S01", e);
+    }
+    stmtHandle = null;
+  }
+
   /**
    *  Closes the prepared statement.
    *
@@ -865,11 +888,12 @@ public class HivePreparedStatement implements PreparedStatement {
    */
 
   public void close() throws SQLException {
-    client = null;
     if (resultSet!=null) {
       resultSet.close();
       resultSet = null;
     }
+    closeClientOperation();
+    client = null;
     isClosed = true;
   }
 
diff --git a/src/jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java b/src/jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java
index aeae800..61419f4 100644
--- a/src/jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java
+++ b/src/jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java
@@ -34,6 +34,7 @@ import org.apache.hive.service.cli.thrift.TExecuteStatementReq;
 import org.apache.hive.service.cli.thrift.TExecuteStatementResp;
 import org.apache.hive.service.cli.thrift.TOperationHandle;
 import org.apache.hive.service.cli.thrift.TSessionHandle;
+import org.apache.hive.service.cli.thrift.TStatusCode;
 
 /**
  * HiveStatement.
@@ -43,7 +44,7 @@ public class HiveStatement implements java.sql.Statement {
   private TCLIService.Iface client;
   private TOperationHandle stmtHandle;
   private final TSessionHandle sessHandle;
-  Map<String,String> sessConf = new HashMap<String,String>();
+  private final Map<String,String> sessConf = new HashMap<String,String>();
   private int fetchSize = 50;
   /**
    * We need to keep a reference to the result set to support the following:
@@ -97,7 +98,7 @@ public class HiveStatement implements java.sql.Statement {
 
   public void cancel() throws SQLException {
     if (isClosed) {
-      throw new SQLException("Can't cancel after statement has been closed");
+      throw new SQLException("Can't cancel after statement has been closed", "24000");
     }
 
     TCancelOperationReq cancelReq = new TCancelOperationReq();
@@ -108,7 +109,7 @@ public class HiveStatement implements java.sql.Statement {
     } catch (SQLException e) {
       throw e;
     } catch (Exception e) {
-      throw new SQLException(e.toString(), "08S01");
+      throw new SQLException(e.getMessage(), "08S01", e);
     }
   }
 
@@ -134,19 +135,20 @@ public class HiveStatement implements java.sql.Statement {
 
   private void closeClientOperation() throws SQLException {
     try {
+      clearWarnings();
       if (stmtHandle != null) {
         TCloseOperationReq closeReq = new TCloseOperationReq();
         closeReq.setOperationHandle(stmtHandle);
         TCloseOperationResp closeResp = client.CloseOperation(closeReq);
-        Utils.verifySuccessWithInfo(closeResp.getStatus());
       }
     } catch (SQLException e) {
       throw e;
     } catch (Exception e) {
-      throw new SQLException(e.toString(), "08S01");
+      throw new SQLException(e.getMessage(), "08S01", e);
     }
     stmtHandle = null;
   }
+
   /*
    * (non-Javadoc)
    *
@@ -179,7 +181,11 @@ public class HiveStatement implements java.sql.Statement {
       TExecuteStatementReq execReq = new TExecuteStatementReq(sessHandle, sql);
       execReq.setConfOverlay(sessConf);
       TExecuteStatementResp execResp = client.ExecuteStatement(execReq);
-      Utils.verifySuccessWithInfo(execResp.getStatus());
+      if (execResp.getStatus().getStatusCode().equals(TStatusCode.STILL_EXECUTING_STATUS)) {
+        warningChain = Utils.addWarning(warningChain, new SQLWarning("Query execuing asynchronously"));
+      } else {
+        Utils.verifySuccessWithInfo(execResp.getStatus());
+      }
       stmtHandle = execResp.getOperationHandle();
     } catch (SQLException eS) {
       throw eS;
diff --git a/src/jdbc/src/java/org/apache/hive/jdbc/Utils.java b/src/jdbc/src/java/org/apache/hive/jdbc/Utils.java
index 433b2c5..ac25996 100644
--- a/src/jdbc/src/java/org/apache/hive/jdbc/Utils.java
+++ b/src/jdbc/src/java/org/apache/hive/jdbc/Utils.java
@@ -20,6 +20,7 @@ package org.apache.hive.jdbc;
 
 import java.net.URI;
 import java.sql.SQLException;
+import java.sql.SQLWarning;
 import java.sql.Types;
 import java.util.HashMap;
 import java.util.Map;
@@ -248,5 +249,14 @@ public class Utils {
     return connParams;
   }
 
+  public static SQLWarning addWarning(SQLWarning warningChain, SQLWarning newWarning) {
+    if (warningChain == null) {
+      return newWarning;
+    } else {
+      warningChain.setNextWarning(newWarning);
+      return warningChain;
+    }
+  }
+
 
 }
diff --git a/src/jdbc/src/test/org/apache/hive/jdbc/TestJdbcDriver2.java b/src/jdbc/src/test/org/apache/hive/jdbc/TestJdbcDriver2.java
index 2e26ce0..aa9704b 100644
--- a/src/jdbc/src/test/org/apache/hive/jdbc/TestJdbcDriver2.java
+++ b/src/jdbc/src/test/org/apache/hive/jdbc/TestJdbcDriver2.java
@@ -35,11 +35,14 @@ import java.util.HashMap;
 import java.util.HashSet;
 import java.util.Map;
 import java.util.Set;
+import java.util.concurrent.ConcurrentHashMap;
 
 import junit.framework.TestCase;
 
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.hooks.ExecuteWithHookContext;
+import org.apache.hadoop.hive.ql.hooks.HookContext;
 
 /**
  * TestJdbcDriver2
@@ -57,6 +60,7 @@ public class TestJdbcDriver2 extends TestCase {
   private static final String partitionedTableComment = "Partitioned table";
   private static final String dataTypeTableName = "testDataTypeTable";
   private static final String dataTypeTableComment = "Table with many column data types";
+  private static final String QueryTag = "myQueryTag";
   private final HiveConf conf;
   private final Path dataFilePath;
   private final Path dataTypeDataFilePath;
@@ -171,6 +175,8 @@ public class TestJdbcDriver2 extends TestCase {
     // drop table
     Statement stmt = con.createStatement();
     assertNotNull("Statement is null", stmt);
+    stmt.execute("set hive.server2.blocking.query=true");
+    QueryBlockHook.clearAll();
     stmt.execute("drop table " + tableName);
     stmt.execute("drop table " + partitionedTableName);
     stmt.execute("drop table " + dataTypeTableName);
@@ -1252,4 +1258,278 @@ public class TestJdbcDriver2 extends TestCase {
     assertFalse(res.next());
   }
 
+  /**
+   * run DDLs with async mode
+   * @throws SQLException
+   */
+  public void testAsyncStmts() throws SQLException {
+    String fooQueryTag = "foo";
+    Statement stmt = con.createStatement();
+    stmt.execute("DROP TABLE IF EXISTS tabz");
+    stmt.close();
+    stmt = con.createStatement();
+    stmt.execute("set hive.server2.blocking.query=false");
+    stmt.execute("set " + QueryTag + " = " + fooQueryTag);
+    stmt.close();
+    stmt = con.createStatement();
+    stmt.execute("set hive.exec.post.hooks = org.apache.hive.jdbc.TestJdbcDriver2$QueryBlockHook");
+    stmt.close();
+    QueryBlockHook.setupBlock(fooQueryTag);
+    stmt = con.createStatement();
+    stmt.execute("CREATE TABLE tabz (id INT)");
+    assertNotNull(stmt.getWarnings());
+    QueryBlockHook.clearBlock(fooQueryTag);
+    stmt.close();
+    stmt = con.createStatement();
+    QueryBlockHook.setupBlock(fooQueryTag);
+    stmt.execute("DROP TABLE IF EXISTS tabz");
+    assertNotNull(stmt.getWarnings());
+    QueryBlockHook.clearBlock(fooQueryTag);
+    stmt.close();
+  }
+
+  /**
+   * run queries in async mode
+   * @throws SQLException
+   */
+  public void testAsyncQuery() throws SQLException {
+    String fooQueryTag = "foo";
+    Statement stmt = con.createStatement();
+    stmt.execute("set hive.server2.blocking.query=false");
+    stmt.close();
+    stmt = con.createStatement();
+    stmt.execute("set hive.exec.post.hooks = org.apache.hive.jdbc.TestJdbcDriver2$QueryBlockHook");
+    stmt.close();
+    stmt = con.createStatement();
+    stmt.execute("set " + QueryTag + " = " + fooQueryTag);
+    QueryBlockHook.setupBlock(fooQueryTag);
+    ResultSet res = stmt.executeQuery("select c1 from " + dataTypeTableName +
+          " where c1 = 1");
+    assertNotNull(stmt.getWarnings());
+    ResultSetMetaData md = res.getMetaData();
+    // sanity check metadata
+    assertEquals(md.getColumnCount(), 1); // only one result column
+    assertEquals(md.getColumnLabel(1), "c1" ); // verify the column name
+    try {
+      res.next();
+      assertTrue(false);
+    } catch (SQLException e) {
+      // verify that the fetch fails with query still running error
+      assertEquals("HY010", e.getSQLState());
+    }
+    QueryBlockHook.clearBlock(fooQueryTag); // continue query
+    // verify that we can now fetch data
+    // the query could be still be running and might take a few more iteration to finish
+    do {
+      try {
+        res.next();
+        break;
+      } catch (SQLException e) {
+        if (e.getSQLState().equals("HY010")) {
+          // if query is not complete, then try next time
+          continue;
+        } else {
+          throw e;
+        }
+      }
+    } while (true);
+    assertEquals(1, res.getInt(1));
+    stmt.close();
+  }
+
+  /**
+   * Run multiple queries in async mode
+   */
+  /**
+   * run queries in async mode
+   * @throws SQLException
+   */
+  public void testMultiAsyncQueries() throws SQLException {
+    String fooQueryTag = "foo";
+    String barQueryTag = "bar";
+
+    Statement stmt = con.createStatement();
+    stmt.execute("set hive.server2.blocking.query=false");
+    stmt.close();
+    stmt = con.createStatement();
+    stmt.execute("set hive.exec.post.hooks = org.apache.hive.jdbc.TestJdbcDriver2$QueryBlockHook");
+    stmt.close();
+
+    // start foo query
+    stmt = con.createStatement();
+    stmt.execute("set " + QueryTag + " = " + fooQueryTag);
+    QueryBlockHook.setupBlock(fooQueryTag);
+    ResultSet res = stmt.executeQuery("select c1 from " + dataTypeTableName +
+          " where c1 = 1");
+    assertNotNull(stmt.getWarnings());
+    ResultSetMetaData md = res.getMetaData();
+    // sanity check metadata
+    assertEquals(md.getColumnCount(), 1); // only one result column
+    assertEquals(md.getColumnLabel(1), "c1" ); // verify the column name
+    try {
+      res.next();
+      assertTrue(false);
+    } catch (SQLException e) {
+      // verify that the fetch fails with query still running error
+      assertEquals("HY010", e.getSQLState());
+    }
+
+    // start bar query
+    Statement stmt2 = con.createStatement();
+    stmt2.execute("set " + QueryTag + " = " + barQueryTag);
+    QueryBlockHook.setupBlock(barQueryTag);
+    ResultSet res2 = stmt2.executeQuery("select c1 from " + dataTypeTableName +
+    " where c1 = 1");
+    assertNotNull(stmt2.getWarnings());
+    ResultSetMetaData md2 = res2.getMetaData();
+    // sanity check metadata
+    assertEquals(md2.getColumnCount(), 1); // only one result column
+    assertEquals(md2.getColumnLabel(1), "c1" ); // verify the column name
+    try {
+      res2.next();
+      assertTrue(false);
+    } catch (SQLException e) {
+      // verify that the fetch fails with query still running error
+      assertEquals("HY010", e.getSQLState());
+    }
+
+    QueryBlockHook.clearBlock(fooQueryTag); // continue foo query
+    // verify that we can now fetch data
+    // the query could be still be running and might take a few more iteration to finish
+    do {
+      try {
+        res.next();
+        break;
+      } catch (SQLException e) {
+        if (e.getSQLState().equals("HY010")) {
+          // if query is not complete, then try next time
+          continue;
+        } else {
+          throw e;
+        }
+      }
+    } while (true);
+    assertEquals(1, res.getInt(1));
+    stmt.close();
+
+    // verify that the bar query is still blocked
+    try {
+      res2.next();
+      assertTrue(false);
+    } catch (SQLException e) {
+      // verify that the fetch fails with query still running error
+      assertEquals("HY010", e.getSQLState());
+    }
+
+    QueryBlockHook.clearBlock(barQueryTag); // continue bar query
+    // verify that we can now fetch data for bar query
+    // the query could be still be running and might take a few more iteration to finish
+    do {
+      try {
+        res2.next();
+        break;
+      } catch (SQLException e) {
+        if (e.getSQLState().equals("HY010")) {
+          // if query is not complete, then try next time
+          continue;
+        } else {
+          throw e;
+        }
+      }
+    } while (true);
+    assertEquals(1, res2.getInt(1));
+    stmt2.close();
+  }
+
+  /**
+   * run prepared statement in async mode
+   * @throws SQLException
+   */
+  public void testAsyncPreparedStmt() throws SQLException {
+    String fooQueryTag = "foo";
+
+    Statement stmt = con.createStatement();
+    stmt.execute("set hive.server2.blocking.query=false");
+    stmt.execute("set hive.exec.post.hooks = org.apache.hive.jdbc.TestJdbcDriver2$QueryBlockHook");
+    stmt.execute("set " + QueryTag + " = " + fooQueryTag);
+    stmt.close();
+    QueryBlockHook.setupBlock(fooQueryTag);
+    PreparedStatement pStmt = con.prepareStatement("select c1 from " + dataTypeTableName +
+          " where c1 = 1") ;
+    ResultSet res = pStmt.executeQuery();
+    assertNotNull(pStmt.getWarnings());
+    ResultSetMetaData md = res.getMetaData();
+    // sanity check metadata
+    assertEquals(md.getColumnCount(), 1); // only one result column
+    assertEquals(md.getColumnLabel(1), "c1" ); // verify the column name
+    try {
+      res.next();
+      assertTrue(false);
+    } catch (SQLException e) {
+      // verify that the fetch fails with query still running error
+      assertEquals("HY010", e.getSQLState());
+    }
+    QueryBlockHook.clearBlock(fooQueryTag); // continue query
+    // verify that we can now fetch data
+    // the query could be still be running and might take a few more iteration to finish
+    do {
+      try {
+        res.next();
+        break;
+      } catch (SQLException e) {
+        if (e.getSQLState().equals("HY010")) {
+          // if query is not complete, then try next time
+          continue;
+        } else {
+          throw e;
+        }
+      }
+    } while (true);
+    assertEquals(1, res.getInt(1));
+    pStmt.close();
+  }
+
+  /**
+   *  Post execute hook that blocks the execution
+   *  Used for async query testing
+   */
+  public static class QueryBlockHook implements ExecuteWithHookContext {
+
+    private static Map<String, Boolean> triggerMap = new ConcurrentHashMap<String, Boolean>();
+
+    public void run(HookContext hookContext) {
+      String myTag = hookContext.getConf().get(QueryTag, "");
+      if (myTag.isEmpty() || !triggerMap.containsKey(myTag)) {
+        return;
+      }
+      while (triggerMap.get(myTag)) {
+        try {
+          Thread.sleep(200);
+        } catch (InterruptedException e) {
+          break;
+        }
+      }
+    }
+
+    // enable the post hook wait
+    public static void setupBlock(String queryTag) {
+      if (triggerMap.containsKey(queryTag)) {
+        triggerMap.put(queryTag, true);
+      }
+    }
+
+    // resume the post hook
+    public static void clearBlock(String queryTag) {
+      if (triggerMap.containsKey(queryTag)) {
+        triggerMap.put(queryTag, false);
+      }
+    }
+
+    public static void clearAll() {
+      for (Map.Entry<String, Boolean> entry : triggerMap.entrySet()) {
+        entry.setValue(false);
+      }
+    }
+
+  }
 }
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/Driver.java b/src/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
index 5c00008..494a9a0 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
@@ -347,6 +347,17 @@ public class Driver implements CommandProcessor {
     return compile(command, true);
   }
 
+  public CommandProcessorResponse compileAndRespond(String command) {
+    int ret;
+    synchronized (compileMonitor) {
+      ret = compile(command);
+    }
+    if (ret != 0) {
+      releaseLocks(ctx.getHiveLocks());
+      // console.printError("Compilation failed for statement '" + command + "'");
+    }
+    return new CommandProcessorResponse(ret, errorMessage, SQLState);
+  }
   /**
    * Hold state variables specific to each query being executed, that may not
    * be consistent in the overall SessionState
@@ -875,6 +886,11 @@ public class Driver implements CommandProcessor {
   }
 
   public CommandProcessorResponse run(String command) throws CommandNeedRetryException {
+    return run(command, true);
+  }
+
+  public CommandProcessorResponse run(String command, boolean compileQuery)
+        throws CommandNeedRetryException {
     errorMessage = null;
     SQLState = null;
 
@@ -904,12 +920,14 @@ public class Driver implements CommandProcessor {
     perfLogger.PerfLogBegin(LOG, PerfLogger.TIME_TO_SUBMIT);
 
     int ret;
-    synchronized (compileMonitor) {
-      ret = compile(command);
-    }
-    if (ret != 0) {
-      releaseLocks(ctx.getHiveLocks());
-      return new CommandProcessorResponse(ret, errorMessage, SQLState);
+    if (compileQuery) {
+      synchronized (compileMonitor) {
+        ret = compile(command);
+      }
+      if (ret != 0) {
+        releaseLocks(ctx.getHiveLocks());
+        return new CommandProcessorResponse(ret, errorMessage, SQLState);
+      }
     }
 
     boolean requireLock = false;
diff --git a/src/service/src/java/org/apache/hive/service/cli/operation/AsyncExecStmtOperation.java b/src/service/src/java/org/apache/hive/service/cli/operation/AsyncExecStmtOperation.java
new file mode 100644
index 0000000..3890479
--- /dev/null
+++ b/src/service/src/java/org/apache/hive/service/cli/operation/AsyncExecStmtOperation.java
@@ -0,0 +1,166 @@
+package org.apache.hive.service.cli.operation;
+
+import java.util.Map;
+import java.util.concurrent.Callable;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.Future;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hive.service.cli.FetchOrientation;
+import org.apache.hive.service.cli.HiveSQLException;
+import org.apache.hive.service.cli.OperationState;
+import org.apache.hive.service.cli.RowSet;
+import org.apache.hive.service.cli.TableSchema;
+import org.apache.hive.service.cli.session.HiveSession;
+
+public class AsyncExecStmtOperation extends ExecuteStatementOperation {
+  private ExecuteStatementOperation execOP;
+  private final ExecutorService opExecutor;
+  private Future<String> execFuture = null;
+  private static final Log LOG = LogFactory.getLog(AsyncExecStmtOperation.class);
+
+  public AsyncExecStmtOperation(HiveSession parentSession, String statement,
+      Map<String, String> confOverlay) {
+    super(parentSession, statement, confOverlay);
+    opExecutor = Executors.newSingleThreadExecutor();
+    LOG.info("Got ansync exec for query " + statement);
+  }
+
+  public static AsyncExecStmtOperation wrapExecStmtOperation(ExecuteStatementOperation execOP) {
+    AsyncExecStmtOperation newExecOP =
+          new AsyncExecStmtOperation(execOP.getParentSession(), execOP.getStatement(), execOP.confOverlay);
+    newExecOP.setExecOP(execOP);
+    return newExecOP;
+  }
+
+  @Override
+  public void run() throws HiveSQLException {
+    prepare();
+    final ExecuteStatementOperation currExec = execOP;
+    execFuture = opExecutor.submit(new Callable<String>() {
+      public String  call() throws HiveSQLException {
+        currExec.run();
+        return null;
+      }
+    });
+  }
+
+  @Override
+  public void prepare() throws HiveSQLException {
+    final ExecuteStatementOperation currExec = execOP;
+    execFuture = opExecutor.submit(new Callable<String>() {
+      public String  call() throws HiveSQLException {
+        // Clone the current configuration for an async query. we don't want the
+        // query to see the config changes after the query starts to execute
+        HiveConf queryConf = new HiveConf(getParentSession().getHiveConf());
+        SessionState.start(currExec.getParentSession().getSessionState());
+        currExec.prepare(queryConf);
+        return null;
+      }
+    });
+    waitForCompletion(execFuture);
+    setHasResultSet(execOP.hasResultSet());
+  }
+
+  @Override
+  public void close() throws HiveSQLException {
+    // wait for the async statement to finish
+    waitForCompletion(execFuture);
+    final ExecuteStatementOperation currExec = execOP;
+    Future<String> opFuture = opExecutor.submit(new Callable<String>() {
+      public String  call() throws HiveSQLException {
+        currExec.close();
+        return null;
+      }
+    });
+    waitForCompletion(opFuture);
+    opExecutor.shutdown();
+  }
+
+  @Override
+  public void cancel() throws HiveSQLException {
+    final ExecuteStatementOperation currExec = execOP;
+    Future<String> opFuture = opExecutor.submit(new Callable<String>() {
+      public String  call() throws HiveSQLException {
+        currExec.cancel();
+        return null;
+      }
+    });
+    waitForCompletion(opFuture);
+    opExecutor.shutdown();
+  }
+
+  @Override
+  public TableSchema getResultSetSchema() throws HiveSQLException {
+    return execOP.getResultSetSchema();
+  }
+
+  @Override
+  public RowSet getNextRowSet(final FetchOrientation orientation, final long maxRows)
+          throws HiveSQLException {
+    checkExecutionStatus();
+    final ExecuteStatementOperation currExec = execOP;
+    Future<RowSet> opFuture = opExecutor.submit(new Callable<RowSet>() {
+      public RowSet call() throws HiveSQLException {
+        return currExec.getNextRowSet(orientation, maxRows);
+      }
+    });
+    return waitForCompletion(opFuture);
+  }
+
+  @Override
+  public OperationState getState() {
+    return execOP.getState();
+  }
+
+  @Override
+  public boolean isPrepared() {
+    return execOP.isPrepared();
+  }
+
+  // check if the query is still running or failed
+  private void checkExecutionStatus() throws HiveSQLException {
+    if (execFuture == null) {
+      // no background thread running. 'Invalid cursor state' exception
+      throw new HiveSQLException("No background query executed", "24000");
+    }
+    if (getState().equals(OperationState.RUNNING) || !execFuture.isDone()) {
+      throw new HiveSQLException("Query still runing", "HY010");
+    }
+    waitForCompletion(execFuture);
+    if (getState().equals(OperationState.ERROR)) {
+      throw new HiveSQLException("Query execution failed", "07000");
+    }
+    if (getState().equals(OperationState.CANCELED)) {
+      // query is already canceled. 'Invalid cursor state' exception
+      throw new HiveSQLException("Query execution was canceled", "24000");
+    }
+  }
+
+  // wait for the given future to complete
+  private <T> T waitForCompletion(Future<T> opFuture)
+        throws HiveSQLException {
+    T result = null;
+    try {
+      result = opFuture.get();
+    } catch (InterruptedException e) {
+      throw  new HiveSQLException(e.getMessage(), "24000", -1, e);
+    } catch (ExecutionException e) {
+      if (e.getCause() instanceof HiveSQLException) {
+        throw (HiveSQLException)e.getCause();
+      } else {
+        throw  new HiveSQLException(e.getMessage(), e.getCause());
+      }
+    }
+    return result;
+  }
+
+  private void setExecOP(ExecuteStatementOperation execOP) {
+    this.execOP = execOP;
+  }
+}
diff --git a/src/service/src/java/org/apache/hive/service/cli/operation/ExecuteStatementOperation.java b/src/service/src/java/org/apache/hive/service/cli/operation/ExecuteStatementOperation.java
index 9a1da59..a25dbda 100644
--- a/src/service/src/java/org/apache/hive/service/cli/operation/ExecuteStatementOperation.java
+++ b/src/service/src/java/org/apache/hive/service/cli/operation/ExecuteStatementOperation.java
@@ -21,7 +21,9 @@ package org.apache.hive.service.cli.operation;
 
 import java.util.HashMap;
 import java.util.Map;
+import java.util.Map.Entry;
 
+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hive.service.cli.OperationType;
 import org.apache.hive.service.cli.session.HiveSession;
 
@@ -44,16 +46,31 @@ public abstract class ExecuteStatementOperation extends Operation {
     String[] tokens = statement.trim().split("\\s+");
     String command = tokens[0].toLowerCase();
 
+    for (Entry<String, String> opConf : confOverlay.entrySet()) {
+      parentSession.getHiveConf().set(opConf.getKey(), opConf.getValue());
+    }
+
+    ExecuteStatementOperation newExecOP;
     if ("set".equals(command)) {
-      return new SetOperation(parentSession, statement, confOverlay);
+      newExecOP = new SetOperation(parentSession, statement, confOverlay);
     } else if ("dfs".equals(command)) {
-      return new DfsOperation(parentSession, statement, confOverlay);
+      newExecOP = new DfsOperation(parentSession, statement, confOverlay);
     } else if ("add".equals(command)) {
-      return new AddResourceOperation(parentSession, statement, confOverlay);
+      newExecOP = new AddResourceOperation(parentSession, statement, confOverlay);
     } else if ("delete".equals(command)) {
-      return new DeleteResourceOperation(parentSession, statement, confOverlay);
+      newExecOP = new DeleteResourceOperation(parentSession, statement, confOverlay);
     } else {
-      return new SQLOperation(parentSession, statement, confOverlay);
+      newExecOP = new SQLOperation(parentSession, statement, confOverlay);
+      // check if this is needs to be run asynchronously
+      boolean isAsyncOP = (parentSession.getHiveConf().getBoolVar(ConfVars.HIVE_SERVER2_BLOCKING_QUERY) == false);
+      if(confOverlay.containsKey(ConfVars.HIVE_SERVER2_BLOCKING_QUERY.toString())) {
+        isAsyncOP = confOverlay.get(ConfVars.HIVE_SERVER2_BLOCKING_QUERY.toString()).equalsIgnoreCase("false");
+      }
+      if (isAsyncOP) {
+        newExecOP = AsyncExecStmtOperation.wrapExecStmtOperation(newExecOP);
+      }
     }
+
+    return newExecOP;
   }
 }
diff --git a/src/service/src/java/org/apache/hive/service/cli/operation/Operation.java b/src/service/src/java/org/apache/hive/service/cli/operation/Operation.java
index b354ac9..29f6a9a 100644
--- a/src/service/src/java/org/apache/hive/service/cli/operation/Operation.java
+++ b/src/service/src/java/org/apache/hive/service/cli/operation/Operation.java
@@ -91,6 +91,12 @@ public abstract class Operation {
     }
   }
 
+  protected final void assertAtLeastState(OperationState state) throws HiveSQLException {
+    if (this.state.compareTo(state) < 0) {
+      throw new HiveSQLException("Expected state " + state + ", but found " + this.state);
+    }
+  }
+
   public boolean isRunning() {
     return OperationState.RUNNING.equals(getState());
   }
@@ -109,6 +115,19 @@ public abstract class Operation {
 
   public abstract void run() throws HiveSQLException;
 
+  public void prepare() throws HiveSQLException {
+    prepare(null);
+  }
+
+  public void prepare(HiveConf conf) throws HiveSQLException {
+    setState(OperationState.INITIALIZED);
+    throw new UnsupportedOperationException("SQLOperation.prepare()");
+  }
+
+  public boolean isPrepared () {
+    return false;
+  }
+
   // TODO: make this abstract and implement in subclasses.
   public void cancel() throws HiveSQLException {
     setState(OperationState.CANCELED);
diff --git a/src/service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java b/src/service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java
index f3a48a2..2db47aa 100644
--- a/src/service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java
+++ b/src/service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java
@@ -60,57 +60,68 @@ public class SQLOperation extends ExecuteStatementOperation {
   private TableSchema resultSchema = null;
   private Schema mResultSchema = null;
   private SerDe serde = null;
-
+  private boolean isPrepared = false;
+  private String subStatement = null;
 
   public SQLOperation(HiveSession parentSession, String statement, Map<String, String> confOverlay) {
     // TODO: call setRemoteUser in ExecuteStatementOperation or higher.
     super(parentSession, statement, confOverlay);
   }
 
-
+  // Compile the current statement
+  @Override
   public void prepare() throws HiveSQLException {
+    prepare(null);
   }
 
+  // Compile the current statement with the give configuration
   @Override
-  public void run() throws HiveSQLException {
+  public void prepare(HiveConf conf) throws HiveSQLException {
     setState(OperationState.RUNNING);
-    String statement_trimmed = statement.trim();
-    String[] tokens = statement_trimmed.split("\\s");
-    String cmd_1 = statement_trimmed.substring(tokens[0].length()).trim();
-
-    int ret = 0;
-    String errorMessage = "";
-    String SQLState = null;
-
-    try {
-      driver = new Driver(getParentSession().getHiveConf(), getParentSession().getIpAddress(),
+    HiveConf queryConf = conf;
+    if (queryConf == null) {
+      queryConf = getParentSession().getHiveConf();
+    }
+    driver = new Driver(queryConf, getParentSession().getIpAddress(),
                             getParentSession().getUsername());
-      // In Hive server mode, we are not able to retry in the FetchTask
-      // case, when calling fetch queries since execute() has returned.
-      // For now, we disable the test attempts.
-      driver.setTryCount(Integer.MAX_VALUE);
-
-      String subStatement = new VariableSubstitution().substitute(getParentSession().getHiveConf(), statement);
+    // In Hive server mode, we are not able to retry in the FetchTask
+    // case, when calling fetch queries since execute() has returned.
+    // For now, we disable the test attempts.
+    driver.setTryCount(Integer.MAX_VALUE);
+    subStatement = new VariableSubstitution().substitute(getParentSession().getHiveConf(),
+            statement);
+    response = driver.compileAndRespond(subStatement);
+    if (0 != response.getResponseCode()) {
+      throw new HiveSQLException("Error while processing statement: "
+          + response.getErrorMessage(), response.getSQLState(), response.getResponseCode());
+    }
+    mResultSchema = driver.getSchema();
+    if (mResultSchema != null && mResultSchema.isSetFieldSchemas()) {
+      resultSchema = new TableSchema(mResultSchema);
+      setHasResultSet(true);
+    } else {
+      setHasResultSet(false);
+    }
+    setPrepared();
+  }
 
-      response = driver.run(subStatement);
+  @Override
+  public void run() throws HiveSQLException {
+    try {
+      if (!isPrepared()) {
+        prepare();
+      }
+      response = driver.run(subStatement, false);
       if (0 != response.getResponseCode()) {
         throw new HiveSQLException("Error while processing statement: "
             + response.getErrorMessage(), response.getSQLState(), response.getResponseCode());
       }
-
-      mResultSchema = driver.getSchema();
-      if (mResultSchema != null && mResultSchema.isSetFieldSchemas()) {
-        resultSchema = new TableSchema(mResultSchema);
-        setHasResultSet(true);
-      } else {
-        setHasResultSet(false);
-      }
     } catch (HiveSQLException e) {
       setState(OperationState.ERROR);
       throw e;
     } catch (Exception e) {
       setState(OperationState.ERROR);
-      throw new HiveSQLException("Error running query: " + e.toString());
+      throw new HiveSQLException("Error running query: " + e.getMessage(), "07000", e);
     }
     setState(OperationState.FINISHED);
   }
@@ -145,7 +156,7 @@ public class SQLOperation extends ExecuteStatementOperation {
 
   @Override
   public TableSchema getResultSetSchema() throws HiveSQLException {
-    assertState(OperationState.FINISHED);
+    assertAtLeastState(OperationState.RUNNING);
     if (resultSchema == null) {
       resultSchema = new TableSchema(driver.getSchema());
     }
@@ -254,4 +265,12 @@ public class SQLOperation extends ExecuteStatementOperation {
     return serde;
   }
 
+  @Override
+  public boolean isPrepared() {
+    return isPrepared;
+  }
+
+  private void setPrepared() {
+    isPrepared = true;
+  }
 }
diff --git a/src/service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java b/src/service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java
index e0e68ec..dda6ccd 100644
--- a/src/service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java
+++ b/src/service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java
@@ -21,6 +21,7 @@ package org.apache.hive.service.cli.session;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
+import java.util.Map.Entry;
 
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;
diff --git a/src/service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java b/src/service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java
index 709709e..933e10a 100644
--- a/src/service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java
+++ b/src/service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java
@@ -59,6 +59,7 @@ public class ThriftCLIService extends AbstractService implements TCLIService.Ifa
   protected CLIService cliService;
   private static final TStatus OK_STATUS = new TStatus(TStatusCode.SUCCESS_STATUS);
   private static final TStatus ERROR_STATUS = new TStatus(TStatusCode.ERROR_STATUS);
+  private static final TStatus STILL_EXECUTING_STATUS = new TStatus(TStatusCode.STILL_EXECUTING_STATUS);
 
   private static HiveAuthFactory hiveAuthFactory;
 
@@ -190,7 +191,11 @@ public class ThriftCLIService extends AbstractService implements TCLIService.Ifa
       OperationHandle operationHandle =
           cliService.executeStatement(sessionHandle, statement, confOverlay);
       resp.setOperationHandle(operationHandle.toTOperationHandle());
-      resp.setStatus(OK_STATUS);
+      if (cliService.getOperationStatus(operationHandle).equals(OperationState.RUNNING)) {
+        resp.setStatus(STILL_EXECUTING_STATUS);
+      } else {
+        resp.setStatus(OK_STATUS);
+      }
     } catch (Exception e) {
       e.printStackTrace();
       resp.setStatus(HiveSQLException.toTStatus(e));
-- 
1.7.0.4

