From 302cf0f27dec3ed37a2ac6dc00f9c0fefcf84b1b Mon Sep 17 00:00:00 2001
From: Prasad Mujumdar <prasadm@cloudera.com>
Date: Tue, 7 May 2013 18:04:40 -0700
Subject: [PATCH 112/121] CDH-11980: Create a white list of UDFs allowed when auth is turned on

---
 .../java/org/apache/hadoop/hive/conf/HiveConf.java |    2 +-
 .../hadoop/hive/ql/exec/FunctionRegistry.java      |   57 ++++++++++++--------
 .../org/apache/hadoop/hive/ql/hooks/Entity.java    |   27 ++++++++--
 .../apache/hadoop/hive/ql/hooks/ReadEntity.java    |    4 ++
 .../hadoop/hive/ql/parse/SemanticAnalyzer.java     |    6 ++
 .../hadoop/hive/ql/session/SessionState.java       |   10 ++++
 .../hive/service/cli/session/HiveSessionHook.java  |   26 +++++++++
 .../cli/session/HiveSessionHookContext.java        |   30 ++++++++++
 .../cli/session/HiveSessionHookContextImpl.java    |   46 ++++++++++++++++
 .../hive/service/cli/session/SessionManager.java   |   22 ++++++++
 10 files changed, 203 insertions(+), 27 deletions(-)
 create mode 100644 service/src/java/org/apache/hive/service/cli/session/HiveSessionHook.java
 create mode 100644 service/src/java/org/apache/hive/service/cli/session/HiveSessionHookContext.java
 create mode 100644 service/src/java/org/apache/hive/service/cli/session/HiveSessionHookContextImpl.java

diff --git a/src/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/src/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index 7f708fe..3232238 100644
--- a/src/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/src/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -698,7 +698,7 @@ public class HiveConf extends Configuration {
     HIVE_SERVER2_BLOCKING_QUERY("hive.server2.blocking.query", true),
     HIVE_SERVER2_IN_MEM_LOGGING("hive.server2.in.mem.logging", true),
     HIVE_SERVER2_IN_MEM_LOG_SIZE("hive.server2.in.mem.log.size", 128 * 1024),
-
+    HIVE_SERVER2_SESSION_HOOK("hive.server2.session.hook", ""),
     HIVE_CONF_RESTRICTED_LIST("hive.conf.restricted.list", null),
 
     // If this is set all move tasks at the end of a multi-insert query will only begin once all
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java b/src/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
index d40bd0d..4fe0866 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
@@ -43,6 +43,7 @@ import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
 import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;
+import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.ql.udf.UDAFPercentile;
 import org.apache.hadoop.hive.ql.udf.UDFAbs;
 import org.apache.hadoop.hive.ql.udf.UDFAcos;
@@ -504,7 +505,7 @@ public final class FunctionRegistry {
     if (UDF.class.isAssignableFrom(UDFClass)) {
       FunctionInfo fI = new FunctionInfo(isNative, displayName,
           new GenericUDFBridge(displayName, isOperator, UDFClass));
-      mFunctions.put(functionName.toLowerCase(), fI);
+      getFunctionRegistry().put(functionName.toLowerCase(), fI);
     } else {
       throw new RuntimeException("Registering UDF Class " + UDFClass
           + " which does not extend " + UDF.class);
@@ -526,7 +527,7 @@ public final class FunctionRegistry {
     if (GenericUDF.class.isAssignableFrom(genericUDFClass)) {
       FunctionInfo fI = new FunctionInfo(isNative, functionName,
           (GenericUDF) ReflectionUtils.newInstance(genericUDFClass, null));
-      mFunctions.put(functionName.toLowerCase(), fI);
+      getFunctionRegistry().put(functionName.toLowerCase(), fI);
     } else {
       throw new RuntimeException("Registering GenericUDF Class "
           + genericUDFClass + " which does not extend " + GenericUDF.class);
@@ -548,7 +549,7 @@ public final class FunctionRegistry {
     if (GenericUDTF.class.isAssignableFrom(genericUDTFClass)) {
       FunctionInfo fI = new FunctionInfo(isNative, functionName,
           (GenericUDTF) ReflectionUtils.newInstance(genericUDTFClass, null));
-      mFunctions.put(functionName.toLowerCase(), fI);
+      getFunctionRegistry().put(functionName.toLowerCase(), fI);
     } else {
       throw new RuntimeException("Registering GenericUDTF Class "
           + genericUDTFClass + " which does not extend " + GenericUDTF.class);
@@ -556,7 +557,7 @@ public final class FunctionRegistry {
   }
 
   public static FunctionInfo getFunctionInfo(String functionName) {
-    return mFunctions.get(functionName.toLowerCase());
+    return getFunctionRegistry().get(functionName.toLowerCase());
   }
 
   /**
@@ -566,7 +567,7 @@ public final class FunctionRegistry {
    * @return set of strings contains function names
    */
   public static Set<String> getFunctionNames() {
-    return mFunctions.keySet();
+    return getFunctionRegistry().keySet();
   }
 
   /**
@@ -586,7 +587,7 @@ public final class FunctionRegistry {
     } catch (PatternSyntaxException e) {
       return funcNames;
     }
-    for (String funcName : mFunctions.keySet()) {
+    for (String funcName : getFunctionRegistry().keySet()) {
       if (funcPattern.matcher(funcName).matches()) {
         funcNames.add(funcName);
       }
@@ -610,11 +611,11 @@ public final class FunctionRegistry {
     }
 
     Class<?> funcClass = funcInfo.getFunctionClass();
-    for (String name : mFunctions.keySet()) {
+    for (String name : getFunctionRegistry().keySet()) {
       if (name.equals(funcName)) {
         continue;
       }
-      if (mFunctions.get(name).getFunctionClass().equals(funcClass)) {
+      if (getFunctionRegistry().get(name).getFunctionClass().equals(funcClass)) {
         synonyms.add(name);
       }
     }
@@ -822,7 +823,7 @@ public final class FunctionRegistry {
 
   public static void registerGenericUDAF(boolean isNative, String functionName,
       GenericUDAFResolver genericUDAFResolver) {
-    mFunctions.put(functionName.toLowerCase(), new FunctionInfo(isNative,
+    getFunctionRegistry().put(functionName.toLowerCase(), new FunctionInfo(isNative,
         functionName.toLowerCase(), genericUDAFResolver));
   }
 
@@ -837,16 +838,16 @@ public final class FunctionRegistry {
 
   public static void registerUDAF(boolean isNative, String functionName,
       Class<? extends UDAF> udafClass) {
-    mFunctions.put(functionName.toLowerCase(), new FunctionInfo(isNative,
+    getFunctionRegistry().put(functionName.toLowerCase(), new FunctionInfo(isNative,
         functionName.toLowerCase(), new GenericUDAFBridge(
         (UDAF) ReflectionUtils.newInstance(udafClass, null))));
   }
 
   public static void unregisterTemporaryUDF(String functionName) throws HiveException {
-    FunctionInfo fi = mFunctions.get(functionName.toLowerCase());
+    FunctionInfo fi = getFunctionRegistry().get(functionName.toLowerCase());
     if (fi != null) {
       if (!fi.isNative()) {
-        mFunctions.remove(functionName.toLowerCase());
+        getFunctionRegistry().remove(functionName.toLowerCase());
       } else {
         throw new HiveException("Function " + functionName
             + " is hive native, it can't be dropped");
@@ -858,7 +859,7 @@ public final class FunctionRegistry {
     if (LOG.isDebugEnabled()) {
       LOG.debug("Looking up GenericUDAF: " + functionName);
     }
-    FunctionInfo finfo = mFunctions.get(functionName.toLowerCase());
+    FunctionInfo finfo = getFunctionRegistry().get(functionName.toLowerCase());
     if (finfo == null) {
       return null;
     }
@@ -866,6 +867,18 @@ public final class FunctionRegistry {
     return result;
   }
 
+  private static Map<String, FunctionInfo> getFunctionRegistry() {
+    if (SessionState.get() == null) {
+      return mFunctions;
+    } else {
+      return SessionState.get().getSessionFunctionRegistry();
+    }
+  }
+
+  public static  Map<String, FunctionInfo> getStaticFunctionRegistry() {
+    return Collections.unmodifiableMap(mFunctions);
+  }
+
   public static Object invoke(Method m, Object thisObject, Object... arguments)
       throws HiveException {
     Object o;
@@ -1027,36 +1040,36 @@ public final class FunctionRegistry {
     }
     if (udfMethods.size() > 1) {
 
-      // if the only difference is numeric types, pick the method 
+      // if the only difference is numeric types, pick the method
       // with the smallest overall numeric type.
       int lowestNumericType = Integer.MAX_VALUE;
       boolean multiple = true;
       Method candidate = null;
       List<TypeInfo> referenceArguments = null;
-      
+
       for (Method m: udfMethods) {
         int maxNumericType = 0;
-        
+
         List<TypeInfo> argumentsAccepted = TypeInfoUtils.getParameterTypeInfos(m, argumentsPassed.size());
-        
+
         if (referenceArguments == null) {
-          // keep the arguments for reference - we want all the non-numeric 
+          // keep the arguments for reference - we want all the non-numeric
           // arguments to be the same
           referenceArguments = argumentsAccepted;
         }
-        
+
         Iterator<TypeInfo> referenceIterator = referenceArguments.iterator();
-        
+
         for (TypeInfo accepted: argumentsAccepted) {
           TypeInfo reference = referenceIterator.next();
-          
+
           if (numericTypes.containsKey(accepted)) {
             // We're looking for the udf with the smallest maximum numeric type.
             int typeValue = numericTypes.get(accepted);
             maxNumericType = typeValue > maxNumericType ? typeValue : maxNumericType;
           } else if (!accepted.equals(reference)) {
             // There are non-numeric arguments that don't match from one UDF to
-            // another. We give up at this point. 
+            // another. We give up at this point.
             throw new AmbiguousMethodException(udfClass, argumentsPassed, mlist);
           }
         }
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/hooks/Entity.java b/src/ql/src/java/org/apache/hadoop/hive/ql/hooks/Entity.java
index d1e25d7..47aed70 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/hooks/Entity.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/hooks/Entity.java
@@ -22,11 +22,10 @@ import java.io.Serializable;
 import java.net.URI;
 import java.util.Map;
 
-import org.apache.hadoop.hive.ql.metadata.Partition;
+import org.apache.hadoop.hive.ql.exec.FunctionInfo;
 import org.apache.hadoop.hive.ql.metadata.DummyPartition;
+import org.apache.hadoop.hive.ql.metadata.Partition;
 import org.apache.hadoop.hive.ql.metadata.Table;
-import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.hive.conf.HiveConf;
 
 /**
  * This class encapsulates an object that is being read or written to by the
@@ -40,7 +39,7 @@ public class Entity implements Serializable {
    * The type of the entity.
    */
   public static enum Type {
-    TABLE, PARTITION, DUMMYPARTITION, DFS_DIR, LOCAL_DIR
+    TABLE, PARTITION, DUMMYPARTITION, DFS_DIR, LOCAL_DIR, UDF
   };
 
   /**
@@ -64,6 +63,10 @@ public class Entity implements Serializable {
   private String d;
 
   /**
+   * If this is a function
+   */
+  private FunctionInfo udf;
+  /**
    * This is derived from t and p, but we need to serialize this field to make
    * sure Entity.hashCode() does not need to recursively read into t and p.
    */
@@ -201,6 +204,13 @@ public class Entity implements Serializable {
     this.complete = complete;
   }
 
+  public Entity (FunctionInfo udf) {
+    this.udf = udf;
+    typ = Type.UDF;
+    name = computeName();
+    complete = true;
+  }
+
   /**
    * Get the parameter map of the Entity.
    */
@@ -253,6 +263,13 @@ public class Entity implements Serializable {
   }
 
   /**
+   * Get the UDF associated with the entity.
+   */
+  public FunctionInfo getUDF() {
+    return udf;
+  }
+
+  /**
    * toString function.
    */
   @Override
@@ -268,6 +285,8 @@ public class Entity implements Serializable {
       return t.getDbName() + "@" + t.getTableName() + "@" + p.getName();
     case DUMMYPARTITION:
       return p.getName();
+    case UDF:
+      return udf.getDisplayName();
     default:
       return d;
     }
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/hooks/ReadEntity.java b/src/ql/src/java/org/apache/hadoop/hive/ql/hooks/ReadEntity.java
index 00a9c76..d86fead 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/hooks/ReadEntity.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/hooks/ReadEntity.java
@@ -22,6 +22,7 @@ import java.io.Serializable;
 import java.util.HashSet;
 import java.util.Set;
 
+import org.apache.hadoop.hive.ql.exec.FunctionInfo;
 import org.apache.hadoop.hive.ql.metadata.Partition;
 import org.apache.hadoop.hive.ql.metadata.Table;
 
@@ -103,6 +104,9 @@ public class ReadEntity extends Entity implements Serializable {
     super(path, false);
   }
 
+  public ReadEntity (FunctionInfo udf) {
+    super(udf);
+  }
   /**
    * Equals function.
    */
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
index 87aa9b0..0188a40 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
@@ -385,6 +385,9 @@ public class SemanticAnalyzer extends BaseSemanticAnalyzer {
       if (expressionTree.getChild(0).getType() == HiveParser.Identifier) {
         String functionName = unescapeIdentifier(expressionTree.getChild(0)
             .getText());
+        if (conf.getBoolVar(ConfVars.HIVE_EXTENDED_ENITITY_CAPTURE)) {
+          getInputs().add(new ReadEntity(FunctionRegistry.getFunctionInfo(functionName)));
+        }
         if (FunctionRegistry.getGenericUDAFResolver(functionName) != null) {
           aggregations.put(expressionTree.toStringTree(), expressionTree);
           FunctionInfo fi = FunctionRegistry.getFunctionInfo(functionName);
@@ -2293,6 +2296,9 @@ public class SemanticAnalyzer extends BaseSemanticAnalyzer {
       FunctionInfo fi = FunctionRegistry.getFunctionInfo(funcName);
       if (fi != null) {
         genericUDTF = fi.getGenericUDTF();
+        if (conf.getBoolVar(ConfVars.HIVE_EXTENDED_ENITITY_CAPTURE)) {
+          getInputs().add(new ReadEntity(fi));
+        }
       }
       isUDTF = (genericUDTF != null);
       if (isUDTF) {
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java b/src/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
index 392b74b..37569ad 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
@@ -29,6 +29,7 @@ import java.util.Calendar;
 import java.util.GregorianCalendar;
 import java.util.HashMap;
 import java.util.HashSet;
+import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
@@ -41,6 +42,7 @@ import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.MapRedStats;
+import org.apache.hadoop.hive.ql.exec.FunctionInfo;
 import org.apache.hadoop.hive.ql.exec.FunctionRegistry;
 import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.history.HiveHistory;
@@ -129,6 +131,9 @@ public class SessionState {
 
   private Map<String, List<String>> localMapRedErrors;
 
+  private final Map<String, FunctionInfo> sessionFunctionRegistry =
+           new LinkedHashMap<String, FunctionInfo>();
+
   /**
    * Lineage state.
    */
@@ -204,6 +209,7 @@ public class SessionState {
         ResourceType.JAR, jarLocation.toString());
       FunctionRegistry.registerFunctionsFromPluginJar(
         jarLocation, pluginClass.getClassLoader());
+      sessionFunctionRegistry.putAll(FunctionRegistry.getStaticFunctionRegistry());
     } catch (Exception ex) {
       throw new RuntimeException("Failed to load Hive builtin functions", ex);
     }
@@ -788,4 +794,8 @@ public class SessionState {
   public void setLocalMapRedErrors(Map<String, List<String>> localMapRedErrors) {
     this.localMapRedErrors = localMapRedErrors;
   }
+
+  public Map<String, FunctionInfo> getSessionFunctionRegistry() {
+    return this.sessionFunctionRegistry;
+  }
 }
diff --git a/src/service/src/java/org/apache/hive/service/cli/session/HiveSessionHook.java b/src/service/src/java/org/apache/hive/service/cli/session/HiveSessionHook.java
new file mode 100644
index 0000000..f0e90b6
--- /dev/null
+++ b/src/service/src/java/org/apache/hive/service/cli/session/HiveSessionHook.java
@@ -0,0 +1,26 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hive.service.cli.session;
+
+import org.apache.hive.service.cli.HiveSQLException;
+
+public interface HiveSessionHook {
+
+  public void run(HiveSessionHookContext sessionHookContext) throws HiveSQLException;
+}
diff --git a/src/service/src/java/org/apache/hive/service/cli/session/HiveSessionHookContext.java b/src/service/src/java/org/apache/hive/service/cli/session/HiveSessionHookContext.java
new file mode 100644
index 0000000..66def38
--- /dev/null
+++ b/src/service/src/java/org/apache/hive/service/cli/session/HiveSessionHookContext.java
@@ -0,0 +1,30 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hive.service.cli.session;
+
+import org.apache.hadoop.hive.conf.HiveConf;
+
+public interface HiveSessionHookContext {
+
+  public HiveConf getSessionConf();
+
+  public String getSessionUser();
+
+  public String getSessionHandle();
+}
diff --git a/src/service/src/java/org/apache/hive/service/cli/session/HiveSessionHookContextImpl.java b/src/service/src/java/org/apache/hive/service/cli/session/HiveSessionHookContextImpl.java
new file mode 100644
index 0000000..c884bb2
--- /dev/null
+++ b/src/service/src/java/org/apache/hive/service/cli/session/HiveSessionHookContextImpl.java
@@ -0,0 +1,46 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hive.service.cli.session;
+
+import org.apache.hadoop.hive.conf.HiveConf;
+
+public class HiveSessionHookContextImpl implements HiveSessionHookContext {
+
+  private final HiveSession hiveSession;
+
+  HiveSessionHookContextImpl(HiveSession hiveSession) {
+    this.hiveSession = hiveSession;
+  }
+
+  @Override
+  public HiveConf getSessionConf() {
+    return hiveSession.getHiveConf();
+  }
+
+
+  @Override
+  public String getSessionUser() {
+    return hiveSession.getUserName();
+  }
+
+  @Override
+  public String getSessionHandle() {
+    return hiveSession.getSessionHandle().toString();
+  }
+}
diff --git a/src/service/src/java/org/apache/hive/service/cli/session/SessionManager.java b/src/service/src/java/org/apache/hive/service/cli/session/SessionManager.java
index 8739b1d..75db28f 100644
--- a/src/service/src/java/org/apache/hive/service/cli/session/SessionManager.java
+++ b/src/service/src/java/org/apache/hive/service/cli/session/SessionManager.java
@@ -21,6 +21,7 @@ package org.apache.hive.service.cli.session;
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.hadoop.hive.common.JavaUtils;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hive.service.CompositeService;
 import org.apache.hive.service.cli.HiveSQLException;
@@ -94,6 +95,11 @@ public class SessionManager extends CompositeService {
     synchronized(sessionMapLock) {
       handleToSession.put(session.getSessionHandle(), session);
     }
+    try {
+      executeSessionHooks(session);
+    } catch (Exception e) {
+      throw new HiveSQLException("Failed to execute session hooks", e);
+    }
     return session.getSessionHandle();
   }
 
@@ -162,4 +168,20 @@ public class SessionManager extends CompositeService {
     clearIpAddress();
     clearUserName();
   }
+
+  // execute session hooks
+  private void executeSessionHooks(HiveSession session) throws Exception {
+    String hookList = hiveConf.getVar(HiveConf.ConfVars.HIVE_SERVER2_SESSION_HOOK).trim();
+    if (hookList == null || hookList.isEmpty()) {
+      return;
+    }
+
+    String[] hookClasses = hookList.split(",");
+    for (String hookClass : hookClasses) {
+      HiveSessionHook sessionHook = (HiveSessionHook)
+          Class.forName(hookClass.trim(), true, JavaUtils.getClassLoader()).newInstance();
+      sessionHook.run(new HiveSessionHookContextImpl(session));
+    }
+  }
+
 }
-- 
1.7.0.4

