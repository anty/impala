From 2dc677776a2c3f2dc89819a8dac02ad356c7555b Mon Sep 17 00:00:00 2001
From: Owen O'Malley <omalley@apache.org>
Date: Sat, 11 May 2013 15:05:10 +0000
Subject: [PATCH 115/121] HIVE-4505 Hive can't load transforms with remote scripts. (Prasad Majumdar and Gunther Hagleitner
 via omalley)

git-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1481347 13f79535-47bb-0310-9956-ffa450edef68
---
 build-common.xml                                   |    2 +-
 .../apache/hadoop/hive/cli/CliSessionState.java    |    9 +-
 .../java/org/apache/hadoop/hive/conf/HiveConf.java |    3 +-
 .../hadoop/hive/ql/session/SessionState.java       |   79 +++++++++-----------
 ql/src/test/queries/clientpositive/remote_script.q |   13 +++
 .../results/clientpositive/remote_script.q.out     |   50 ++++++++++++
 .../hive/service/cli/session/HiveSessionImpl.java  |   26 ++++++-
 7 files changed, 130 insertions(+), 52 deletions(-)
 create mode 100644 ql/src/test/queries/clientpositive/remote_script.q
 create mode 100644 ql/src/test/results/clientpositive/remote_script.q.out

diff --git a/src/build-common.xml b/src/build-common.xml
index 6aaf482..48b862a 100644
--- a/src/build-common.xml
+++ b/src/build-common.xml
@@ -87,7 +87,7 @@
   <property name="test.output" value="true"/>
   <property name="test.junit.output.format" value="xml"/>
   <property name="test.junit.output.usefile" value="true"/>
-  <property name="minimr.query.files" value="input16_cc.q,scriptfile1.q,bucket4.q,bucketmapjoin6.q,disable_merge_for_bucketing.q,reduce_deduplicate.q,smb_mapjoin_8.q,join1.q,groupby2.q,bucketizedhiveinputformat.q,bucketmapjoin7.q,optrstat_groupby.q,bucket_num_reducers.q"/>
+  <property name="minimr.query.files" value="input16_cc.q,scriptfile1.q,bucket4.q,bucketmapjoin6.q,disable_merge_for_bucketing.q,reduce_deduplicate.q,smb_mapjoin_8.q,join1.q,groupby2.q,bucketizedhiveinputformat.q,bucketmapjoin7.q,optrstat_groupby.q,bucket_num_reducers.q,remote_script.q"/>
   <property name="minimr.query.negative.files" value="cluster_tasklog_retrieval.q,minimr_broken_pipe.q,mapreduce_stack_trace.q,mapreduce_stack_trace_turnoff.q,mapreduce_stack_trace_hadoop20.q,mapreduce_stack_trace_turnoff_hadoop20.q" />
   <property name="test.silent" value="true"/>
   <property name="hadoopVersion" value="${hadoop.version.ant-internal}"/>
diff --git a/src/cli/src/java/org/apache/hadoop/hive/cli/CliSessionState.java b/src/cli/src/java/org/apache/hadoop/hive/cli/CliSessionState.java
index dfb30e2..d11b873 100644
--- a/src/cli/src/java/org/apache/hadoop/hive/cli/CliSessionState.java
+++ b/src/cli/src/java/org/apache/hadoop/hive/cli/CliSessionState.java
@@ -18,6 +18,7 @@
 
 package org.apache.hadoop.hive.cli;
 
+import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Properties;
@@ -77,11 +78,6 @@ public class CliSessionState extends SessionState {
 
   private Hive hive; // currently only used (and init'ed) in getCurrentDbName
 
-  public CliSessionState() {
-    super();
-    remoteMode = false;
-  }
-
   public CliSessionState(HiveConf conf) {
     super(conf);
     remoteMode = false;
@@ -112,10 +108,13 @@ public class CliSessionState extends SessionState {
 
   public void close() {
     try {
+      super.close();
       if (remoteMode) {
         client.clean();
         transport.close();
       }
+    } catch (IOException ioe) {
+      ioe.printStackTrace();
     } catch (TException e) {
       e.printStackTrace();
     }
diff --git a/src/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/src/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index 094e1f0..bc4130d 100644
--- a/src/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/src/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -208,7 +208,8 @@ public class HiveConf extends Configuration {
     DYNAMICPARTITIONMAXPARTS("hive.exec.max.dynamic.partitions", 1000),
     DYNAMICPARTITIONMAXPARTSPERNODE("hive.exec.max.dynamic.partitions.pernode", 100),
     MAXCREATEDFILES("hive.exec.max.created.files", 100000L),
-    DOWNLOADED_RESOURCES_DIR("hive.downloaded.resources.dir", "/tmp/"+System.getProperty("user.name")+"/hive_resources"),
+    DOWNLOADED_RESOURCES_DIR("hive.downloaded.resources.dir",
+        "/tmp/${hive.session.id}_resources"),
     DEFAULTPARTITIONNAME("hive.exec.default.partition.name", "__HIVE_DEFAULT_PARTITION__"),
     DEFAULT_ZOOKEEPER_PARTITION_NAME("hive.lockmgr.zookeeper.default.partition.name", "__HIVE_DEFAULT_ZOOKEEPER_PARTITION__"),
     // Whether to show a link to the most failed task + debugging tips
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java b/src/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
index 37569ad..7db4c14 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
@@ -22,11 +22,12 @@ import java.io.File;
 import java.io.IOException;
 import java.io.InputStream;
 import java.io.PrintStream;
+import java.lang.management.ManagementFactory;
 import java.net.URI;
 import java.net.URL;
+import java.text.SimpleDateFormat;
 import java.util.ArrayList;
-import java.util.Calendar;
-import java.util.GregorianCalendar;
+import java.util.Date;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.LinkedHashMap;
@@ -34,6 +35,7 @@ import java.util.List;
 import java.util.Map;
 import java.util.Set;
 
+import org.apache.commons.io.FileUtils;
 import org.apache.commons.lang.StringUtils;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
@@ -62,6 +64,7 @@ import org.apache.hadoop.hive.ql.util.DosToUnix;
  * configuration information
  */
 public class SessionState {
+  private static final Log LOG = LogFactory.getLog(SessionState.class);
 
   /**
    * current configuration.
@@ -189,10 +192,6 @@ public class SessionState {
     this.isVerbose = isVerbose;
   }
 
-  public SessionState() {
-    this(null);
-  }
-
   public SessionState(HiveConf conf) {
     this.conf = conf;
     isSilent = conf.getBoolVar(HiveConf.ConfVars.HIVESESSIONSILENT);
@@ -213,8 +212,15 @@ public class SessionState {
     } catch (Exception ex) {
       throw new RuntimeException("Failed to load Hive builtin functions", ex);
     }
+    // if there isn't already a session name, go ahead and create it.
+    if (StringUtils.isEmpty(conf.getVar(HiveConf.ConfVars.HIVESESSIONID))) {
+      conf.setVar(HiveConf.ConfVars.HIVESESSIONID, makeSessionId());
+    }
   }
 
+  private static final SimpleDateFormat DATE_FORMAT =
+    new SimpleDateFormat("yyyyMMddHHmm");
+
   public void setCmd(String cmdString) {
     conf.setVar(HiveConf.ConfVars.HIVEQUERYSTRING, cmdString);
   }
@@ -269,12 +275,6 @@ public class SessionState {
     HiveConf conf = startSs.getConf();
     Thread.currentThread().setContextClassLoader(conf.getClassLoader());
 
-    if (StringUtils.isEmpty(startSs.getConf().getVar(
-        HiveConf.ConfVars.HIVESESSIONID))) {
-      startSs.getConf()
-          .setVar(HiveConf.ConfVars.HIVESESSIONID, makeSessionId());
-    }
-
     if (startSs.hiveHist == null) {
       startSs.hiveHist = new HiveHistory(startSs);
     }
@@ -327,15 +327,15 @@ public class SessionState {
     return hiveHist;
   }
 
+  /**
+   * Create a session ID. Looks like:
+   *   $user_$pid@$host_$date
+   * @return the unique string
+   */
   private static String makeSessionId() {
-    GregorianCalendar gc = new GregorianCalendar();
     String userid = System.getProperty("user.name");
-
-    return userid
-        + "_"
-        + String.format("%1$4d%2$02d%3$02d%4$02d%5$02d", gc.get(Calendar.YEAR),
-        gc.get(Calendar.MONTH) + 1, gc.get(Calendar.DAY_OF_MONTH), gc
-        .get(Calendar.HOUR_OF_DAY), gc.get(Calendar.MINUTE));
+    return userid + "_" + ManagementFactory.getRuntimeMXBean().getName() + "_"
+        + DATE_FORMAT.format(new Date());
   }
 
   public String getCurrentDB() {
@@ -626,35 +626,15 @@ public class SessionState {
   private String downloadResource(String value, boolean convertToUnix) {
     if (canDownloadResource(value)) {
       getConsole().printInfo("converting to local " + value);
-      String location = getConf().getVar(HiveConf.ConfVars.DOWNLOADED_RESOURCES_DIR);
-
+      File resourceDir = new File(getConf().getVar(HiveConf.ConfVars.DOWNLOADED_RESOURCES_DIR));
       String destinationName = new Path(value).getName();
-      String prefix = destinationName;
-      String postfix = null;
-      int index = destinationName.lastIndexOf(".");
-      if (index > 0) {
-        prefix = destinationName.substring(0, index);
-        postfix = destinationName.substring(index);
-      }
-      if (prefix.length() < 3) {
-        prefix += ".tmp";   // prefix should be longer than 3
-      }
-
-      File resourceDir = new File(location);
-      if (resourceDir.exists() && !resourceDir.isDirectory()) {
-        throw new RuntimeException("The resource directory is not a directory, " +
-            "resourceDir is set to " + resourceDir);
+      File destinationFile = new File(resourceDir, destinationName);
+      if (resourceDir.exists() && ! resourceDir.isDirectory()) {
+        throw new RuntimeException("The resource directory is not a directory, resourceDir is set to" + resourceDir);
       }
       if (!resourceDir.exists() && !resourceDir.mkdirs()) {
         throw new RuntimeException("Couldn't create directory " + resourceDir);
       }
-
-      File destinationFile;
-      try {
-        destinationFile = File.createTempFile(prefix, postfix, resourceDir);
-      } catch (Exception e) {
-        throw new RuntimeException("Failed to create temporary file for " + value, e);
-      }
       try {
         FileSystem fs = FileSystem.get(new URI(value), conf);
         fs.copyToLocalFile(new Path(value), new Path(destinationFile.getCanonicalPath()));
@@ -798,4 +778,17 @@ public class SessionState {
   public Map<String, FunctionInfo> getSessionFunctionRegistry() {
     return this.sessionFunctionRegistry;
   }
+
+  public void close() throws IOException {
+    File resourceDir =
+      new File(getConf().getVar(HiveConf.ConfVars.DOWNLOADED_RESOURCES_DIR));
+    LOG.debug("Removing resource dir " + resourceDir);
+    try {
+      if (resourceDir.exists()) {
+        FileUtils.deleteDirectory(resourceDir);
+      }
+    } catch (IOException e) {
+      LOG.info("Error removing session resource dir " + resourceDir, e);
+    }
+  }
 }
diff --git a/src/ql/src/test/queries/clientpositive/remote_script.q b/src/ql/src/test/queries/clientpositive/remote_script.q
new file mode 100644
index 0000000..926601c
--- /dev/null
+++ b/src/ql/src/test/queries/clientpositive/remote_script.q
@@ -0,0 +1,13 @@
+dfs -put ../data/scripts/newline.py /newline.py;
+add file hdfs:///newline.py;
+set hive.transform.escape.input=true;
+
+create table tmp_tmp(key string, value string) stored as rcfile;
+insert overwrite table tmp_tmp
+SELECT TRANSFORM(key, value) USING
+'python newline.py' AS key, value FROM src limit 6;
+
+select * from tmp_tmp ORDER BY key ASC, value ASC;
+
+dfs -rmr /newline.py;
+drop table tmp_tmp;
diff --git a/src/ql/src/test/results/clientpositive/remote_script.q.out b/src/ql/src/test/results/clientpositive/remote_script.q.out
new file mode 100644
index 0000000..8806b2b
--- /dev/null
+++ b/src/ql/src/test/results/clientpositive/remote_script.q.out
@@ -0,0 +1,50 @@
+PREHOOK: query: create table tmp_tmp(key string, value string) stored as rcfile
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: create table tmp_tmp(key string, value string) stored as rcfile
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@tmp_tmp
+PREHOOK: query: insert overwrite table tmp_tmp
+SELECT TRANSFORM(key, value) USING
+'python newline.py' AS key, value FROM src limit 6
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@tmp_tmp
+POSTHOOK: query: insert overwrite table tmp_tmp
+SELECT TRANSFORM(key, value) USING
+'python newline.py' AS key, value FROM src limit 6
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@tmp_tmp
+POSTHOOK: Lineage: tmp_tmp.key SCRIPT [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: tmp_tmp.value SCRIPT [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: select * from tmp_tmp ORDER BY key ASC, value ASC
+PREHOOK: type: QUERY
+PREHOOK: Input: default@tmp_tmp
+#### A masked pattern was here ####
+POSTHOOK: query: select * from tmp_tmp ORDER BY key ASC, value ASC
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@tmp_tmp
+#### A masked pattern was here ####
+POSTHOOK: Lineage: tmp_tmp.key SCRIPT [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: tmp_tmp.value SCRIPT [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:value, type:string, comment:default), ]
+1	2	NULL
+1	2	NULL
+1	NULL
+2	NULL
+1	NULL
+2	NULL
+1	NULL
+2	NULL
+1	NULL
+2	NULL
+#### A masked pattern was here ####
+PREHOOK: query: drop table tmp_tmp
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@tmp_tmp
+PREHOOK: Output: default@tmp_tmp
+POSTHOOK: query: drop table tmp_tmp
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@tmp_tmp
+POSTHOOK: Output: default@tmp_tmp
+POSTHOOK: Lineage: tmp_tmp.key SCRIPT [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: tmp_tmp.value SCRIPT [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/src/service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java b/src/service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java
index 9668fe6..6678525 100644
--- a/src/service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java
+++ b/src/service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java
@@ -18,11 +18,17 @@
 
 package org.apache.hive.service.cli.session;
 
+import java.io.File;
+import java.io.IOException;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
+import org.apache.commons.io.FileUtils;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;
 import org.apache.hadoop.hive.metastore.IMetaStoreClient;
 import org.apache.hadoop.hive.metastore.api.MetaException;
@@ -61,6 +67,8 @@ public class HiveSessionImpl implements HiveSession {
 
   private static final String FETCH_WORK_SERDE_CLASS =
       "org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe";
+  private static final Log LOG = LogFactory.getLog(HiveSessionImpl.class);
+
 
   private SessionManager sessionManager;
   private OperationManager operationManager;
@@ -78,7 +86,9 @@ public class HiveSessionImpl implements HiveSession {
         hiveConf.set(entry.getKey(), entry.getValue());
       }
     }
-
+    // set an explicit session name to control the download directory name
+    hiveConf.set(ConfVars.HIVESESSIONID.varname,
+        sessionHandle.getHandleIdentifier().toString());
     sessionState = new SessionState(hiveConf);
   }
 
@@ -344,8 +354,20 @@ public class HiveSessionImpl implements HiveSession {
       if (metastoreClient != null) {
         metastoreClient.close();
       }
-    } finally {
+      // Iterate through the opHandles and close their operations
+      for (OperationHandle opHandle : opHandleSet) {
+        operationManager.closeOperation(opHandle);
+      }
+      opHandleSet.clear();
+      HiveHistory hiveHist = sessionState.getHiveHistory();
+      if (null != hiveHist) {
+        hiveHist.closeStream();
+      }
+      sessionState.close();
+      release();
+    } catch (IOException ioe) {
       release();
+      throw new HiveSQLException("Failure to close", ioe);
     }
   }
 
-- 
1.7.0.4

