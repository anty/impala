From 51c4c813c18e284d08425241bd6cd7a760ddf0f8 Mon Sep 17 00:00:00 2001
From: Brock Noland <brock@apache.org>
Date: Mon, 11 Feb 2013 10:58:57 -0600
Subject: [PATCH 063/121] CDH-10390: Hive MR2 Tests fail randomly due to race with LocalJobRunner

---
 .../hadoop/hive/ql/exec/HadoopJobExecHelper.java   |   86 +++++++++++---------
 1 files changed, 46 insertions(+), 40 deletions(-)

diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/exec/HadoopJobExecHelper.java b/src/ql/src/java/org/apache/hadoop/hive/ql/exec/HadoopJobExecHelper.java
index 83cf653..5abbb52 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/exec/HadoopJobExecHelper.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/exec/HadoopJobExecHelper.java
@@ -228,6 +228,7 @@ public class HadoopJobExecHelper {
     int numMap = -1;
     int numReduce = -1;
     List<ClientStatsPublisher> clientStatPublishers = getClientStatPublishers();
+    boolean localMode = ShimLoader.getHadoopShims().isLocalMode(job);
 
     while (!rj.isComplete()) {
       try {
@@ -244,53 +245,58 @@ public class HadoopJobExecHelper {
         initializing = false;
       }
 
-      if (!initOutputPrinted) {
-        SessionState ss = SessionState.get();
+      /*
+       * JobClient is useless after job completes in local mode so the
+       * the jc.get* calls below were racy and threw runtime exceptions
+       */
+      if(!localMode) {
+        if (!initOutputPrinted) {
+          SessionState ss = SessionState.get();
 
-        String logMapper;
-        String logReducer;
+          String logMapper;
+          String logReducer;
 
-        TaskReport[] mappers = jc.getMapTaskReports(rj.getJobID());
-        if (mappers == null) {
-          logMapper = "no information for number of mappers; ";
-        } else {
-          numMap = mappers.length;
-          if (ss != null) {
-            ss.getHiveHistory().setTaskProperty(SessionState.get().getQueryId(), getId(),
-              Keys.TASK_NUM_MAPPERS, Integer.toString(numMap));
+          TaskReport[] mappers = jc.getMapTaskReports(rj.getJobID());
+          if (mappers == null) {
+            logMapper = "no information for number of mappers; ";
+          } else {
+            numMap = mappers.length;
+            if (ss != null) {
+              ss.getHiveHistory().setTaskProperty(SessionState.get().getQueryId(), getId(),
+                Keys.TASK_NUM_MAPPERS, Integer.toString(numMap));
+            }
+            logMapper = "number of mappers: " + numMap + "; ";
           }
-          logMapper = "number of mappers: " + numMap + "; ";
-        }
 
-        TaskReport[] reducers = jc.getReduceTaskReports(rj.getJobID());
-        if (reducers == null) {
-          logReducer = "no information for number of reducers. ";
-        } else {
-          numReduce = reducers.length;
-          if (ss != null) {
-            ss.getHiveHistory().setTaskProperty(SessionState.get().getQueryId(), getId(),
-              Keys.TASK_NUM_REDUCERS, Integer.toString(numReduce));
+          TaskReport[] reducers = jc.getReduceTaskReports(rj.getJobID());
+          if (reducers == null) {
+            logReducer = "no information for number of reducers. ";
+          } else {
+            numReduce = reducers.length;
+            if (ss != null) {
+              ss.getHiveHistory().setTaskProperty(SessionState.get().getQueryId(), getId(),
+                Keys.TASK_NUM_REDUCERS, Integer.toString(numReduce));
+            }
+            logReducer = "number of reducers: " + numReduce;
           }
-          logReducer = "number of reducers: " + numReduce;
-        }
 
-        console
-            .printInfo("Hadoop job information for " + getId() + ": " + logMapper + logReducer);
-        initOutputPrinted = true;
-      }
+          console
+              .printInfo("Hadoop job information for " + getId() + ": " + logMapper + logReducer);
+          initOutputPrinted = true;
+        }
 
-      RunningJob newRj = jc.getJob(rj.getJobID());
-      if (newRj == null) {
-        // under exceptional load, hadoop may not be able to look up status
-        // of finished jobs (because it has purged them from memory). From
-        // hive's perspective - it's equivalent to the job having failed.
-        // So raise a meaningful exception
-        throw new IOException("Could not find status of job:" + rj.getJobID());
-      } else {
-        th.setRunningJob(newRj);
-        rj = newRj;
+        RunningJob newRj = jc.getJob(rj.getJobID());
+        if (newRj == null) {
+          // under exceptional load, hadoop may not be able to look up status
+          // of finished jobs (because it has purged them from memory). From
+          // hive's perspective - it's equivalent to the job having failed.
+          // So raise a meaningful exception
+          throw new IOException("Could not find status of job:" + rj.getJobID());
+        } else {
+          th.setRunningJob(newRj);
+          rj = newRj;
+        }
       }
-
       // If fatal errors happen we should kill the job immediately rather than
       // let the job retry several times, which eventually lead to failure.
       if (fatal) {
-- 
1.7.0.4

